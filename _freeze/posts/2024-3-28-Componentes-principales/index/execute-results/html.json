{
  "hash": "bde6d468c4aa748bf191d70000e4267f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Análisis de Componentes Principales (PCA)\nauthor: Joel Burbano\ndate: 2024-03-28\ncategories: [Python, R, Aprendizaje No Supervisado, Clusters ]\n---\n\n\n## Introducción \n\nEl Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica fundamental en ciencia de datos utilizada para la reducción de dimensionalidad. Este método transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales. Es especialmente útil cuando se trabaja con grandes volúmenes de datos y se busca simplificar el análisis sin perder información significativa.\n\n## ¿Qué es el Análisis de Componentes Principales?\n\nPCA es un método estadístico que transforma los datos originales en nuevas variables no correlacionadas ordenadas según la cantidad de varianza explicada. Los primeros componentes principales capturan la mayor parte de la variabilidad en los datos, lo que permite una reducción significativa de la dimensionalidad mientras se preserva la mayor cantidad de información posible.\n\n:::{.callout-important}\n\nConceptos Clave:\n\n* **Varianza:** Medida de la dispersión de los datos\n\n* **Covarianza:** Indica la dirección de la relación entre dos variables.\n\n* **Componentes Principales:** Nuevas variables no correlacionadas formadas por combinaciones lineales de las variables originales\n\n* **Valores y Vectores propios:** Los valores propios indican la cantidad de varianza capturada por cada componente principal, y los vectores propios definen la dirección de los componentes.\n\n:::\n\n## Ejemplo Practico\n:::{.callout-note icon=false appearance=\"simple\" }\n\n\nPara ilustrar un ejemplo se utilizara el dataset de Wine Quality disponible en Kaggle.\n\n### Paso 1: Instalación de librerias\n\n:::{.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n:::\n\n\n:::\n\n### Paso 2: Cara y Preprocesamiento de Datos\n\n:::{.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata = pd.read_csv(\"WineQT.csv\")\nfeatures = data.drop('quality', axis=1)\n\nfeatures = StandardScaler().fit_transform(features)\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv(\"WineQT.csv\")\nfeatures <- data %>% select(-quality)\n\nfeatures <- scale(features)\n```\n:::\n\n\n:::\n\n### Paso 3: Aplicación de PCA\n\n:::{.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(features)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\nfinalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(features, center = TRUE, scale. = TRUE)\nprincipalComponents <- data.frame(pca$x[,1:2])\nfinalDf <- cbind(principalComponents, quality = data$quality)\n```\n:::\n\n\n:::\n\n### Paso 4: Visualización de Resultados\n\n:::{.panel-tabset}\n\n## python \n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.figure(figsize = (8,6))\nsb.scatterplot(x = 'PC1', y = 'PC2', hue = 'quality', data = finalDf, palette = 'viridis')\nplt.title('PCA de Vinos')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=768}\n:::\n\n```{.python .cell-code}\nplt.clf()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(finalDf, aes(x = PC1, y = PC2, color = as.factor(quality))) +\n  geom_point(alpha=0.5) +\n  labs(title = \"PCA de Vinos\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n:::\n\n\n\n:::\n\n\n:::\n\n\n## Ventajas y Desventajas \n\n**Ventajas**\n\n* **Reducción de Dimensionalidad:** Facilita la visualización y análisis de datos de alta dimensionalidad.\n\n* **Eliminación de Redundancia:** Reduce la redundancia al eliminar las correlaciones entre variables.\n\n**Desventajas**\n\n* **Interpretabilidad:** Los componentes principales no siempre tienen un significado intuitivo.\n\n* **Pérdida de Información:** Aunque PCA preserva la mayor varianza posible, siempre hay alguna pérdida de información.\n\n\n## Conclusión\n\nEl PCA es una herramienta poderosa en el arsenal de un científico de datos. Su capacidad para simplificar conjuntos de datos complejos y reducir la dimensionalidad lo hace indispensable para la exploración y visualización de datos. A través de ejemplos prácticos en Python y R, podemos ver cómo esta técnica se aplica en la práctica, facilitando el análisis y la toma de decisiones basadas en datos.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}