[
  {
    "objectID": "proyects.html",
    "href": "proyects.html",
    "title": "Proyectos",
    "section": "",
    "text": "Manejo de ETL‚Äôs\n\n\n\nPython\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\nJoel Burbano\n\n\n8 ago 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)\n\n\n\nRenta permanente\n\n\nModelo de Ciclo de vida\n\n\nEcuaci√≥n de Euler\n\n\nCausalidad\n\n\nModelos de Consumo\n\n\n\nEl estudio del modelo te√≥rico del consumo es un tema de relevancia tanto a nivel de individuo como nivel de hogares. Es as√≠ que, en este trabajo se realiza una nueva‚Ä¶\n\n\n\n\n\n\n23 feb 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyects/ETL/index.html",
    "href": "proyects/ETL/index.html",
    "title": "Manejo de ETL‚Äôs",
    "section": "",
    "text": "Automatizaci√≥n de Procesos ETL\nEn este proyecto vamos a automatizar un proceso ETL, el mismo que tiene los siguientes pasos\n\nExtraer los: En nuestro caso utilizaremos los datos de ‚ÄúWorld Happiness report‚Äù\nTransformar los datos\nCargar los datos: se los cargarara en una base de SQLite\n\nViaualizaci√≥n del dataset\n\n\n\n  \n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\n\nimport pandas as pd\nimport sqlite3\nimport logging\n\n\n# Configuraci√≥n b√°sica de logging\nlogging.basicConfig(filename='etl_processP.log', level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef extract_data():\n    try:\n        df = pd.read_csv('world_happiness_report.csv')\n        logging.info(\"Extracci√≥n de datos completada con √©xito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la extracci√≥n de datos: %s\", e)\n        raise\n\ndef transform_data(df):\n    try:\n        df = df.drop(columns=['Standard Error'])\n        df.iloc[:,2:].fillna(df.iloc[:,2:].mean(), inplace=True)\n        df = df.drop_duplicates()\n        df['Economy (GDP per Capita)'] = df['Economy (GDP per Capita)'] / df['Economy (GDP per Capita)'].max()\n        df['Happiness Level'] = df['Happiness Score'].apply(lambda x: 'High' if x &gt; df['Happiness Score'].mean() else 'Low')\n        logging.info(\"Transformaci√≥n de datos completada con √©xito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la transformaci√≥n de datos: %s\", e)\n        raise\n\ndef load_data(df):\n    try:\n        conn = sqlite3.connect('world_happiness.db')\n        cursor = conn.cursor()\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS happiness_data (\n            Country TEXT,\n            Region TEXT,\n            Happiness_Rank INTEGER,\n            Happiness_Score REAL,\n            GDP_per_capita REAL,\n            Happiness_Level TEXT\n        )\n        ''')\n        conn.commit()\n        df.to_sql('happiness_data', conn, if_exists='replace', index=False)\n        conn.close()\n        logging.info(\"Carga de datos completada con √©xito.\")\n    except Exception as e:\n        logging.error(\"Error durante la carga de datos: %s\", e)\n        raise\n\ndef etl_process():\n    try:\n        logging.info(\"Inicio del proceso ETL.\")\n        data = extract_data()\n        transformed_data = transform_data(data)\n        load_data(transformed_data)\n        logging.info(\"Proceso ETL completado con √©xito.\")\n    except Exception as e:\n        logging.critical(\"El proceso ETL fall√≥: %s\", e)\n        raise\n\nif __name__ == \"__main__\":\n    etl_process()\n\n\n\n\n\n\nC√≥digo\n# Cargar las bibliotecas necesarias\n#install.packages(\"dplyr\")\n#install.packages(\"readr\")\n#install.packages(\"DBI\")\n#install.packages(\"RSQLite\")\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Configuraci√≥n b√°sica de logging\nlog_file &lt;- \"etl_processR.log\"\nlog_message &lt;- function(level, message) {\n  cat(sprintf(\"%s - %s - %s\\n\", Sys.time(), level, message), file = log_file, append = TRUE)\n}\n\n# Funci√≥n para extraer datos\nextract_data &lt;- function() {\n  tryCatch({\n    df &lt;- read_csv('world_happiness_report.csv')\n    log_message(\"INFO\", \"Extracci√≥n de datos completada con √©xito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la extracci√≥n de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Funci√≥n para transformar datos\ntransform_data &lt;- function(df) {\n  tryCatch({\n    df &lt;- df %&gt;% \n      select(-`Standard Error`) %&gt;%\n      mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))) %&gt;%\n      distinct() %&gt;%\n      mutate(`Economy (GDP per Capita)` = `Economy (GDP per Capita)` / max(`Economy (GDP per Capita)`, na.rm = TRUE)) %&gt;%\n      mutate(`Happiness Level` = ifelse(`Happiness Score` &gt; mean(`Happiness Score`, na.rm = TRUE), 'High', 'Low'))\n    \n    log_message(\"INFO\", \"Transformaci√≥n de datos completada con √©xito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la transformaci√≥n de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Funci√≥n para cargar datos en SQLite\nload_data &lt;- function(df) {\n  tryCatch({\n    conn &lt;- dbConnect(SQLite(), dbname = \"world_happiness.db\")\n    \n    dbExecute(conn, '\n    CREATE TABLE IF NOT EXISTS happiness_data (\n      Country TEXT,\n      Region TEXT,\n      Happiness_Rank INTEGER,\n      Happiness_Score REAL,\n      GDP_per_capita REAL,\n      Happiness_Level TEXT\n    )')\n    \n    dbWriteTable(conn, \"happiness_data\", df, overwrite = TRUE, row.names = FALSE)\n    dbDisconnect(conn)\n    log_message(\"INFO\", \"Carga de datos completada con √©xito.\")\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la carga de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Funci√≥n principal del proceso ETL\netl_process &lt;- function() {\n  tryCatch({\n    log_message(\"INFO\", \"Inicio del proceso ETL.\")\n    data &lt;- extract_data()\n    transformed_data &lt;- transform_data(data)\n    load_data(transformed_data)\n    log_message(\"INFO\", \"Proceso ETL completado con √©xito.\")\n  }, error = function(e) {\n    log_message(\"CRITICAL\", paste(\"El proceso ETL fall√≥:\", e$message))\n    stop(e)\n  })\n}\n\n# Ejecutar el proceso ETL\nif (interactive()) {\n  etl_process()\n}"
  },
  {
    "objectID": "posts/Linear_Regression/index.html",
    "href": "posts/Linear_Regression/index.html",
    "title": "Regresi√≥n Lineal Simple",
    "section": "",
    "text": "En este post presentamos un ejemplo b√°sico de regresi√≥n lineal simple, es decir, el caso de ajustar una l√≠nea a datos \\(x,y\\).\nAntes que nada importamos las librerias necesarias\n\n\nC√≥digo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\nPrimeramente generamos unos puntos para \\(x\\) y para \\(y\\)\n\n\nC√≥digo\nrng=np.random.RandomState(42)\nx=18*rng.rand(40)\ny=1.5*x-3+rng.rand(40)\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora definimos los hiperpar√°metros de nuestro modelo\n\n\nC√≥digo\nLineal=LinearRegression(fit_intercept=True) # La regresi√≥n lineal es de la forma y=ax+b, donde b!=0\n\n\nDefinimos la variable \\(X\\)\n\n\nC√≥digo\nX=x[:,np.newaxis]\nX.shape\n\n\n(40, 1)\n\n\nNota: en este caso como estamos trabajando con datos simulados no es necesario definir \\(y\\) puesto que sklearn si nos permite ingresar ese array\nAhor ajustamos el modelo\n\n\nC√≥digo\nLineal.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nObservemos el valor del coeficiente (\\(a\\))\n\n\nC√≥digo\nLineal.coef_\n\n\narray([1.50922213])\n\n\nObservemos el valor del intercepto (\\(b\\))\n\n\nC√≥digo\nLineal.intercept_\n\n\n-2.60022468062337\n\n\nProbemos el poder predictivo de nuestro modelo\n\n\nC√≥digo\nxfit=np.arange(17,22,0.5)\nXfit=xfit[:,np.newaxis]\nyfit=Lineal.predict(Xfit)\n\n\nAhora realizemos una visualizaci√≥n\n\n\nC√≥digo\ndef f(x):\n  return 1.51*x-2.6\n\nplt.scatter(x,y)\nplt.scatter(xfit,yfit,color='red')\nplt.plot(range(-1,22),[f(i) for i in range(-1,22)],color='cyan')\nplt.legend(['Datos modelo','Datos predichos','y=ax+b'])\nplt.show()"
  },
  {
    "objectID": "posts/Deteccion_fraude/index.html",
    "href": "posts/Deteccion_fraude/index.html",
    "title": "Detecci√≥n de fraude con tarjetas de Cr√©dito",
    "section": "",
    "text": "En el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias recolectadas durante dos d√≠as en el mes de de Septiembre del 2013 por European cardholders\nEmpezaremos por importar las librerias necesarias para realizar el an√°lisis\n\n\nC√≥digo\nimport pandas as pd\npd.options.display.max_columns=None\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\nProcedemos a leer los datos con pandas\n\n\nC√≥digo\n#df: base de datos de las transacciones\ndf=pd.read_csv(\"creditcard.csv.zip\")\n\n\nVerificamos que cantidad de datos tenemos\n\n\nC√≥digo\ndf.shape\n\n\n(284807, 31)\n\n\nTenemos que existen 31 columnas (variables) y 284807 filas (registros)\nAhora bien procedemos a revisar la calidad de este conjunto de datos\n\n\nC√≥digo\ndf.isnull().any()\n\n\nTime      False\nV1        False\nV2        False\nV3        False\nV4        False\nV5        False\nV6        False\nV7        False\nV8        False\nV9        False\nV10       False\nV11       False\nV12       False\nV13       False\nV14       False\nV15       False\nV16       False\nV17       False\nV18       False\nV19       False\nV20       False\nV21       False\nV22       False\nV23       False\nV24       False\nV25       False\nV26       False\nV27       False\nV28       False\nAmount    False\nClass     False\ndtype: bool\n\n\nObservamos que no hay variables con datos nulos.\nAhora bien echemos un vistazo a la variable Class la cual contiene la informaci√≥n sobre las transacciones fraudulentas\n\n\nC√≥digo\ndf[\"Class\"].value_counts()\n\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nNotamos que solamente 492 transacciones son fraudulentas\n\n\nC√≥digo\ndf['Class'].value_counts(normalize=True)\n\n\nClass\n0    0.998273\n1    0.001727\nName: proportion, dtype: float64\n\n\nes decir solo el \\(0.17\\%\\) de transacciones son fraudulentas\nAhora bien empecemos a intentar predecir\n\n\nC√≥digo\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nC√≥digo\nX=df.drop(labels='Class',axis=1)\ny=df.loc[:,'Class']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=1, stratify=y)\n\n\nAhora bien, se realiza un an√°lisis exploratorio de los datos\n\n\nC√≥digo\nX_train['Time'].describe()\n\n\ncount    199364.000000\nmean      94675.212852\nstd       47536.519022\nmin           0.000000\n25%       54039.000000\n50%       84588.500000\n75%      139243.250000\nmax      172792.000000\nName: Time, dtype: float64\n\n\nRealicemos una conversi√≥n de la variable Time de segundos a horas para facilitar la interpretaci√≥n\n\n\nC√≥digo\nX_train.loc[:,'Time']=X_train.Time/3600\nX_test.loc[:,'Time']=X_test.Time/3600\n\n\n\n\nC√≥digo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Time'],bins=40,kde=False)\nplt.xlim([0,40])\nplt.xticks(np.arange(0,48,6))\nplt.xlabel('Tiempo despues de la primera transacci√≥n (h)')\nplt.ylabel('Conteo')\nplt.title('Tiempo de transacciones')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNotamos que existen dos picos en la gr√°fica, el primero entre las 10 y 22 primeras horas y el segundo entre las 34 y 40 horas\nAnalicemos la variable Amount\n\n\nC√≥digo\nX_train['Amount'].describe()\n\n\ncount    199364.000000\nmean         88.659351\nstd         247.240287\nmin           0.000000\n25%           5.637500\n50%          22.000000\n75%          78.000000\nmax       25691.160000\nName: Amount, dtype: float64\n\n\nrealicemos una revisi√≥n gr√°fica\nprimero un histograma\n\n\nC√≥digo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Amount'],bins=50,kde=False)\nplt.ylabel('Conteo')\nplt.title('Montos de Transacci√≥n')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nC√≥digo\nplt.figure(figsize=(12,4))\nsb.boxplot(x=X_train[\"Amount\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nObservemos que los datos se encuentran fuertemente sesgados a la derecha. Para asegurarnos calculamos la asimetr√≠a\n\n\nC√≥digo\nX_train['Amount'].skew()\n\n\n16.950540423177653\n\n\n\n\nC√≥digo\nX_train.head(5)\n\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\n\n\n\n\n105644\n19.340833\n1.135011\n-0.663898\n0.703924\n0.069871\n-0.488154\n1.312078\n-0.897198\n0.463148\n-0.478801\n0.396879\n0.268544\n0.752264\n0.264092\n-0.252977\n0.515272\n-3.211514\n1.648897\n-1.297012\n-2.246958\n-0.677938\n-0.331487\n-0.069644\n0.183987\n-0.618678\n0.089015\n0.521419\n0.086390\n0.004782\n1.00\n\n\n139790\n23.155278\n-1.786262\n1.118886\n1.347969\n-0.379954\n-1.240680\n0.467667\n0.081125\n0.964933\n0.042585\n-1.275754\n-1.871478\n0.375166\n0.692938\n-0.149358\n-0.396145\n0.802805\n-0.405073\n0.153925\n-0.241419\n-0.099266\n-0.047902\n-0.182530\n-0.162509\n-0.405178\n0.512595\n0.299398\n-0.042882\n-0.059130\n141.73\n\n\n158758\n31.034167\n-0.683414\n0.679341\n2.615556\n2.362138\n-0.012716\n0.603826\n0.574245\n-0.679978\n-0.811409\n2.035115\n-0.564418\n-1.407557\n-0.094656\n-0.859411\n1.749530\n0.099132\n-0.035668\n-0.053624\n1.656191\n0.372610\n-0.007167\n0.463597\n-0.243134\n0.084557\n-0.453177\n2.687676\n-1.084269\n-0.511626\n36.19\n\n\n130845\n22.067778\n1.183540\n-0.493000\n0.755202\n-0.963160\n-0.850295\n0.145905\n-0.794616\n0.302199\n1.656943\n-0.939787\n1.101727\n1.138109\n-0.592931\n0.156943\n0.903035\n-0.419096\n-0.207755\n0.403235\n0.614310\n-0.168134\n0.039588\n0.339340\n-0.053125\n-0.298049\n0.423994\n-0.652284\n0.102582\n0.017292\n1.00\n\n\n88908\n17.318056\n1.137583\n0.105478\n0.784402\n1.254973\n-0.600870\n-0.360836\n-0.161727\n0.076092\n0.280587\n0.015787\n1.084154\n1.016011\n-0.666982\n0.250706\n-0.835022\n-0.130522\n-0.216624\n-0.058071\n0.265563\n-0.178887\n-0.195692\n-0.443664\n0.046270\n0.516246\n0.447943\n-0.554949\n0.031821\n0.018177\n7.60"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "",
    "text": "C√≥digo\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npd.options.display.max_columns=None #para que se despliegue todas las columnas\n\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom matplotlib import cm\nc4=cm.get_cmap('Set3')"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Frecuencia de los tipos de acciedentes",
    "text": "Frecuencia de los tipos de acciedentes\n\n\nC√≥digo\nsb.countplot(x=df['Collision Type'],palette='Set3')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Frecuencia de tipos de Accidentes\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que existe una mayor cantidad de accidentes entre dos veh√≠culos, por otro lado la cantidad de accidentes de bicicletas es la menor."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Accidentes ocurridos entre semana vs fin de semana",
    "text": "Accidentes ocurridos entre semana vs fin de semana\n\n\nC√≥digo\nsb.countplot(x=df['Weekend?'],palette='Accent')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Entre semana vs Fin de semana\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el Fin de semana es cuando mas ocurren accidentes."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Porcentaje de lesiones por Categoria",
    "text": "Porcentaje de lesiones por Categoria\n\n\nC√≥digo\nles_val=df['Injury Type'].value_counts()\nles_val\n\n\nInjury Type\nNo injury/unknown     41603\nNon-incapacitating    11136\nIncapacitating         1089\nFatal                   115\nName: count, dtype: int64\n\n\n\n\nC√≥digo\nplt.pie(les_val,labels=les_val.index,startangle=30,shadow=True,autopct='%1.1f%%',rotatelabels=30,explode=(0.1,0.1,0.1,0.1),colors=[c4(0.9),c4(0.2),c4(0.3),c4(0.6)])\nplt.title('Porcentaje de lesiones por Categoria')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el \\(0.2\\%\\) de accidentes son Fatales. Por otro lado algo que ser√≠a de mucho interes saber de cuantos accidentes no se conoce la lesi√≥n ocasionada puesto que la probabilidad de no tener lesiones es muy baja."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Motivo mas comun por el cual suceden accidentes",
    "text": "Motivo mas comun por el cual suceden accidentes\nPrimero indagaremos cuantos Factores Primarios existen en la data\n\n\nC√≥digo\ndf['Primary Factor'].nunique()\n\n\n55\n\n\nDado que existen \\(55\\) Factores Primarios nos quedaremos con el top 20\n\n\nC√≥digo\npfdf=df['Primary Factor'].value_counts().head(20)\n\n\n\n\nC√≥digo\nfig=plt.figure(figsize=(10,10))\naxis=fig.add_axes([1,1,1,1])\nsb.swarmplot(x=pfdf,y=pfdf.index,ax=axis)\nfor i,j in enumerate(pfdf):\n  axis.text(j,i,j)\nplt.xlabel('Ocurrencia')\nplt.title('Motivos principales por los que ocurren accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Top 30 lugares mas frecuente donde ocurren accidentes",
    "text": "Top 30 lugares mas frecuente donde ocurren accidentes\n\n\nC√≥digo\nldf=df['Reported_Location'].value_counts().head(30)\n\n\n\n\nC√≥digo\nfig1=plt.figure()\naxis1=fig1.add_axes([1,1,1,1])\nsb.barplot(x=ldf,y=ldf.index,ax=axis1,palette=\"viridis\")\nfor i,j in enumerate(ldf):\n  axis1.text(j,i,j,va='top')\naxis1.set_xlabel('Frecuencia')\naxis1.set_title('Lugares con mayor frecuencia de Accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-a√±os",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-a√±os",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Tipos de Colisiones en diferentes a√±os",
    "text": "Tipos de Colisiones en diferentes a√±os\n\n\nC√≥digo\na√±os=df.groupby('Year')\nkeys=a√±os.groups.keys()\n\n\n\n\nC√≥digo\ninfobox=[]\nfor i in range(2003,2016):\n  infobox.append(a√±os.get_group(i)['Collision Type'].value_counts())\n\n\n\n\nC√≥digo\nc2=cm.get_cmap('terrain')\n\n\n\n\nC√≥digo\nfrom IPython.display import display, Markdown\n\n\nColisiones por A√±o\n\n2003200420052006200720082009201020112012201320142015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#gr√°fico-dinamico",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#gr√°fico-dinamico",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Gr√°fico Dinamico",
    "text": "Gr√°fico Dinamico\n\n\nC√≥digo\nimport plotly.express as px\n\n\n\n\nC√≥digo\npx.histogram(df,x='Collision Type', \nanimation_frame=df['Year'].sort_values(ascending=True),\ncolor='Collision Type', title='Accidentes por Tipo de Colisi√≥n'\n)"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html",
    "href": "posts/2024-04-10-KNN/index.html",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos M√°s Cercanos (KNN) es uno de los m√©todos m√°s simples y efectivos para clasificaci√≥n y regresi√≥n en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los par√°metros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#introducci√≥n",
    "href": "posts/2024-04-10-KNN/index.html#introducci√≥n",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos M√°s Cercanos (KNN) es uno de los m√©todos m√°s simples y efectivos para clasificaci√≥n y regresi√≥n en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los par√°metros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#qu√©-es-k-vecinos-m√°s-cercanos",
    "href": "posts/2024-04-10-KNN/index.html#qu√©-es-k-vecinos-m√°s-cercanos",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "¬øQu√© es K Vecinos M√°s Cercanos?",
    "text": "¬øQu√© es K Vecinos M√°s Cercanos?\nKNN es un algoritmo de aprendizaje supervisado que clasifica una muestra en funci√≥n de las categor√≠as de sus \\(K\\) vecinos m√°s cercanos. Para la clasificaci√≥n, asigna la categor√≠a m√°s com√∫n entre sus vecinos, y para la regresi√≥n, predice el valor promedio de los vecinos m√°s cercanos.\n\nFuncionamiento del KNN:\n\nSelecci√≥n de K: Elige el n√∫mero de vecinos m√°s cercanos (\\(K\\)).\nDistancia: Calcula la distancia entre la muestra y todas las dem√°s muestras en el conjunto de datos (com√∫nmente usando la distancia euclidiana).\nVecinos: Identifica los \\(K\\) vecinos m√°s cercanos a la muestra.\nVotaci√≥n: Asigna la clase m√°s com√∫n (clasificaci√≥n) o el promedio de los valores (regresi√≥n) entre los \\(K\\) vecinos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ejemplo",
    "href": "posts/2024-04-10-KNN/index.html#ejemplo",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\nPara ilustrar el uso de KNN, usaremos el dataset Iris.\n\nPaso 1: Instalaci√≥n de Librer√≠as\n\nPythonR\n\n\n\n\nC√≥digo\nimport pandas as pd \npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nC√≥digo\nlibrary(class)\nlibrary(caret)\nlibrary(GGally)\nlibrary(ggplot2)\n\n\n\n\n\n\n\nPaso 2: Cargar y Preprocesar los Datos\n\nPythonR\n\n\n\n\nC√≥digo\ndata = pd.read_csv(\"Iris.csv\")\ndata = data.drop(['Id'], axis = 1)\nX = data.drop(['Species'], axis = 1)\ny = data['Species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n\n\n\n\n\nC√≥digo\ndata &lt;- read.csv(\"Iris.csv\")\ndata &lt;- data[,-1]\n\nany(is.na(data))\n\n\n[1] FALSE\n\n\nC√≥digo\nX &lt;- data[,-5]\ny &lt;- data$Species\n\nset.seed(42)\ntrainIndex &lt;- createDataPartition(y, p = .7, list = FALSE)\nX_train &lt;- X[trainIndex,]\nX_test &lt;- X[-trainIndex,]\ny_train &lt;- y[trainIndex]\ny_test &lt;- y[-trainIndex]\n\npreProcValues &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train &lt;- predict(preProcValues, X_train)\nX_test &lt;- predict(preProcValues, X_test)\n\n\n\n\n\n\n\nPaso 3: Entrenar el Modelo\n\nPythonR\n\n\n\n\nC√≥digo\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\n\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n\n\n\nC√≥digo\nknn_model &lt;- knn(train = X_train, test = X_test, cl = y_train, k = 3)\n\n\n\n\n\n\n\nPaso 4: Evaluar el Modelo\n\nPythonR\n\n\n\n\nC√≥digo\ny_pred = knn.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n\nAccuracy: 0.9111111111111111\n\n\nC√≥digo\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n\n\nConfusion Matrix:\n [[19  0  0]\n [ 0  9  4]\n [ 0  0 13]]\n\n\nC√≥digo\nprint('Classifcation Report:\\n', classification_report(y_test, y_pred))\n\n\nClassifcation Report:\n                  precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        19\nIris-versicolor       1.00      0.69      0.82        13\n Iris-virginica       0.76      1.00      0.87        13\n\n       accuracy                           0.91        45\n      macro avg       0.92      0.90      0.89        45\n   weighted avg       0.93      0.91      0.91        45\n\n\n\n\n\n\nC√≥digo\nconfusion &lt;- confusionMatrix(knn_model, as.factor(y_test))\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n                 Reference\nPrediction        Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              14               0              0\n  Iris-versicolor           1              14              2\n  Iris-virginica            0               1             13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9111          \n                 95% CI : (0.7878, 0.9752)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.467e-16       \n                                          \n                  Kappa : 0.8667          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Iris-setosa Class: Iris-versicolor\nSensitivity                      0.9333                 0.9333\nSpecificity                      1.0000                 0.9000\nPos Pred Value                   1.0000                 0.8235\nNeg Pred Value                   0.9677                 0.9643\nPrevalence                       0.3333                 0.3333\nDetection Rate                   0.3111                 0.3111\nDetection Prevalence             0.3111                 0.3778\nBalanced Accuracy                0.9667                 0.9167\n                     Class: Iris-virginica\nSensitivity                         0.8667\nSpecificity                         0.9667\nPos Pred Value                      0.9286\nNeg Pred Value                      0.9355\nPrevalence                          0.3333\nDetection Rate                      0.2889\nDetection Prevalence                0.3111\nBalanced Accuracy                   0.9167\n\n\n\n\n\n\n\nPaso 5: Visualizaci√≥n de Resultados\n\nPythonR\n\n\n\n\nC√≥digo\nsb.pairplot(data, hue = 'Species')\n\n\n\n\n\n\n\n\n\nC√≥digo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nC√≥digo\nggpairs(data, aes(color = Species, alpha = 0.5)) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "href": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nSimplicidad: F√°cil de entender e implementar.\nNo Param√©trico: No asume ninguna distribuci√≥n de datos.\n\nDesventajas\n\nCosto Computacional: Requiere almacenar todos los datos de entrenamiento y calcular distancias para cada predicci√≥n.\nSensibilidad al Ruido: Afectado por la escala y por valores at√≠picos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#conclusi√≥n",
    "href": "posts/2024-04-10-KNN/index.html#conclusi√≥n",
    "title": "K Vecinos m√°s cercanos (KNN)",
    "section": "Conclusi√≥n",
    "text": "Conclusi√≥n\nEl algoritmo de K Vecinos M√°s Cercanos es una herramienta poderosa y f√°cil de usar para tareas de clasificaci√≥n y regresi√≥n. A trav√©s de ejemplos pr√°cticos en Python y R, hemos demostrado c√≥mo implementar y evaluar este modelo en la pr√°ctica. Aunque tiene algunas limitaciones, KNN sigue siendo una opci√≥n valiosa para muchos problemas en ciencia de datos."
  },
  {
    "objectID": "posts/2023-12-20-PCA/index.html",
    "href": "posts/2023-12-20-PCA/index.html",
    "title": "Analisis de Componentes Principales",
    "section": "",
    "text": "En este post se pretende reducir dimensiones de una cantidad de datos es decir encontrar una transformaci√≥n en la cu√°l se represente de mejor manera los datos reduciendo as√≠ su dimensi√≥n\nComo ya es costumbre primero importamos las librer√≠as necesarias\n\n\nC√≥digo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n\nImportemos los datos\n\n\nC√≥digo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nC√≥digo\nACP=PCA(n_components=2)\n\n\nAjustamos el modelo\n\n\nC√≥digo\nACP.fit(X_iris)\n\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2)\n\n\ntransformamos la data\n\n\nC√≥digo\nX_2d=ACP.transform(X_iris)\n\n\nRealizamos una representaci√≥n gr√°fica\n\n\nC√≥digo\n#para facilitarnos el trabajo vamos a extender sobre el data iris los nuevos ejes encontrados\niris['PCA1']=X_2d[:,0]\niris['PCA2']=X_2d[:,1]\nsb.lmplot(x=\"PCA1\",y=\"PCA2\",hue='species',data=iris,fit_reg=False)"
  },
  {
    "objectID": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "href": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "title": "Tratando los datos Keane",
    "section": "",
    "text": "En este post abordaremos un poco los datos keane obtenidos de Gretl\nEmpezaremos por importar las librer√≠as necesarias\n\n\nC√≥digo\nimport pandas as pd\npd.options.display.max_columns=None\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt \nimport seaborn as sb\n\n\n\nA continuaci√≥n nos disponemos a visualizar los datos\n\n\nC√≥digo\nkeane=pd.read_csv(\"keane.csv\")\nkeane=pd.DataFrame(keane)\n\n\n\n\nC√≥digo\nkeane.head(5)\n\n\n\n\n\n\n\n\n\nid\nnumyrs\nyear\nchoice\nwage\neduc\nexpwc\nexpbc\nexpser\nmanuf\nblack\nlwage\nenroll\nemploy\nattrit\nexper\nexpersq\nstatus\n\n\n\n\n0\n1\n9\n81\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n1\n1\n9\n82\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n2\n1\n9\n83\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n3\n1\n9\n84\n1.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n1\n0\n0\n0\n0\n1.0\n\n\n4\n1\n9\n85\n2.0\nNaN\n11\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n\n\n\n\n\n\nCrearemos etiquetas para las observaciones de acuerdo a ‚Äúchoice‚Äù estudiante=1, hogar=2, cualificado=3, no-cualificado=4, servicio=5\n\n\nC√≥digo\nkeane[\"choice\"]=np.where(keane[\"choice\"]==1,\"estudiante\",\n         np.where(keane[\"choice\"]==2,\"hogar\",\n                  np.where(keane[\"choice\"]==3,\"cualificado\",\n                           np.where(keane[\"choice\"]==4,\"no-cualificado\",\"servicio\"))))\n\n\n\nProcedemos a gr√°ficar la evoluci√≥n de salarios separado por color de piel\n\n\nC√≥digo\nsb.scatterplot(data=keane,x=\"year\",y=\"wage\",hue=\"black\",style=\"black\",style_order=[1,0])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nEn esta gr√°fica evidenciamos que a lo largo de los a√±os aumenta la discriminaci√≥n.\nVisualicemos lo siguiente: seleccionando s√≥lo las personas que trabajan se realizara un gr√°fico de la evoluci√≥n de los salarios separados por la variable choice\n\n\nC√≥digo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\", y=\"wage\",hue=\"choice\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nSe observa que la terciarizaci√≥n de la econom√≠a ha aumentado las diferencias entre trabajadores cualificados y no cualificados, as√≠ como, entre servicio e industria.\nAhora procedamos al an√°lisis de la variable educ para ello primero la Codificaremos de acuerdo a educaci√≥n b√°sica=1, educaci√≥n media=2, y educaci√≥n superior=3.\n\n\nC√≥digo\nkeane[\"educCode\"]=np.where(keane[\"educ\"]&lt;=9,1,np.where(keane[\"educ\"]&lt;=12,2,3))\n\n\nSeleccionando solo las personas que trabajan tenemos lo siguiente:\n\n\nC√≥digo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\",y=\"wage\",hue=\"educCode\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "contacto.html#email-joelburbanooutlook.com",
    "href": "contacto.html#email-joelburbanooutlook.com",
    "title": "Contactos",
    "section": "Email : joelburbano@outlook.com",
    "text": "Email : joelburbano@outlook.com"
  },
  {
    "objectID": "certificados/25-05-2023_Curso_power-BI/index.html",
    "href": "certificados/25-05-2023_Curso_power-BI/index.html",
    "title": "Curso Power BI",
    "section": "",
    "text": "Curso Power BI"
  },
  {
    "objectID": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "href": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "title": "Curso PHP y MySQL",
    "section": "",
    "text": "Curso PHP y MySQL"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          T√≠tulo\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - M√°s reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nMedidas con F√≥rmulas Temporales DAX y Power BI\n\n\n\nPower BI\n\n\nDAX\n\n\n\n\n\n\n\nJoel Burbano\n\n\n25 may 2024\n\n\n\n\n\n\n\n\n\n\n\n\nK Vecinos m√°s cercanos (KNN)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje Supervisado\n\n\nKNN\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn√°lisis de Componentes Principales (PCA)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegresi√≥n Log√≠stica\n\n\n\nPython\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n11 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAccidentes automovilisticos Conjunto de datos EDA\n\n\n\nPython\n\n\n\nEn este apartado vamos a analizar datos de accidentes automovilisticos\n\n\n\nJoel burbano\n\n\n13 ene 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Mixture Models\n\n\n\nPython\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalisis de Componentes Principales\n\n\n\nPython\n\n\nPCA\n\n\nReducci√≥n de dimensiones\n\n\nAprendizaje No Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegresi√≥n Lineal Simple\n\n\n\nPython\n\n\nAprendizaje Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Clasificaci√≥n\n\n\n\nAprendizaje supervisado\n\n\nPython\n\n\nNaive Bayes\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDetecci√≥n de fraude con tarjetas de Cr√©dito\n\n\n\nPython\n\n\n\nEn el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias.\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHiperparametros y Modelos de Validaci√≥n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTratando los datos Keane\n\n\n\nEconometr√≠a\n\n\nPython\n\n\n\nEn este post se realizara un analisis de los datos keane\n\n\n\nJoel Burbano\n\n\n7 dic 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "üëã ¬°Hola! Soy Joel, graduado de Ingeniero Matem√°tico en la Facultad de Ciencias de la Escuela Polit√©cnica Nacional.\nüéì Durante mi etapa universitaria he adquirido las bases para gestionar modelos de riesgo, modelos econom√©tricos, modelos estad√≠sticos, modelos de programaci√≥n entera. As√≠ tambi√©n, he adquirido las bases de matem√°tica actuarial, estad√≠stica matem√°tica, investigaci√≥n operativa. Adem√°s, me he capacitado en el manejo de lenguajes de programaci√≥n tales como: C++, Matlab, R, Python, y tambi√©n manejo de paquetes estad√≠sticos Statgraphics y Gretl.\nüìù El desarrollo de mi trabajo de titulaci√≥n se enfoca en desarrollar un modelo de consumo a partir de la Hip√≥tesis de Renta Permanente de Friedman (Novel Economia,1976) y El Ciclo de Vida de Modigliani.\nüéØMi objetivo es seguir desarrollando mis habilidades en el √°rea estad√≠stica, econom√©trica, actuarial y ciencia de datos, por lo que me encuentro altamente interesado en trabajar en las mencionadas √°reas."
  },
  {
    "objectID": "certific.html",
    "href": "certific.html",
    "title": "Certificados",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          T√≠tulo\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - M√°s reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR para data scientist avanzado\n\n\n\nSQL\n\n\nConsultas\n\n\nCombinanciones y Subconsultas\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR para data scientist avanzado\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\nSeries Temporales\n\n\nArboles de decisi√≥n\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython para data science y big data esencial\n\n\n\nPython\n\n\nData Science\n\n\nBig Data\n\n\npandas\n\n\nPySpark\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n24 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos profesionales del an√°lisis de datos, por Microsfot y LinkedIn\n\n\n\nAnal√≠tica de datos\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n16 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprende data science Conceptos b√°sicos\n\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n12 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducci√≥n a las habilidades profesionales en an√°lisis de datos\n\n\n\nAn√°lisis de datos\n\n\nAnal√≠tica de datos\n\n\nAptitudes para carreras tecnologicas\n\n\nExcel\n\n\nPower BI\n\n\n\n\n\n\n\n\n\n\n5 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Completo de Data Science en Python Desde Cero [2023]\n\n\n\nPython\n\n\nData Science\n\n\npandas\n\n\nsklearn\n\n\nstatsmodels\n\n\nBeautifulSoup\n\n\n\n\n\n\n\n\n\n\n22 jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Power BI\n\n\n\nPower BI\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n25 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Tableau Desktop\n\n\n\nTableau\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n19 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso PHP y MySQL\n\n\n\nMySQL\n\n\nPHP\n\n\n\n\n\n\n\n\n\n\n16 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Pron√≥stico Ventas\n\n\n\nPron√≥sticos Ventas\n\n\nExcel\n\n\n\n\n\n\n\n\n\n\n16 abr 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso te√≥rico pr√°ctico El ambiente de Programaci√≥n R en el √°mbito de la investigaci√≥n cient√≠fica\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\ngit\n\n\nRmarkdown\n\n\n\n\n\n\n\n\n\n\n3 sept 2020\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "href": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "title": "Curso Pron√≥stico Ventas",
    "section": "",
    "text": "Curso Prono√≥stico Ventas"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joel Burbano",
    "section": "",
    "text": "Ingeniero Matem√°tico | Analista de Datos | Cient√≠fico de Datos"
  },
  {
    "objectID": "index.html#educaci√≥n",
    "href": "index.html#educaci√≥n",
    "title": "Joel Burbano",
    "section": "Educaci√≥n",
    "text": "Educaci√≥n\nEscuela Polit√©cnica Nacional, Quito | Quito, Ec\nIngeniero Matem√°tico | Meci√≥n Estadistica e Investigaci√≥n Operativa\nTesis: Estudio Te√≥rico del Comportamiento del Consumo en Latinoam√©rica y Caribe. En el trabajo de tesis se propuso un modelo te√≥rico del consumo, basado en la hip√≥tesis de renta permanente de Friedman y el modelo de Ciclo de Vida de Modigliani. Adem√°s, se realizo una comprobaci√≥n con datos de panel para pa√≠ses de Am√©rica Latina y Caribe."
  },
  {
    "objectID": "index.html#competencias",
    "href": "index.html#competencias",
    "title": "Joel Burbano",
    "section": "Competencias",
    "text": "Competencias\nR Python SQL MongoDB Power BI Excel(VBA)"
  },
  {
    "objectID": "posts/2023-12-20-Clustering/index.html",
    "href": "posts/2023-12-20-Clustering/index.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "En este post veremos un modelos de cluster\nComo ya es costumbre primero importamos las librerias necesarias\n\n\nC√≥digo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\n\nImportemos los datos\n\n\nC√≥digo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nC√≥digo\nGauss=GaussianMixture(n_components=3,\ncovariance_type='full')\n\n\nAjustamos el modelo\n\n\nC√≥digo\nGauss.fit(X_iris)\n\n\nGaussianMixture(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=3)\n\n\nDeterminamos las etiquetas\n\n\nC√≥digo\ny_gmm=Gauss.predict(X_iris)\n\n\ngraficamente\n\n\nC√≥digo\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\nC√≥digo\niris['cluster']=y_gmm\nsb.lmplot(data=iris,x='sepal_length',y='petal_length',hue='species',col='cluster',fit_reg=False)"
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html",
    "href": "posts/2024-03-11-regresion-logistica/index.html",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "La regresi√≥n log√≠stica es una t√©cnica de modelado estad√≠stico utilizada para predecir el resultado de una variable categ√≥rica basada en una o m√°s variables independientes. Es especialmente √∫til cuando queremos clasificar observaciones en dos o m√°s clases. A diferencia de la regresi√≥n lineal, que predice valores continuos, la regresi√≥n log√≠stica predice probabilidades de pertenencia a una categor√≠a.\n\n\nLa regresi√≥n log√≠stica utiliza la funci√≥n log√≠stica, tambi√©n conocida como funci√≥n sigmoide, para transformar la salida de una combinaci√≥n lineal de las variables independientes. La funci√≥n sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa f√≥rmula de la funci√≥n sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinaci√≥n lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)\n\n\n\n\n\n\n\n\n\n\n\nImaginemos que queremos predecir si un correo electr√≥nico es spam o no, usando caracter√≠sticas como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nC√≥digo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nC√≥digo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nC√≥digo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nC√≥digo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nC√≥digo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nC√≥digo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nC√≥digo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nC√≥digo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nC√≥digo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nC√≥digo\n# Matriz de consfusi√≥n\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nC√≥digo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nC√≥digo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo.\n\n\n\n\n\n\n\nVentajas\n\nInterpretabilidad: Es f√°cil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros m√©todos complejos\n\nDesventajas\n\nLinealidad: Asume una relaci√≥n lineal entre las variables independientes y la probabilidad de la clase.\nLimitaci√≥n: La versi√≥n b√°sica est√° dise√±ada para clasificaci√≥n binaria.\n\n\n\n\nLa regresi√≥n log√≠stica es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificaci√≥n. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagn√≥stico m√©dico hasta la detecci√≥n de fraude."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#com√≥-funciona",
    "href": "posts/2024-03-11-regresion-logistica/index.html#com√≥-funciona",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "La regresi√≥n log√≠stica utiliza la funci√≥n log√≠stica, tambi√©n conocida como funci√≥n sigmoide, para transformar la salida de una combinaci√≥n lineal de las variables independientes. La funci√≥n sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa f√≥rmula de la funci√≥n sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinaci√≥n lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)"
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-pr√°ctico",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-pr√°ctico",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "Imaginemos que queremos predecir si un correo electr√≥nico es spam o no, usando caracter√≠sticas como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nC√≥digo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nC√≥digo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nC√≥digo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nC√≥digo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nC√≥digo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nC√≥digo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nC√≥digo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nC√≥digo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nC√≥digo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nC√≥digo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nC√≥digo\n# Matriz de consfusi√≥n\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nC√≥digo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nC√≥digo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluaci√≥n",
    "href": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluaci√≥n",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "En general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "Ventajas\n\nInterpretabilidad: Es f√°cil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros m√©todos complejos\n\nDesventajas\n\nLinealidad: Asume una relaci√≥n lineal entre las variables independientes y la probabilidad de la clase.\nLimitaci√≥n: La versi√≥n b√°sica est√° dise√±ada para clasificaci√≥n binaria."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#conclusi√≥n",
    "href": "posts/2024-03-11-regresion-logistica/index.html#conclusi√≥n",
    "title": "Regresi√≥n Log√≠stica",
    "section": "",
    "text": "La regresi√≥n log√≠stica es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificaci√≥n. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagn√≥stico m√©dico hasta la detecci√≥n de fraude."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "",
    "text": "Las medidas con f√≥rmulas temporales en DAX (Data Analysis Expressions) y Power BI son herramientas poderosas que permiten a los analistas de datos realizar c√°lculos avanzados en base a per√≠odos de tiempo. Estas medidas son esenciales para el an√°lisis de series temporales, permitiendo comparar datos entre diferentes per√≠odos y obtener insights sobre tendencias y patrones. En este art√≠culo, exploraremos qu√© son estas medidas, para qu√© sirven, y veremos un ejemplo pr√°ctico utilizando Power BI."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Paso 1: Cargar los Datos en Power BI",
    "text": "Paso 1: Cargar los Datos en Power BI\n\nAbre Power BI Desktop.\nHaz clic en ‚ÄúObtener datos‚Äù y selecciona ‚ÄúArchivo CSV‚Äù\nCarga el dataset ‚ÄúSuperstore Sales‚Äù"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Paso 2: Crear un Modelo de Datos",
    "text": "Paso 2: Crear un Modelo de Datos\n\nAbre Transformar datos y verificamos que las columnas tengan el formato adecuado\nCrea una tabla de fechas ‚ÄúCalendario‚Äù\n\n\n\ndax\n\ncalendario = CALENDAR(MIN(Superstore[Order Date]), MAX(Superstore[Order Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Paso 3: Definir Relaciones",
    "text": "Paso 3: Definir Relaciones\n\nEn la vista ‚ÄúModelo‚Äù, crea una relaci√≥n entre la columna ‚ÄúOrder Date‚Äù de la tabla de Superstore y la columna ‚ÄúDate‚Äù de la tabla calendario."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Paso 4: Crear Medidas Temporales",
    "text": "Paso 4: Crear Medidas Temporales\n\nVentas Totales (Total Sales):\n\n\n\ndax\n\ntotal_sales = SUM(Superstore[Sales])\n\n\nVentas Acumuladas A√±o a la Fecha (YTD Sales):\n\n\n\ndax\n\nYTD_Sales = TOTALYTD([total_sales],calendario[Date])\n\n\nVentas Acumuladas Mes a la Fecha (MTD Sales):\n\n\n\ndax\n\nMTD_Sales = TOTALMTD([total_sales], calendario[Date])\n\n\nVentas del Mismo Per√≠odo el A√±o Pasado (Same Period Last Year Sales):\n\n\n\ndax\n\nSales_last_year = CALCULATE( [total_sales], SAMEPERIODLASTYEAR(calendario[Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Ventajas",
    "text": "Ventajas\n\nFacilidad de Uso: Power BI y DAX facilitan la creaci√≥n y manipulaci√≥n de medidas temporales.\nFlexibilidad: Permite realizar c√°lculos complejos y personalizados seg√∫n las necesidades del an√°lisis.\nVisualizaci√≥n Interactiva: Power BI ofrece herramientas visuales que permiten explorar datos de manera interactiva."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "title": "Medidas con F√≥rmulas Temporales DAX y Power BI",
    "section": "Desventajas",
    "text": "Desventajas\n\nCurva de Aprendizaje: Puede ser desafiante para principiantes sin experiencia previa en DAX.\nRendimiento: Las medidas complejas pueden afectar el rendimiento del informe, especialmente con grandes vol√∫menes de datos."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html",
    "href": "posts/2024-3-28-Componentes-principales/index.html",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El An√°lisis de Componentes Principales (PCA, por sus siglas en ingl√©s) es una t√©cnica fundamental en ciencia de datos utilizada para la reducci√≥n de dimensionalidad. Este m√©todo transforma un conjunto de variables posiblemente correlacionadas en un conjunto m√°s peque√±o de variables no correlacionadas, llamadas componentes principales. Es especialmente √∫til cuando se trabaja con grandes vol√∫menes de datos y se busca simplificar el an√°lisis sin perder informaci√≥n significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#introducci√≥n",
    "href": "posts/2024-3-28-Componentes-principales/index.html#introducci√≥n",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El An√°lisis de Componentes Principales (PCA, por sus siglas en ingl√©s) es una t√©cnica fundamental en ciencia de datos utilizada para la reducci√≥n de dimensionalidad. Este m√©todo transforma un conjunto de variables posiblemente correlacionadas en un conjunto m√°s peque√±o de variables no correlacionadas, llamadas componentes principales. Es especialmente √∫til cuando se trabaja con grandes vol√∫menes de datos y se busca simplificar el an√°lisis sin perder informaci√≥n significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#qu√©-es-el-an√°lisis-de-componentes-principales",
    "href": "posts/2024-3-28-Componentes-principales/index.html#qu√©-es-el-an√°lisis-de-componentes-principales",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "¬øQu√© es el An√°lisis de Componentes Principales?",
    "text": "¬øQu√© es el An√°lisis de Componentes Principales?\nPCA es un m√©todo estad√≠stico que transforma los datos originales en nuevas variables no correlacionadas ordenadas seg√∫n la cantidad de varianza explicada. Los primeros componentes principales capturan la mayor parte de la variabilidad en los datos, lo que permite una reducci√≥n significativa de la dimensionalidad mientras se preserva la mayor cantidad de informaci√≥n posible.\n\n\n\n\n\n\nImportante\n\n\n\nConceptos Clave:\n\nVarianza: Medida de la dispersi√≥n de los datos\nCovarianza: Indica la direcci√≥n de la relaci√≥n entre dos variables.\nComponentes Principales: Nuevas variables no correlacionadas formadas por combinaciones lineales de las variables originales\nValores y Vectores propios: Los valores propios indican la cantidad de varianza capturada por cada componente principal, y los vectores propios definen la direcci√≥n de los componentes."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "Ejemplo Practico",
    "text": "Ejemplo Practico\n\n\n\n\n\n\nPara ilustrar un ejemplo se utilizara el dataset de Wine Quality disponible en Kaggle.\n\nPaso 1: Instalaci√≥n de librerias\n\nPythonR\n\n\n\n\nC√≥digo\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nC√≥digo\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\n\n\nPaso 2: Cara y Preprocesamiento de Datos\n\nPythonR\n\n\n\n\nC√≥digo\ndata = pd.read_csv(\"WineQT.csv\")\nfeatures = data.drop('quality', axis=1)\n\nfeatures = StandardScaler().fit_transform(features)\n\n\n\n\n\n\nC√≥digo\ndata &lt;- read.csv(\"WineQT.csv\")\nfeatures &lt;- data %&gt;% select(-quality)\n\nfeatures &lt;- scale(features)\n\n\n\n\n\n\n\nPaso 3: Aplicaci√≥n de PCA\n\nPythonR\n\n\n\n\nC√≥digo\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(features)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\nfinalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n\n\n\n\n\n\nC√≥digo\npca &lt;- prcomp(features, center = TRUE, scale. = TRUE)\nprincipalComponents &lt;- data.frame(pca$x[,1:2])\nfinalDf &lt;- cbind(principalComponents, quality = data$quality)\n\n\n\n\n\n\n\nPaso 4: Visualizaci√≥n de Resultados\n\npythonR\n\n\n\n\nC√≥digo\nplt.figure(figsize = (8,6))\nsb.scatterplot(x = 'PC1', y = 'PC2', hue = 'quality', data = finalDf, palette = 'viridis')\nplt.title('PCA de Vinos')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nC√≥digo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nC√≥digo\nggplot(finalDf, aes(x = PC1, y = PC2, color = as.factor(quality))) +\n  geom_point(alpha=0.5) +\n  labs(title = \"PCA de Vinos\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nReducci√≥n de Dimensionalidad: Facilita la visualizaci√≥n y an√°lisis de datos de alta dimensionalidad.\nEliminaci√≥n de Redundancia: Reduce la redundancia al eliminar las correlaciones entre variables.\n\nDesventajas\n\nInterpretabilidad: Los componentes principales no siempre tienen un significado intuitivo.\nP√©rdida de Informaci√≥n: Aunque PCA preserva la mayor varianza posible, siempre hay alguna p√©rdida de informaci√≥n."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#conclusi√≥n",
    "href": "posts/2024-3-28-Componentes-principales/index.html#conclusi√≥n",
    "title": "An√°lisis de Componentes Principales (PCA)",
    "section": "Conclusi√≥n",
    "text": "Conclusi√≥n\nEl PCA es una herramienta poderosa en el arsenal de un cient√≠fico de datos. Su capacidad para simplificar conjuntos de datos complejos y reducir la dimensionalidad lo hace indispensable para la exploraci√≥n y visualizaci√≥n de datos. A trav√©s de ejemplos pr√°cticos en Python y R, podemos ver c√≥mo esta t√©cnica se aplica en la pr√°ctica, facilitando el an√°lisis y la toma de decisiones basadas en datos."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html",
    "href": "posts/Hyperparameters and Model Validation/index.html",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta raz√≥n es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nC√≥digo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAqu√≠, cada fila de la data se refiere a la observaci√≥n de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#representaci√≥n-de-la-data-en-scikit-learn",
    "href": "posts/Hyperparameters and Model Validation/index.html#representaci√≥n-de-la-data-en-scikit-learn",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta raz√≥n es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nC√≥digo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAqu√≠, cada fila de la data se refiere a la observaci√≥n de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "href": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Matriz de caracteristicas (Features matrix)",
    "text": "Matriz de caracteristicas (Features matrix)\nEsta tabla contienene la informaci√≥n caracteeristica del conjunto de datos en nuestro ejemplo contiene informaci√≥n de flores. Matem√°ticamente estas caracteristicas pasan a representar las variables independientes de nuestro conjunto de datos generalmente denotado por \\(X\\)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "href": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Objetivo (Target Array)",
    "text": "Objetivo (Target Array)\nEs un arraglo que matem√°ticamente representa la variable dependiente generalmente notada por \\(y\\)\n\n\nC√≥digo\nimport matplotlib.pyplot as plt\n\n\n\n\nC√≥digo\n#plt.figure(figsize=(12,8))\nsb.set()\nsb.pairplot(iris,hue='species',size=1.5)\n\n\nC:\\Users\\JXBS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\nPara usar los datos en Scikit-Learn, tenemos que extraer los matriz \\(X\\) y el objetivo \\(y\\)\n\n\nC√≥digo\nX_iris=iris.drop('species',axis=1)\nprint(X_iris.shape)\ny_iris=iris['species']\nprint(y_iris.shape)\n\n\n(150, 4)\n(150,)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "href": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Scikit-Learn‚Äôs API estimador",
    "text": "Scikit-Learn‚Äôs API estimador\nLa API de Scikit-Learn esta dise√±ada con los siguientes principios en mente\n\nCoherencia.- Todos los objetos comparten unaa interfaz com√∫n extraida de un conunto limitado de m√©todos, con documentaci√≥n consistente.\nInspecci√≥n Todos los valores de par√°metros especificados se exponen como atributos p√∫blicos. Jerarqu√≠a de objetos limitada S√≥lo los algoritmos est√°n representados por clases de Python; los conjuntos de datos se representan en formatos est√°ndar (matrices NumPy, Pandas DataFrames, matrices dispersas de SciPy) y los nombres de los par√°metros utilizan cadenas est√°ndar de Python.\nComposici√≥n Muchas tareas de aprendizaje autom√°tico se pueden expresar como secuencias de algoritmos m√°s fundamentales, y ScikitLearn hace uso de esto siempre que es posible.\nValores predeterminados sensatos Cuando los modelos requieren par√°metros especificados por el usuario, la biblioteca define un valor predeterminado apropiado."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#b√°sicos-de-la-api",
    "href": "posts/Hyperparameters and Model Validation/index.html#b√°sicos-de-la-api",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "B√°sicos de la API",
    "text": "B√°sicos de la API\nPor lo general, los pasos para usar la API del estimador Scikit-Learn son los siguientes (repasaremos algunos ejemplos detallados en las secciones siguientes):\n\nElija una clase de modelo importando la clase de estimador adecuada de ScikitLearn.\nElija los hiperpar√°metros del modelo creando una instancia de esta clase con los valores deseados.\nOrganice los datos en una matriz de caracter√≠sticas y un vector objetivo siguiendo la discusi√≥n anterior.\nAjuste el modelo a sus datos llamando al m√©todo fit() de la instancia del modelo.\nAplique el modelo a nuevos datos:\n\n‚Ä¢ Para el aprendizaje supervisado, a menudo predecimos etiquetas para datos desconocidos usando el m√©todo predict().\n‚Ä¢ Para el aprendizaje no supervisado, a menudo transformamos o inferimos propiedades de los datos utilizando el m√©todo transform() o predict()."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresi√≥n-lineal-simple",
    "href": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresi√≥n-lineal-simple",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Ejemplo De Aprendizaje Supervisado: Regresi√≥n Lineal Simple",
    "text": "Ejemplo De Aprendizaje Supervisado: Regresi√≥n Lineal Simple\n\n\nC√≥digo\nimport numpy as np\n\n\n\n\nC√≥digo\nrng=np.random.RandomState(42)\nx=10+rng.rand(50)\ny=2*x-1+rng.rand(50)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1e7115f3c20&gt;\n\n\n\n\n\n\n\n\n\n\nEscojemos el modelo\n\n\n\nC√≥digo\nfrom sklearn.linear_model import LinearRegression\n\n\n\nEscogemos los hiperparametros\n\n\n\nC√≥digo\nmodel=LinearRegression(fit_intercept=True)\nmodel\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nOrganizando los datos en variables independientes y variable dependiente\n\n\n\nC√≥digo\nX=x[:,np.newaxis]\nX.shape\n\n\n(50, 1)\n\n\n\nAjustando el modelo\n\n\n\nC√≥digo\nmodel.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nC√≥digo\nmodel.coef_\n\n\narray([2.06607665])\n\n\n\n\nC√≥digo\nmodel.intercept_\n\n\n-1.1957940680607742\n\n\n\nPredicciendo data desconocida\n\n\n\nC√≥digo\nxfit=np.linspace(-1,11)\n\n\n\n\nC√≥digo\nXfit=xfit[:,np.newaxis]\nyfit=model.predict(Xfit)\n\n\n\n\nC√≥digo\nplt.scatter(x,y)\nplt.plot(xfit,yfit)"
  },
  {
    "objectID": "posts/naive_bayes/index.html",
    "href": "posts/naive_bayes/index.html",
    "title": "Naive Bayes Clasificaci√≥n",
    "section": "",
    "text": "En este post vamos a utilizar los datos iris para entrenar un modelo de clasificaci√≥n y ver que tan bien se puede predecir las etiquetas\nEn este caso para evitarnos particionar el conjunto a ‚Äúmano‚Äù utilizaremos la funci√≥n train_test_split\nPrieramente importamos las librerias necesarias\n\n\nC√≥digo\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\n\nluego importamos el conjunto de datos\n\n\nC√≥digo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nAhora creamos los datos de entrenamiento y validaci√≥n\n\n\nC√≥digo\nXtrain,Xtest,ytrain,ytest =train_test_split(X_iris,y_iris,random_state=1)\n\n\nAhora importamos el modelo que utilizaremos\n\n\nC√≥digo\nfrom sklearn.naive_bayes import GaussianNB\n\n\nDefinimos un nombre\n\n\nC√≥digo\nNB=GaussianNB()\n\n\najustamos el modelo\n\n\nC√≥digo\nNB.fit(Xtrain,ytrain)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\npredecimos la data que no se utilizo para establecer los par√°metros del modelo\n\n\nC√≥digo\ny_NB=NB.predict(Xtest)\n\n\nAhora utilizando la m√©trica de exactitud (accurancy score) evaluaremos que tan bien predice el modelo que se ha creado\n\n\nC√≥digo\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nC√≥digo\naccuracy_score(ytest,y_NB)\n\n\n0.9736842105263158\n\n\nVemos que con una exactitud del \\(97,4\\%\\) el modelo implementado puede etiquetar los datos requeridos."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html",
    "href": "proyects/Tesis_JXBS/index.html",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "",
    "text": "El modelo de consumo y renta surge de manera casi natural cuando un individuo se plantea un escenario del tipo ¬øQu√© pasar√≠a si se destinara una parte de su renta a un fondo de ahorro?. Es as√≠ que se empieza con un planteamiento muy b√°sico en el que el individuo esperar√≠a que su ahorro lo ayude a afrontar crisis futuras. Sin embargo, esto es un poco ingenuo puesto que el individuo no tiene certeza de cuanto tiempo va a vivir, as√≠ como tampoco de si su ahorro lo ayudara en un momento de gran crisis. Es as√≠ que de manera natural se procede a generalizar la idea a un grupo de individuos, lo cual desencadena en lo siguiente ¬øTodos los individuos se comportan de una manera similar?. A partir de esto se ramifica dos situaciones, la primera, considerar que todos los individuos tienen un comportamiento similar y por tanto realizar modelos con datos de medias; la segunda, es considerar que los individuos tiene comportamientos diferentes y por tanto buscar cual es el modelo que mejor describe esta realidad.\n\n\nAl inicio de los a√±os \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la ‚Äúley fundamental de la psicolog√≠a‚Äù mencionada por (Keynes 1936) en la Teor√≠a general. A ese momento, las limitaciones emp√≠ricas y te√≥ricas del mencionado modelo se hicieron cada vez m√°s notorias. Desde un punto de vista te√≥rico, es dif√≠cil construir modelos coherentes basados en la optimizaci√≥n intertemporal del comportamiento que sean consistentes con la descripci√≥n de (Keynes 1936) en la ‚ÄúLey fundamental de la psicolog√≠a‚Äù. Desde el punto de vista emp√≠rico, parec√≠a que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, seg√∫n (Orazio P. Attanasio y Weber 2010) se observ√≥ que la propensi√≥n marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistem√°ticamente con el nivel de rentas. Adem√°s, se observ√≥ que grupos de individuos con, niveles m√°s bajos de rentas en media, ten√≠an tasas de ahorro m√°s altas que otros grupos con niveles m√°s altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observ√≥ que las tasas de ahorro est√°n sistem√°ticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradec√≠an claramente las implicaciones del modelo keynesiano y condujeron a la formulaci√≥n de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia te√≥rica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimizaci√≥n coherente con la capacidad de ajustar la mayor√≠a de los hechos mencionados en el p√°rrafo anterior.\nA nivel macro, es m√°s probable que las fluctuaciones a corto plazo de la renta disponible est√©n dominadas por la varianza de las perturbaciones temporales que se promediar√≠an a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) tambi√©n llev√≥ a la realizaci√≥n de otras implicaciones. En una versi√≥n simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilaci√≥n, los consumidores ahorraran cuando sean j√≥venes para respaldar el consumo en la √∫ltima parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlaci√≥n entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un a√±o dado, las cohortes m√°s j√≥venes, que est√°n ahorrando, son ``m√°s ricas‚Äô‚Äô en t√©rminos de vida que las de mayor edad, que est√°n desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor ser√° la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor ser√° la tasa agregada de ahorro.\nDespu√©s de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo est√°ndar de la macroeconom√≠a moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la d√©cada de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explot√≥ la idea de usar las condiciones de primer orden del problema de optimizaci√≥n intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuaci√≥n de Euler1, hace posible el an√°lisis emp√≠rico de un problema que es anal√≠ticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centr√°ndose en la esencia econ√≥mica del modelo: los consumidores, en el momento √≥ptimo, actuar√°n para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estad√≠stica suficiente para las elecciones de los consumidores y, dadas sus propiedades din√°micas, puede ‚Äúdiferenciarse‚Äù de manera an√°loga al tratamiento de los efectos fijos en la econometr√≠a (Orazio P. Attanasio y Weber 2010).\n\n\n\n\n\nComo se mencion√≥ en la introducci√≥n, el modelo de ciclo de vida/renta permanente se desarroll√≥ para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos vol√°til que la renta y la propensi√≥n marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos ‚Äúmacro acontecimientos‚Äù siguen siendo v√°lidos y algunos tambi√©n se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es m√°s alta que la de los blancos en cualquier nivel de rentas, como se√±al√≥ (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta ‚Äúpermanente‚Äù, como los hogares encabezados por personas con diferentes niveles de educaci√≥n.\nAn√°logamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos √∫ltimos es menor que la de los primeros, como se√±alaron hace varios a√±os (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo econ√≥mico (J. Y. Campbell y Mankiw 1989) encontraron que la regresi√≥n de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estad√≠sticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran n√∫mero de consumidores que siguen una ‚Äúregla general‚Äù y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlaci√≥n significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo est√° mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y adem√°s se identifica las innovaciones permanentes a est√° ultima variable, el modelo predice que estas innovaciones deber√≠an traducirse uno a uno en consumo. Esto implica restricciones param√©tricas de ecuaciones cruzadas sobre la representaci√≥n \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), se√±alaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepci√≥n es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene ra√≠z unitaria (i.e es \\(I(1)\\). 2) la secci√≥n cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribuci√≥n transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad emp√≠rica: la distribuci√≥n de la secci√≥n cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versi√≥n est√°ndar del modelo de ciclo de vida, a cualquier edad el ‚Äú\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)‚Äù 4 y por lo tanto, por sustituci√≥n recursiva, se obtiene que el \\(log(consumo)\\) est√° dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado espec√≠fico, muestran que la evoluci√≥n relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la secci√≥n cruzada de la renta y la del consumo identificar√° los cambios en la varianza de la secci√≥n cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribuci√≥n inicial del consumo (sin importar c√≥mo se determine) en presencia de un riesgo compartido perfecto esa distribuci√≥n debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evoluci√≥n de secci√≥n cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar expl√≠citamente los movimientos en la clasificaci√≥n relativa en la distribuci√≥n del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evoluci√≥n del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlaci√≥n en bajas frecuencias entre estas dos variables como evidencia en contra de la hip√≥tesis del mercado completo. No pueden rechazar la hip√≥tesis de que a frecuencias relativamente altos (como un a√±o) no existe una relaci√≥n entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisi√≥n-bibliogr√°fica",
    "href": "proyects/Tesis_JXBS/index.html#revisi√≥n-bibliogr√°fica",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "",
    "text": "Al inicio de los a√±os \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la ‚Äúley fundamental de la psicolog√≠a‚Äù mencionada por (Keynes 1936) en la Teor√≠a general. A ese momento, las limitaciones emp√≠ricas y te√≥ricas del mencionado modelo se hicieron cada vez m√°s notorias. Desde un punto de vista te√≥rico, es dif√≠cil construir modelos coherentes basados en la optimizaci√≥n intertemporal del comportamiento que sean consistentes con la descripci√≥n de (Keynes 1936) en la ‚ÄúLey fundamental de la psicolog√≠a‚Äù. Desde el punto de vista emp√≠rico, parec√≠a que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, seg√∫n (Orazio P. Attanasio y Weber 2010) se observ√≥ que la propensi√≥n marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistem√°ticamente con el nivel de rentas. Adem√°s, se observ√≥ que grupos de individuos con, niveles m√°s bajos de rentas en media, ten√≠an tasas de ahorro m√°s altas que otros grupos con niveles m√°s altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observ√≥ que las tasas de ahorro est√°n sistem√°ticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradec√≠an claramente las implicaciones del modelo keynesiano y condujeron a la formulaci√≥n de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia te√≥rica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimizaci√≥n coherente con la capacidad de ajustar la mayor√≠a de los hechos mencionados en el p√°rrafo anterior.\nA nivel macro, es m√°s probable que las fluctuaciones a corto plazo de la renta disponible est√©n dominadas por la varianza de las perturbaciones temporales que se promediar√≠an a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) tambi√©n llev√≥ a la realizaci√≥n de otras implicaciones. En una versi√≥n simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilaci√≥n, los consumidores ahorraran cuando sean j√≥venes para respaldar el consumo en la √∫ltima parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlaci√≥n entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un a√±o dado, las cohortes m√°s j√≥venes, que est√°n ahorrando, son ``m√°s ricas‚Äô‚Äô en t√©rminos de vida que las de mayor edad, que est√°n desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor ser√° la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor ser√° la tasa agregada de ahorro.\nDespu√©s de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo est√°ndar de la macroeconom√≠a moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la d√©cada de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explot√≥ la idea de usar las condiciones de primer orden del problema de optimizaci√≥n intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuaci√≥n de Euler1, hace posible el an√°lisis emp√≠rico de un problema que es anal√≠ticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centr√°ndose en la esencia econ√≥mica del modelo: los consumidores, en el momento √≥ptimo, actuar√°n para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estad√≠stica suficiente para las elecciones de los consumidores y, dadas sus propiedades din√°micas, puede ‚Äúdiferenciarse‚Äù de manera an√°loga al tratamiento de los efectos fijos en la econometr√≠a (Orazio P. Attanasio y Weber 2010)."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "href": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "",
    "text": "Como se mencion√≥ en la introducci√≥n, el modelo de ciclo de vida/renta permanente se desarroll√≥ para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos vol√°til que la renta y la propensi√≥n marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos ‚Äúmacro acontecimientos‚Äù siguen siendo v√°lidos y algunos tambi√©n se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es m√°s alta que la de los blancos en cualquier nivel de rentas, como se√±al√≥ (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta ‚Äúpermanente‚Äù, como los hogares encabezados por personas con diferentes niveles de educaci√≥n.\nAn√°logamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos √∫ltimos es menor que la de los primeros, como se√±alaron hace varios a√±os (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo econ√≥mico (J. Y. Campbell y Mankiw 1989) encontraron que la regresi√≥n de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estad√≠sticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran n√∫mero de consumidores que siguen una ‚Äúregla general‚Äù y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlaci√≥n significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo est√° mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y adem√°s se identifica las innovaciones permanentes a est√° ultima variable, el modelo predice que estas innovaciones deber√≠an traducirse uno a uno en consumo. Esto implica restricciones param√©tricas de ecuaciones cruzadas sobre la representaci√≥n \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), se√±alaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepci√≥n es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene ra√≠z unitaria (i.e es \\(I(1)\\). 2) la secci√≥n cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribuci√≥n transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad emp√≠rica: la distribuci√≥n de la secci√≥n cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versi√≥n est√°ndar del modelo de ciclo de vida, a cualquier edad el ‚Äú\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)‚Äù 4 y por lo tanto, por sustituci√≥n recursiva, se obtiene que el \\(log(consumo)\\) est√° dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado espec√≠fico, muestran que la evoluci√≥n relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la secci√≥n cruzada de la renta y la del consumo identificar√° los cambios en la varianza de la secci√≥n cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribuci√≥n inicial del consumo (sin importar c√≥mo se determine) en presencia de un riesgo compartido perfecto esa distribuci√≥n debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evoluci√≥n de secci√≥n cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar expl√≠citamente los movimientos en la clasificaci√≥n relativa en la distribuci√≥n del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evoluci√≥n del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlaci√≥n en bajas frecuencias entre estas dos variables como evidencia en contra de la hip√≥tesis del mercado completo. No pueden rechazar la hip√≥tesis de que a frecuencias relativamente altos (como un a√±o) no existe una relaci√≥n entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#ecuaci√≥n-de-euler-del-consumo",
    "href": "proyects/Tesis_JXBS/index.html#ecuaci√≥n-de-euler-del-consumo",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Ecuaci√≥n de Euler del consumo",
    "text": "Ecuaci√≥n de Euler del consumo\n(Parker 2007) Dice que, considerando un agente de vida infinita que elige una variable de control \\(C\\) en cada per√≠odo \\(t\\) para maximizar un objetivo intertemporal: \\(\\sum\\limits_{t=1}^\\infty \\beta u(C_t)\\), donde \\(u(C_t)\\) representa el flujo de pago en \\(t\\), \\(u'&gt;0,~u''&lt;0\\), y \\(\\beta\\) es el factor de descuento, \\(0&lt;\\beta&lt;1\\). El agente se enfrenta a una restricci√≥n presupuestaria de valor presente: \\(\\sum\\limits_{t=1} R^{1-t}C_t\\leq W_1\\), donde \\(R\\) es la tasa de inter√©s bruta (\\(R=1+r\\), donde \\(r\\) es la tasa de inter√©s) y \\(W_1\\) es dado (mas adelante veremos que es el patrimonio). Por la teor√≠a de optimizaci√≥n, si una trayectoria de tiempo del control es √≥ptima, un aumento marginal en el control en cualquier \\(t\\), \\(dC_t\\), debe tener beneficios en \\(t+1\\) de la misma cantidad de valor presente, \\(-RdC_t\\), as√≠: \\(\\beta^{t-1}u'(C_t)dC_t-\\beta^tu'(C_{t+1})RdC_t=0\\). Reorganizando obtenemos las ecuaciones de Euler: \\(u'(C_t)=\\beta Ru'(C_{t+1}),\\) para \\(t=1,2,3,\\dots.\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#modelo-te√≥rico-el-modelo-de-ciclo-de-vida",
    "href": "proyects/Tesis_JXBS/index.html#modelo-te√≥rico-el-modelo-de-ciclo-de-vida",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Modelo Te√≥rico ‚ÄúEL modelo de ciclo de vida‚Äù",
    "text": "Modelo Te√≥rico ‚ÄúEL modelo de ciclo de vida‚Äù\n\nPreferencias\nLa versi√≥n del modelo que se considera es aquella en la que una unidad de consumo maximiza la utilidad esperada en un intervalo finito sujeto a un conjunto de restricciones. \\[\n\\max E_t \\left[ \\sum_{j=0}^{T-t}\\beta_{t+j}U\\left(C_{t+j},z_{t+j},v_{t+j}\\right) \\right]\n\\tag{1}\\] donde, \\(C\\) representa el ‚Äúconsumo‚Äù, \\(z\\) es un vector de variables observables que afecta a la utilidad, \\(v\\) es un vector para factores no observables que afectan a la utilidad, y \\(\\beta\\) es un factor de descuento.\nsujeto a las siguientes restricciones\n\\[\nW_{t+j+1}=W_{t+j}\\left(1+R_{t+j}^\\ast\\right) +y_{t+j} -C_{t+j},\n\\tag{2}\\]\n\\[\nW_{t+j}=\\sum_{i=1}^N A_{t+j}^i,\n\\tag{3}\\]\n\\[\nR_{t+j}^\\ast=\\sum_{i=0}^N \\omega_{t+j}^i R_{t+j}^i,\n\\tag{4}\\]\n\\[\nW_T\\geq0\n\\tag{5}\\]\ndonde, \\(W\\) es el patrimonio neto y su rendimiento, \\(\\omega\\) son las ponderaciones de la cartera, \\(R\\) son los rendimientos, \\(A\\) son los activos y \\(y\\) es el ingreso.\nLa restricci√≥n Ecuaci√≥n¬†2 es una restricci√≥n presupuestaria gen√©rica donde el valor neto aparece junto con su retorno, ingreso y consumo8. Las restricciones Ecuaci√≥n¬†3 y Ecuaci√≥n¬†4 definen el patrimonio neto, \\(W\\), y su rendimiento \\(-\\omega_{t+j}^i\\): son las acciones (o ponderaciones) de la cartera. El rendimiento del patrimonio neto est√° dado por el promedio ponderado de los rendimientos individuales, \\(R_{t+j}^i\\). Se supone que estos rendimientos no dependen de la posici√≥n neta que tome el consumidor sobre cada uno de estos activos, \\(A_{t+j}^i\\). (ver , Orazio P. Attanasio y Weber 2010)\nLa restricci√≥n Ecuaci√≥n¬†5 da el l√≠mite para el patrimonio neto total en el periodo \\(T\\). El consumidor tiene que morir sin deuda, es decir tiene que pagar su deuda con probabilidad uno9.\nEn esta formulaci√≥n se supone varias restricciones importantes: * El consumidor maximiza la utilidad esperada. * Las preferencias son aditivamente separables a lo largo del tiempo * Impl√≠citamente es posible anotar la utilidad como una funci√≥n de una sola mercanc√≠a. Esta practica presupone un teorema de agregaci√≥n del tipo estudiado por (Gorman 1959).\nEl problema formulado anteriormente es capaz de abarcar diferentes versiones del modelo que se han considerado en la literatura. En particular, tratamos como casos especiales el modelo est√°ndar de ingresos permanentes/ciclo de vida con preferencias cuadr√°ticas, el llamado ahorro de existencias reguladoras, as√≠ como versiones flexibles del modelo (con un papel importante para la demograf√≠a y la oferta laboral) que se han ajustado a los datos.\nComencemos con un caso en el que la funci√≥n de consumo se puede derivar anal√≠ticamente. Sea la utilidad cuadr√°tica en \\(C\\) (y aditivamente separable en sus otros argumentos \\(z\\)), y suponga que al menos un activo financiero se negocia libremente y produce un rendimiento real fijo, igual al par√°metro de referencia temporal constante \\(\\frac{1-\\beta}{\\beta}\\). La condici√≥n de primer orden con respecto al consumo, o ecuaci√≥n de Euler, implica que el consumo es paseo aleatorio. \\[\n     E(C_{t+1}|I_t )=C_t\n\\tag{6}\\]\ndonde \\(I_t\\), denota la informaci√≥n disponible al instante \\(t\\) . En efecto notemos que al ser la funci√≥n de utilidad cuadr√°tica en \\(C\\) y aditivamente separable en sus otros argumentos (Hall 1978) no dice que se cumple exactamente \\(C_{t+1}=\\beta_0 +\\gamma C_t -\\varepsilon_{t+1}\\) de donde tomando la Esperanza al tiempo \\(t\\) y dado que se produce un rendimiento real fijo, tenemos la Ecuaci√≥n¬†6.\nen \\(t\\), el consumidor escoge \\(C_t\\) tal que maximiza \\[\n     \\beta_0 U(C_t,z_t,v_t)+ E_t \\sum_{\\tau=t+1}^{T-t} \\beta_{\\tau+j}U(C_\\tau,z_\\tau,v_\\tau)\n\\tag{7}\\] sujeto a \\[\n    W_{\\tau+j}=W_{\\tau-1+j} \\left(1+R_{\\tau-1+j}^\\ast \\right) + y_{\\tau-1+j}-C_{\\tau-1+j}\n\\tag{8}\\] la estrategia secuencial √≥ptima tiene la forma \\[\n    C_t=g_t(w_\\tau,w_{\\tau-1},\\dots,w_0,A_0)\n\\]\nconsiderando una variaci√≥n desde esta estrategia\nSi los consumidores tienen expectativas racionales, entonces: \\[\n     C_{t+1}=C_t+\\varepsilon_{t+1} \\qquad E\\left( \\varepsilon_{t+1}|W_t \\right)=0\n\\tag{9}\\] para todas las variables \\(W\\) conocidas al instante \\(t\\). La ecuaci√≥n Ecuaci√≥n¬†9 se puede utilizar para derivar una funci√≥n de consumo, en el caso de que no exista ning√∫n tipo otro activo disponible para el consumidor (como en, Bewley 1977) y la √∫nica variable estoc√°stica es la renta del trabajo. Sustituyendo en Ecuaci√≥n¬†9 en las restricciones presupuestarias, (Flavin 1981) muestra que el consumo se iguala a la renta permanente, definido como la tasa de inter√©s multiplicada por el valor presente de los ingresos actuales y futuros esperados:\n\\[\n     C_t=\\frac{r}{1+r}A_t +\\frac{r}{1+r}\\sum_{k=0}^\\infty E\\left(y_{t+k} | I_t\\right)\n\\tag{10}\\]\nLa ecuaci√≥n Ecuaci√≥n¬†10 se deriva para el caso especial de vida infinita, pero se puede derivar una extensi√≥n a la vida finita."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "href": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Otro Modelo de Consumo",
    "text": "Otro Modelo de Consumo\nUna vez que tenemos estas definiciones podemos entonces incluir que si se sigue a (J. Y. Campbell 1987) y se define el ahorro como \\[\n    s_t=\\frac{rA_t}{1+r} +y_t -C_t\n\\tag{11}\\]\nAhora si \\(P(L) y_t=a+\\zeta_t\\), donde \\(P(L)\\) es un polinomio en el operador de retardos y \\(\\zeta_t\\) es un ruido blanco. En este caso la ecuaci√≥n Ecuaci√≥n¬†10 implica (Flavin 1981) que: \\[\n    P\\left(\\frac{1}{1+r}\\right)\\Delta C_{t+1}= \\frac{r}{1+r}\\zeta_{t+1}\n\\tag{12}\\]\nluego podemos reescribir Ecuaci√≥n¬†12 como: \\[\n    s_t=- \\sum_{k=1}^\\infty (1+r)^{-k} E\\left(\\Delta y_{t+k}|I_t \\right)\n\\tag{13}\\]\nLa ecuaci√≥n Ecuaci√≥n¬†13 muestra que los individuos deber√≠an ‚Äúahorrar para tiempos dif√≠ciles‚Äù (los ingresos futuros caen), y se mantiene (por la ley de las proyecciones iteradas) incluso si consideramos las expectativas condicionadas a un subconjunto de la informaci√≥n utilizada por los agentes econ√≥micos, como el pasado. ingreso y ahorro.\n\nProposici√≥n 1 Si \\(C_t\\) es un paseo aleatorio y adem√°s los consumidores tienen expectativas racionales y por otro lado \\(Y_t\\sim AR(1)\\) y adem√°s \\(Y_t\\sim I(1)\\) entonces: \\[\nC_{t+1}=\\frac{1}{1- \\eta}C_t +\\frac{\\eta}{1-\\eta} y_{t+1} +\\frac{\\eta^2}{1-\\eta}A_{t+1}\n\\tag{14}\\] en el caso de no tener informaci√≥n disponible sobre \\(\\eta\\), donde \\(\\eta\\) es la raz√≥n entre la tasa de inter√©s pasada y la tasa de inter√©s actual, y no disponer informaci√≥n del activo \\(A_{t+1}\\), el modelo puedo escribirse como \\[\n    C_{t+1}=aC_t+bY_{t+1}+ u_{t+1}\n\\tag{15}\\] donde \\(u_{t+1}\\) denotar√≠a la innovaci√≥n al tiempo \\(t+1\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisi√≥n-de-los-datos",
    "href": "proyects/Tesis_JXBS/index.html#revisi√≥n-de-los-datos",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Revisi√≥n de los datos",
    "text": "Revisi√≥n de los datos\nEn esta secci√≥n se empezara con el an√°lisis de las series de datos obtenidos del Banco Mundial las cuales son Households and NPISHs Final consumption expenditure per capita (constant 2015 US$) [NE.CON.PRVT.PC.KD] de los cuales se seleccionaran 14 pa√≠ses los cuales no tienen perdida de informaci√≥n. Estos pa√≠ses son: Bolivia, Brasil, Chile, Colombia, Costa Rica, Ecuador, Guatemala, Honduras, M√©xico, Nicaragua, Panam√°, Paraguay, Per√∫, y Rep√∫blica Dominicana.\nEn esta secci√≥n Hablaremos un poco de la comunidad andina que es de la cual se tiene conocimiento relevante en cuanto a su historia. La comunidad andina (CAN) esta integrada por los siguientes pa√≠ses: Bolivia, Colombia, Ecuador y Per√∫\n\nBolivia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura¬†1: Serie para Bolivia original y corregida por software TRAMO/SEATS10\n\n\n\n\n\n\n\n\n\nFigura¬†2: Serie de irregularidades para Bolivia\n\n\n\nEn la figura Figura¬†2 se observa que la serie presenta irregularidades que se describen a continuaci√≥n:\n\n\\(1962\\)\n\\(1964\\) El gobierno se vio interrumpido por medio de un golpe militar, a partir de lo cual Bolivia habr√≠a de vivir dictaduras.\n\\(1972\\) Presencia de una dictadura en el gobierno y contrato de venta de gas a Argentina.\n\\(1977\\) Extraordinario nivel de precios de las materias primas (el esta√±o lleg√≥ a cotizarse en ocho d√≥lares la libra Ô¨Åna) y una gran apertura de cr√©ditos internacionales.\n\nEn la figura Figura¬†1 (b) se puede observar que la serie posee cierta tendencia que en principio debe ser estoc√°stica puesto que la poblaci√≥n y el desarrollo sigue en crecimiento.\n\n\nColombia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura¬†3: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura¬†4: Serie de irregularidades para Colombia\n\n\n\nEn la figura Figura¬†4} se observa que la serie presenta irregularidades que se describen a continuaci√≥n\n\n\\(1964\\) Aparecen las FARC ,quienes aprobaron en su constituci√≥n un programa agrario que pretende la entrega gratuita de las tierras a los campesinos\n\\(1965\\) Empiezan los ataques de las guerrillas mismo que se extienden hasta estos d√≠as.\n\\(1992\\) Dr√°sticos recortes de energ√≠a en este pa√≠s, mismos que acarrearon perdidas millonarias.\n\\(1998\\) Se crea la ‚ÄúZona de distensi√≥n‚Äù e inicia el proceso de paz con las guerrillas.\n\\(1999\\) Debido a la gran cantidad de demandas que se interpusieron frente a la voracidad de los bancos y contra el refinado mecanismo del UPAC, el cual estaf√≥ a quienes se arriesgaron a endeudarse para tener casa, la Corte Constitucional resolvi√≥ acabarlo con la sentencia C-700\n\\(2019\\)\n\\(2020\\) COVID\n\n\n\nEcuador\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura¬†5: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura¬†6: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura¬†6} se observa que la serie presenta irregularidades que se describen a continuaci√≥n:\n\n\\(1973\\) La apropiaci√≥n de los beneficios del petr√≥leo ‚Äúla renta petrolera‚Äù se constituy√≥ en objetivo de disputa de grupos sociales y organizaciones pol√≠ticas.\n\\(1998\\) La permisividad social a favor de monopolios y oligopolios11\n\\(1999\\) Feriado Bancario\n\n\n\nPer√∫\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura¬†7: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura¬†8: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura¬†8 se observa que la serie presenta irregularidades que se describen a continuaci√≥n:\n\n\\(1983\\) Fen√≥meno del ni√±o y ca√≠da del precio de los metales.\n\\(1985\\) El sol fue reemplazado por el inti con un valor de \\(1000\\) soles (devaluaci√≥n de la moneda).\n\\(1987\\) Se empieza a sentir los efectos de las pol√≠ticas intervencionistas implementadas un a√±os atr√°s.\n\\(1989\\) Devaluaci√≥n del inti\n\nA continuaci√≥n en la figura Figura¬†9 presentamos las gr√°ficas de Consumo y Renta para cada uno de los pa√≠ses de Latinoam√©rica y Caribe seleccionados para este estudio.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\nFigura¬†9: Gr√°fica de Series Consumo y Renta para los 14 pa√≠ses de Latino√°merica y Caribe\n\n\n\nEn la tabla Tabla¬†1 se realiza el contraste de Dickey-fuller aumentado (ADF) para las series corregidas.\n\n\n\nTabla¬†1: Contraste de ra√≠z unitaria Dickey Fuller aumentado\n\n\n\n\n\nPa√≠s\nRaiz Unitaria\n\\(p\\)-value\nlags\n\n\n\n\n\nRenta\n\n\n\n\nBolivia\nsi\n0,9861\n1\n\n\nBrasil\nsi\n0,9818\n1\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n1,0000\n0\n\n\nEcuador\nsi\n0,9680\n1\n\n\nGuatemala\nsi\n0,9965\n1\n\n\nHonduras\nsi\n0,9979\n0\n\n\nM√©xico\nsi\n0,9945\n0\n\n\nNicaragua\nsi\n0,6621\n1\n\n\nPanam√°\nsi\n0,9995\n0\n\n\nParaguay\nsi\n0,9966\n1\n\n\nPer√∫\nsi\n0,9830\n1\n\n\nR. Dominicana\nsi\n1,0000\n1\n\n\n\nConsumo\n\n\n\n\nBolivia\nsi\n0,9947\n1\n\n\nBrasil\nsi\n0,9982\n0\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n0,9978\n1\n\n\nEcuador\nsi\n0,9902\n0\n\n\nGuatemala\nsi\n0,9990\n1\n\n\nHonduras\nsi\n0,9998\n1\n\n\nM√©xico\nsi\n0,9789\n1\n\n\nNicaragua\nsi\n0,6414\n1\n\n\nPanam√°\nsi\n0,9975\n0\n\n\nParaguay\nsi\n1,0000\n0\n\n\nPer√∫\nsi\n0,9604\n1\n\n\nR. Dominicana\nsi\n1,0000\n0\n\n\n\n\n\n\nEn la tabla Tabla¬†1 se ha realizado el contraste ADF para cada una de las series tanto de Renta (PIB) como de Consumo (Consumo familiar). Es as√≠ que de manera general tenemos que para todos las series.\nDada la hip√≥tesis nula de no estacionariedad frente a la alternativa de estacionariedad, El estad√≠stico de contraste de ADF para el caso sin constante para el cual su distribuci√≥n se encuentra tabulada por Dickey Fuller. Entonces para un nivel de significancia del \\(\\alpha=0.05\\) puesto que el valor \\(p\\) del estad√≠stico es mayor que \\(\\alpha\\), No se rechazo la hip√≥tesis nula de no estacionariedad.\nAhora bien se proceder√° a realizar el contraste de Johansen para el caso sin constante.\n\n\n\nTabla¬†2: Contraste de Cointegraci√≥n de Johansen\n\n\n\n\n\n\n\n\n\n\n\n\nPa√≠s\nCointegraci√≥n\n\\(p\\)-valor asint.\nboot \\(p\\)-valor\nrango\n\n\n\n\nBolivia\nsi\n0,041\n0,089\n1\n\n\nBrasil\nsi\n0,345\n0,431\n1\n\n\nChile\nsi\n0,077\n0,131\n1\n\n\nColombia\nsi\n0,571\n0,814\n1\n\n\nCosta Rica\nsi\n0,297\n0,639\n1\n\n\nEcuador\nno\n0,018\n0,01\n1\n\n\nGuatemala\nsi\n0,602\n0,906\n1\n\n\nHonduras\nno\n0,082\n0,085\n0\n\n\nM√©xico\nno\n0,144\n0,307\n0\n\n\nNicaragua\nno\n0,063\n0,079\n0\n\n\nPanam√°\nsi\n0,247\n0,354\n1\n\n\nParaguay\nsi\n0,026\n0,055\n1\n\n\nPer√∫\nsi\n0,333\n0,499\n1\n\n\nR. Dominicana\nsi\n0,139\n0,379\n1\n\n\n\n\n\n\nEn la tabla Tabla¬†2 se ha realizado el contraste de cointegraci√≥n de Johansen entre las series tanto de Renta (PIB) como de Consumo (Consumo familiar) para cada pa√≠s. Es as√≠ que de manera general tenemos que para cada pa√≠s.\nDada la hip√≥tesis nula de que el rango de la matriz de cointegraci√≥n es \\(0\\) frente a la alternativa de que el rango de cointegraci√≥n es \\(1\\). Entonces tenemos que para Honduras, M√©xico, Nicaragua, las series de consumo y renta no est√°n cointegradas pues para un nivel de significancia \\(alpha=0.05\\) no se rechaza la hip√≥tesis nula de que el rango de la matriz de cointegraci√≥n es \\(0\\). Por otro lado, para el resto de pa√≠ses menos Ecuador se tiene que dada la hip√≥tesis nula rango de la matriz de cointegraci√≥n es \\(1\\) frente a la alternativa rango de la matriz de cointegraci√≥n es \\(2\\), para un nivel de significancia \\(\\alpha=0.05\\) no se rechaza la hip√≥tesis nula de que el rango de cointegraci√≥n es \\(1\\) con valores \\(p\\) dados en la tabla Tabla¬†2. Finalmente, para Ecuador se tienen que para un nivel de significancia \\(\\alpha=0.05\\) Se rechaza la hip√≥tesis nula de que el rango de cointegraci√≥n es \\(1\\).\n\n\n\nTabla¬†3: Contraste de no Causalidad de Dumitrescu-Hurlin entre Consumo y Renta\n\n\n\n\n\nPa√≠s\n\\(Y\\) no causa \\(C\\)\np-valor\ncausalidad\n\n\n\n\nBolivia\nsi\n0,6174\nno\n\n\nBrasil\nno\n0,0002\nsi\n\n\nChile\nno\n0,0027\nsi\n\n\nColombia\nsi\n0,9225\nno\n\n\nCosta Rica\nsi\n0,2856\nno\n\n\nEcuador\nno\n0,0172\nsi\n\n\nGuatemala\nno\n0,0001\nsi\n\n\nHonduras\nsi\n0,6704\nno\n\n\nM√©xico\nsi\n0,4979\nno\n\n\nNicaragua\nsi\n0,3064\nno\n\n\nPanam√°\nsi\n0,5101\nno\n\n\nParaguay\nsi\n0,8212\nno\n\n\nPer√∫\nno\n0,0002\nsi\n\n\nR. Dominicana\nsi\n0,7106\nno\n\n\n\n\n\n\nEn la tabla Tabla¬†3 se ha realizado un resumen del contraste de no causalidad de Dumitrescu-Hurlin para cada modelo de consumo de cada pa√≠s. Dada la hip√≥tesis nula de \\(Y\\) no causa \\(X\\) frente a la hip√≥tesis alternativa \\(Y\\) causa \\(X\\). tenemos que para los siguientes pa√≠ses: Brasil, Chile, Ecuador, Guatemala, Per√∫. Para un nivel de significancia \\(\\alpha=0.05\\) se rechaza la hip√≥tesis nula de no causalidad con valores \\(p\\) dados en la tabla Tabla¬†3. Para el resto de pa√≠ses no se rechaza la hip√≥tesis nula de no causalidad."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#resultados",
    "href": "proyects/Tesis_JXBS/index.html#resultados",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Resultados",
    "text": "Resultados\nEn la tabla Tabla¬†1 se ha evidenciado que tanto las series de Consumo as√≠ como las series De Renta poseen una ra√≠z unitaria es decir son series integradas de orden uno. Adem√°s se observa que para la mayor√≠a de pa√≠ses el valor \\(p\\) es mayor a \\(0.95\\) es decir tenemos una gran certeza de que nuestras series son no estacionarias, lo cual nos permite realizar el an√°lisis sobre el modelo Ecuaci√≥n¬†15.\nAhora bien, una vez tenemos que las series son no estacionarias en la tabla Tabla¬†2 se ha realizado la prueba de cointegraci√≥n de Johansen para el caso sin constante. De donde, se ha obtenido que los pa√≠ses de Ecuador, Honduras, M√©xico, y Nicaragua no se encuentra cointegrados esto puede ser debido a que para estos pa√≠ses las series tanto de Consumo y Renta al momento de analizar las intervenciones para esta se evidencio que las series tienen ciertas irregularidades que si bien se esperaba que quedaran enterradas en las innovaciones, estas no pueden absorberlas del todo.\nFinalmente, para los pa√≠ses cuyo modelo si esta cointegrado se ha realizado un contraste de causalidad viendo a estos datos como un panel de series apiladas. Es as√≠ que se ha obtenido que para pa√≠ses cuyas intervenciones han sido absorbidas por las innovaciones el modelo propuesto en este trabajo ayuda a mejorar la predicci√≥n del consumo."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#conclusiones",
    "href": "proyects/Tesis_JXBS/index.html#conclusiones",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Conclusiones",
    "text": "Conclusiones\nDado que uno de los objetivos de este trabajo fue revisar los modelos de ciclo de vida y funciones de consumo desde el punto de vista neocl√°sico, para los cuales si bien el lineamiento esta bastante acertado, en este trabajo se ha propuesto un modelo que mejora el modelo neocl√°sico. Esto se evidencia para pa√≠ses en los que los datos no tienen intervenciones de gran impacto es decir que su perturbaciones pueden ser absorbidas por las innovaciones al instante \\(t+1\\). El modelo Ecuaci√≥n¬†15 ayuda a predecir de manera mas eficiente el Consumo; debido a que para estos pa√≠ses el tener informaci√≥n acerca de la renta en cada instante ayuda a predecir el consumo al instante \\(t+1\\). Por otro lado para los pa√≠ses, para los cuales las innovaciones causan impactos demasiado grandes, no se puede decir con certeza que el modelo propuesto no seria bueno. Puesto que, si se lograra mitigar los mencionados impactos en el tratamiento de la data podr√≠a darse cualquier resultado. Desde un punto de vista econom√©trico el modelo propuesto bajo las condiciones impuestas ayuda a establecer una relaci√≥n entre la Renta y el Consumo esta relaci√≥n es del tipo causal de Granger."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#footnotes",
    "href": "proyects/Tesis_JXBS/index.html#footnotes",
    "title": "Estudio Te√≥rico del Comportamiento del Consumo en Latin√°merica y Caribe (Resumen)",
    "section": "Notas",
    "text": "Notas\n\n\nUna ecuaci√≥n de Euler del consumo, a grandes rasgos, es una condici√≥n matem√°tica que describe el comportamiento de una senda √≥ptima de consumo bajo los supuestos de elecci√≥n intertemporal, expectativas racionales y agente representativo, entre otros, (Parker 2007)‚Ü©Ô∏é\nLas series integradas son un caso particular de series no estacionarias. Se dice que una serie temporal \\(x_t\\) es integrada de orden \\(d\\), \\(I(d)\\), cuando es necesario diferenciarla \\(d\\) veces para convertirla en estacionaria (Engle y Granger 1987) ‚Ü©Ô∏é\nEntonces se puede considerar como la varianza cruzada del consumo para una cohorte de individuos.‚Ü©Ô∏é\n\\(u_t\\) innovaciones en la renta permanente‚Ü©Ô∏é\nPor el TCL, \\(\\sum u_t \\overset{d}{\\sim} Normal\\) con \\(u_t\\) independientes , bajo supuestos de regularidad incluso si \\(u_t \\not\\sim Normal\\)‚Ü©Ô∏é\nAl igual que con otros documentos, rechazan en√©rgicamente la suposici√≥n de una perfecta distribuci√≥n del riesgo.‚Ü©Ô∏é\nSeg√∫n (Orazio P. Attanasio y Weber 2010) esto parece indicar que, de alguna manera, en altas frecuencias los choques salariales son absorbidos y no se reflejan en el consumo.‚Ü©Ô∏é\nPor ejemplo, es posible que el ingreso est√© dado por la tasa de salario multiplicada por el n√∫mero de horas trabajadas, donde el n√∫mero de horas es uno de los componentes de \\(z\\)‚Ü©Ô∏é\nEsta simple restricci√≥n impone limitaciones cuantitativamente importantes a la capacidad de suavizar el consumo‚Ü©Ô∏é\nTRAMO significa ‚ÄúTime series Regression with ARIMA noise, Missing values and Outliers‚Äù y SEATS ‚ÄúSignal Extraction in ARIMA Time Series‚Äù. Estos programas (que normalmente se usan juntos) han sido desarrollados por V√≠ctor G√≥mez y Agust√≠n Maravall del Banco de Espa√±a.‚Ü©Ô∏é\nDado el fen√≥meno del ni√±o (1995) el cual fue uno de los desastres naturales mas grandes que ha impactado al Ecuador seria el principio de la bola de nieve que desencadenar√≠a en 1999 con el ya tan conocido ‚ÄúFeriado Bancario‚Äù‚Ü©Ô∏é"
  }
]