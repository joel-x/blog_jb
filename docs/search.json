[
  {
    "objectID": "proyects.html",
    "href": "proyects.html",
    "title": "Proyectos",
    "section": "",
    "text": "Churn Rate (Tasa de Abandono)\n\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n27 ago 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis Roll Rate\n\n\n\nPython\n\n\nRoll Rate\n\n\n\nAnálisis Roll Rate mediante matrices de transición para el dataset Default of Credit Card Clients\n\n\n\nJoel Burbano\n\n\n23 ago 2024\n\n\n\n\n\n\n\n\n\n\n\n\nManejo de ETL’s\n\n\n\nPython\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\nJoel Burbano\n\n\n8 ago 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)\n\n\n\nRenta permanente\n\n\nModelo de Ciclo de vida\n\n\nEcuación de Euler\n\n\nCausalidad\n\n\nModelos de Consumo\n\n\n\nEl estudio del modelo teórico del consumo es un tema de relevancia tanto a nivel de individuo como nivel de hogares. Es así que, en este trabajo se realiza una nueva…\n\n\n\n\n\n\n23 feb 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyects/ETL/index.html",
    "href": "proyects/ETL/index.html",
    "title": "Manejo de ETL’s",
    "section": "",
    "text": "Automatización de Procesos ETL\nEn este proyecto vamos a automatizar un proceso ETL, el mismo que tiene los siguientes pasos\n\nExtraer los: En nuestro caso utilizaremos los datos de “World Happiness report”\nTransformar los datos\nCargar los datos: se los cargarara en una base de SQLite\n\nViaualización del dataset\n\n\n\n  \n\n\n\n\nPythonR\n\n\n\n\nCódigo\n\nimport pandas as pd\nimport sqlite3\nimport logging\n\n\n# Configuración básica de logging\nlogging.basicConfig(filename='etl_processP.log', level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef extract_data():\n    try:\n        df = pd.read_csv('world_happiness_report.csv')\n        logging.info(\"Extracción de datos completada con éxito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la extracción de datos: %s\", e)\n        raise\n\ndef transform_data(df):\n    try:\n        df = df.drop(columns=['Standard Error'])\n        df.iloc[:,2:].fillna(df.iloc[:,2:].mean(), inplace=True)\n        df = df.drop_duplicates()\n        df['Economy (GDP per Capita)'] = df['Economy (GDP per Capita)'] / df['Economy (GDP per Capita)'].max()\n        df['Happiness Level'] = df['Happiness Score'].apply(lambda x: 'High' if x &gt; df['Happiness Score'].mean() else 'Low')\n        logging.info(\"Transformación de datos completada con éxito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la transformación de datos: %s\", e)\n        raise\n\ndef load_data(df):\n    try:\n        conn = sqlite3.connect('world_happiness.db')\n        cursor = conn.cursor()\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS happiness_data (\n            Country TEXT,\n            Region TEXT,\n            Happiness_Rank INTEGER,\n            Happiness_Score REAL,\n            GDP_per_capita REAL,\n            Happiness_Level TEXT\n        )\n        ''')\n        conn.commit()\n        df.to_sql('happiness_data', conn, if_exists='replace', index=False)\n        conn.close()\n        logging.info(\"Carga de datos completada con éxito.\")\n    except Exception as e:\n        logging.error(\"Error durante la carga de datos: %s\", e)\n        raise\n\ndef etl_process():\n    try:\n        logging.info(\"Inicio del proceso ETL.\")\n        data = extract_data()\n        transformed_data = transform_data(data)\n        load_data(transformed_data)\n        logging.info(\"Proceso ETL completado con éxito.\")\n    except Exception as e:\n        logging.critical(\"El proceso ETL falló: %s\", e)\n        raise\n\nif __name__ == \"__main__\":\n    etl_process()\n\n\n\n\n\n\nCódigo\n# Cargar las bibliotecas necesarias\n#install.packages(\"dplyr\")\n#install.packages(\"readr\")\n#install.packages(\"DBI\")\n#install.packages(\"RSQLite\")\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Configuración básica de logging\nlog_file &lt;- \"etl_processR.log\"\nlog_message &lt;- function(level, message) {\n  cat(sprintf(\"%s - %s - %s\\n\", Sys.time(), level, message), file = log_file, append = TRUE)\n}\n\n# Función para extraer datos\nextract_data &lt;- function() {\n  tryCatch({\n    df &lt;- read_csv('world_happiness_report.csv')\n    log_message(\"INFO\", \"Extracción de datos completada con éxito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la extracción de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función para transformar datos\ntransform_data &lt;- function(df) {\n  tryCatch({\n    df &lt;- df %&gt;% \n      select(-`Standard Error`) %&gt;%\n      mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))) %&gt;%\n      distinct() %&gt;%\n      mutate(`Economy (GDP per Capita)` = `Economy (GDP per Capita)` / max(`Economy (GDP per Capita)`, na.rm = TRUE)) %&gt;%\n      mutate(`Happiness Level` = ifelse(`Happiness Score` &gt; mean(`Happiness Score`, na.rm = TRUE), 'High', 'Low'))\n    \n    log_message(\"INFO\", \"Transformación de datos completada con éxito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la transformación de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función para cargar datos en SQLite\nload_data &lt;- function(df) {\n  tryCatch({\n    conn &lt;- dbConnect(SQLite(), dbname = \"world_happiness.db\")\n    \n    dbExecute(conn, '\n    CREATE TABLE IF NOT EXISTS happiness_data (\n      Country TEXT,\n      Region TEXT,\n      Happiness_Rank INTEGER,\n      Happiness_Score REAL,\n      GDP_per_capita REAL,\n      Happiness_Level TEXT\n    )')\n    \n    dbWriteTable(conn, \"happiness_data\", df, overwrite = TRUE, row.names = FALSE)\n    dbDisconnect(conn)\n    log_message(\"INFO\", \"Carga de datos completada con éxito.\")\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la carga de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función principal del proceso ETL\netl_process &lt;- function() {\n  tryCatch({\n    log_message(\"INFO\", \"Inicio del proceso ETL.\")\n    data &lt;- extract_data()\n    transformed_data &lt;- transform_data(data)\n    load_data(transformed_data)\n    log_message(\"INFO\", \"Proceso ETL completado con éxito.\")\n  }, error = function(e) {\n    log_message(\"CRITICAL\", paste(\"El proceso ETL falló:\", e$message))\n    stop(e)\n  })\n}\n\n# Ejecutar el proceso ETL\nif (interactive()) {\n  etl_process()\n}"
  },
  {
    "objectID": "proyects/2024-08-23_Roll_Rate/index.html",
    "href": "proyects/2024-08-23_Roll_Rate/index.html",
    "title": "Análisis Roll Rate",
    "section": "",
    "text": "Carga del dataset\n\n\nCódigo\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\ndf = pd.read_csv(\"UCI_Credit_Card.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nLIMIT_BAL\nSEX\nEDUCATION\nMARRIAGE\nAGE\nPAY_0\nPAY_2\nPAY_3\nPAY_4\n...\nBILL_AMT4\nBILL_AMT5\nBILL_AMT6\nPAY_AMT1\nPAY_AMT2\nPAY_AMT3\nPAY_AMT4\nPAY_AMT5\nPAY_AMT6\ndefault.payment.next.month\n\n\n\n\n0\n1\n20000.0\n2\n2\n1\n24\n2\n2\n-1\n-1\n...\n0.0\n0.0\n0.0\n0.0\n689.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n1\n2\n120000.0\n2\n2\n2\n26\n-1\n2\n0\n0\n...\n3272.0\n3455.0\n3261.0\n0.0\n1000.0\n1000.0\n1000.0\n0.0\n2000.0\n1\n\n\n2\n3\n90000.0\n2\n2\n2\n34\n0\n0\n0\n0\n...\n14331.0\n14948.0\n15549.0\n1518.0\n1500.0\n1000.0\n1000.0\n1000.0\n5000.0\n0\n\n\n3\n4\n50000.0\n2\n2\n1\n37\n0\n0\n0\n0\n...\n28314.0\n28959.0\n29547.0\n2000.0\n2019.0\n1200.0\n1100.0\n1069.0\n1000.0\n0\n\n\n4\n5\n50000.0\n1\n2\n1\n57\n-1\n0\n-1\n0\n...\n20940.0\n19146.0\n19131.0\n2000.0\n36681.0\n10000.0\n9000.0\n689.0\n679.0\n0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\nLimpieza y Preparación de Datos\n\n\nCódigo\nprint(df.isnull().sum())\n\nprint(df.duplicated().sum())\n\n# df = df.drop_duplicates()\n\nprint(df.info())\n\n\n# Las columnas PAY_i con i={0,2,3,...,6} representan el estado de pago del cliente\n# -1: Pago a tiempo, 0: Pago debido, 1: 1-30 diás moroso, .... , 8: 180 + días moroso\n\n# Estandarizamos estas categorias\npay_cols = [\"PAY_\" + str(i) for i in [0,2,3,4,5,6]]\n\n# Reemplazamos valores por categorías más fáciles de interpretar\ndf[pay_cols] = df[pay_cols].replace({-2: 'No deuda',-1: 'Corriente', 0: 'Corriente',\n                                      1: '1-30 días', 2: '31-60 días', 3: '61-90 días',\n                                      4: '91-120 días', 5: '121-150 días',\n                                      6: '151-180 días', 7: '180+ días', 8: '180+ días'})\n                                      \n# Definir el orden deseado de los estados de pago\nestado_pago_orden = ['No deuda', 'Corriente', '1-30 días', '31-60 días', '61-90 días', '91-120 días', '121-150 días', '151-180 días', '180+ días']\nfor col in pay_cols:\n  df[col]=pd.Categorical(df[col], categories= estado_pago_orden, ordered = True)\n\n\nID                            0\nLIMIT_BAL                     0\nSEX                           0\nEDUCATION                     0\nMARRIAGE                      0\nAGE                           0\nPAY_0                         0\nPAY_2                         0\nPAY_3                         0\nPAY_4                         0\nPAY_5                         0\nPAY_6                         0\nBILL_AMT1                     0\nBILL_AMT2                     0\nBILL_AMT3                     0\nBILL_AMT4                     0\nBILL_AMT5                     0\nBILL_AMT6                     0\nPAY_AMT1                      0\nPAY_AMT2                      0\nPAY_AMT3                      0\nPAY_AMT4                      0\nPAY_AMT5                      0\nPAY_AMT6                      0\ndefault.payment.next.month    0\ndtype: int64\n0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 25 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   ID                          30000 non-null  int64  \n 1   LIMIT_BAL                   30000 non-null  float64\n 2   SEX                         30000 non-null  int64  \n 3   EDUCATION                   30000 non-null  int64  \n 4   MARRIAGE                    30000 non-null  int64  \n 5   AGE                         30000 non-null  int64  \n 6   PAY_0                       30000 non-null  int64  \n 7   PAY_2                       30000 non-null  int64  \n 8   PAY_3                       30000 non-null  int64  \n 9   PAY_4                       30000 non-null  int64  \n 10  PAY_5                       30000 non-null  int64  \n 11  PAY_6                       30000 non-null  int64  \n 12  BILL_AMT1                   30000 non-null  float64\n 13  BILL_AMT2                   30000 non-null  float64\n 14  BILL_AMT3                   30000 non-null  float64\n 15  BILL_AMT4                   30000 non-null  float64\n 16  BILL_AMT5                   30000 non-null  float64\n 17  BILL_AMT6                   30000 non-null  float64\n 18  PAY_AMT1                    30000 non-null  float64\n 19  PAY_AMT2                    30000 non-null  float64\n 20  PAY_AMT3                    30000 non-null  float64\n 21  PAY_AMT4                    30000 non-null  float64\n 22  PAY_AMT5                    30000 non-null  float64\n 23  PAY_AMT6                    30000 non-null  float64\n 24  default.payment.next.month  30000 non-null  int64  \ndtypes: float64(13), int64(12)\nmemory usage: 5.7 MB\nNone\n\n\n\n\nCálculo del Roll Rate\n\n\nCódigo\n# Cálculo de las tasas de transición (Roll Rate)\nroll_rates = {}\n\n# Iteramos por pares de meses para calcular la tasa de transición entre estados de pago\nfor i in range(len(pay_cols)-1):\n  transition_matrix = pd.crosstab(df[pay_cols[i]], df[pay_cols[i+1]], normalize= 'index')\n  label = f'Transición de {pay_cols[i]} a {pay_cols[i+1]}'\n  roll_rates[label] = transition_matrix\n  \n# Resumen de tasas de transición\nroll_rates['Transición de PAY_0 a PAY_2']\n\n\n\n\n\n\n\n\nPAY_2\nNo deuda\nCorriente\n1-30 días\n31-60 días\n61-90 días\n91-120 días\n121-150 días\n151-180 días\n180+ días\n\n\nPAY_0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo deuda\n0.928235\n0.069953\n0.000000\n0.001812\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nCorriente\n0.000000\n0.978358\n0.000000\n0.018949\n0.002301\n0.000245\n0.000147\n0.000000\n0.000000\n\n\n1-30 días\n0.331074\n0.166757\n0.007592\n0.453362\n0.029555\n0.008677\n0.001898\n0.000542\n0.000542\n\n\n31-60 días\n0.000000\n0.371579\n0.000000\n0.596550\n0.026622\n0.005249\n0.000000\n0.000000\n0.000000\n\n\n61-90 días\n0.000000\n0.000000\n0.000000\n0.844720\n0.127329\n0.024845\n0.003106\n0.000000\n0.000000\n\n\n91-120 días\n0.000000\n0.000000\n0.000000\n0.000000\n0.763158\n0.197368\n0.039474\n0.000000\n0.000000\n\n\n121-150 días\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.961538\n0.000000\n0.038462\n0.000000\n\n\n151-180 días\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n180+ días\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.321429\n0.678571\n\n\n\n\n\n\n\n\n\nVisualización de Resultados\n\n\nCódigo\n# Visualización de cada matriz de transición utilizando heatmaps\nfor label, matrix in roll_rates.items():\n  plt.figure(figsize= (10,6))\n  sb.heatmap(matrix, annot = True, cmap = \"YlGnBu\", cbar = True)\n  plt.title(label)\n  plt.xlabel('Estado de Pago Siguiente')\n  plt.ylabel('Estado de Pago Actual')\n  plt.show()\n  plt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/Linear_Regression/index.html",
    "href": "posts/Linear_Regression/index.html",
    "title": "Regresión Lineal Simple",
    "section": "",
    "text": "En este post presentamos un ejemplo básico de regresión lineal simple, es decir, el caso de ajustar una línea a datos \\(x,y\\).\nAntes que nada importamos las librerias necesarias\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\nPrimeramente generamos unos puntos para \\(x\\) y para \\(y\\)\n\n\nCódigo\nrng=np.random.RandomState(42)\nx=18*rng.rand(40)\ny=1.5*x-3+rng.rand(40)\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora definimos los hiperparámetros de nuestro modelo\n\n\nCódigo\nLineal=LinearRegression(fit_intercept=True) # La regresión lineal es de la forma y=ax+b, donde b!=0\n\n\nDefinimos la variable \\(X\\)\n\n\nCódigo\nX=x[:,np.newaxis]\nX.shape\n\n\n(40, 1)\n\n\nNota: en este caso como estamos trabajando con datos simulados no es necesario definir \\(y\\) puesto que sklearn si nos permite ingresar ese array\nAhor ajustamos el modelo\n\n\nCódigo\nLineal.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nObservemos el valor del coeficiente (\\(a\\))\n\n\nCódigo\nLineal.coef_\n\n\narray([1.50922213])\n\n\nObservemos el valor del intercepto (\\(b\\))\n\n\nCódigo\nLineal.intercept_\n\n\n-2.60022468062337\n\n\nProbemos el poder predictivo de nuestro modelo\n\n\nCódigo\nxfit=np.arange(17,22,0.5)\nXfit=xfit[:,np.newaxis]\nyfit=Lineal.predict(Xfit)\n\n\nAhora realizemos una visualización\n\n\nCódigo\ndef f(x):\n  return 1.51*x-2.6\n\nplt.scatter(x,y)\nplt.scatter(xfit,yfit,color='red')\nplt.plot(range(-1,22),[f(i) for i in range(-1,22)],color='cyan')\nplt.legend(['Datos modelo','Datos predichos','y=ax+b'])\nplt.show()"
  },
  {
    "objectID": "posts/Deteccion_fraude/index.html",
    "href": "posts/Deteccion_fraude/index.html",
    "title": "Detección de fraude con tarjetas de Crédito",
    "section": "",
    "text": "En el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias recolectadas durante dos días en el mes de de Septiembre del 2013 por European cardholders\nEmpezaremos por importar las librerias necesarias para realizar el análisis\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\nProcedemos a leer los datos con pandas\n\n\nCódigo\n#df: base de datos de las transacciones\ndf=pd.read_csv(\"creditcard.csv.zip\")\n\n\nVerificamos que cantidad de datos tenemos\n\n\nCódigo\ndf.shape\n\n\n(284807, 31)\n\n\nTenemos que existen 31 columnas (variables) y 284807 filas (registros)\nAhora bien procedemos a revisar la calidad de este conjunto de datos\n\n\nCódigo\ndf.isnull().any()\n\n\nTime      False\nV1        False\nV2        False\nV3        False\nV4        False\nV5        False\nV6        False\nV7        False\nV8        False\nV9        False\nV10       False\nV11       False\nV12       False\nV13       False\nV14       False\nV15       False\nV16       False\nV17       False\nV18       False\nV19       False\nV20       False\nV21       False\nV22       False\nV23       False\nV24       False\nV25       False\nV26       False\nV27       False\nV28       False\nAmount    False\nClass     False\ndtype: bool\n\n\nObservamos que no hay variables con datos nulos.\nAhora bien echemos un vistazo a la variable Class la cual contiene la información sobre las transacciones fraudulentas\n\n\nCódigo\ndf[\"Class\"].value_counts()\n\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nNotamos que solamente 492 transacciones son fraudulentas\n\n\nCódigo\ndf['Class'].value_counts(normalize=True)\n\n\nClass\n0    0.998273\n1    0.001727\nName: proportion, dtype: float64\n\n\nes decir solo el \\(0.17\\%\\) de transacciones son fraudulentas\nAhora bien empecemos a intentar predecir\n\n\nCódigo\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCódigo\nX=df.drop(labels='Class',axis=1)\ny=df.loc[:,'Class']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=1, stratify=y)\n\n\nAhora bien, se realiza un análisis exploratorio de los datos\n\n\nCódigo\nX_train['Time'].describe()\n\n\ncount    199364.000000\nmean      94675.212852\nstd       47536.519022\nmin           0.000000\n25%       54039.000000\n50%       84588.500000\n75%      139243.250000\nmax      172792.000000\nName: Time, dtype: float64\n\n\nRealicemos una conversión de la variable Time de segundos a horas para facilitar la interpretación\n\n\nCódigo\nX_train.loc[:,'Time']=X_train.Time/3600\nX_test.loc[:,'Time']=X_test.Time/3600\n\n\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Time'],bins=40,kde=False)\nplt.xlim([0,40])\nplt.xticks(np.arange(0,48,6))\nplt.xlabel('Tiempo despues de la primera transacción (h)')\nplt.ylabel('Conteo')\nplt.title('Tiempo de transacciones')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNotamos que existen dos picos en la gráfica, el primero entre las 10 y 22 primeras horas y el segundo entre las 34 y 40 horas\nAnalicemos la variable Amount\n\n\nCódigo\nX_train['Amount'].describe()\n\n\ncount    199364.000000\nmean         88.659351\nstd         247.240287\nmin           0.000000\n25%           5.637500\n50%          22.000000\n75%          78.000000\nmax       25691.160000\nName: Amount, dtype: float64\n\n\nrealicemos una revisión gráfica\nprimero un histograma\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Amount'],bins=50,kde=False)\nplt.ylabel('Conteo')\nplt.title('Montos de Transacción')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.boxplot(x=X_train[\"Amount\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nObservemos que los datos se encuentran fuertemente sesgados a la derecha. Para asegurarnos calculamos la asimetría\n\n\nCódigo\nX_train['Amount'].skew()\n\n\n16.950540423177653\n\n\n\n\nCódigo\nX_train.head(5)\n\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\n\n\n\n\n105644\n19.340833\n1.135011\n-0.663898\n0.703924\n0.069871\n-0.488154\n1.312078\n-0.897198\n0.463148\n-0.478801\n0.396879\n0.268544\n0.752264\n0.264092\n-0.252977\n0.515272\n-3.211514\n1.648897\n-1.297012\n-2.246958\n-0.677938\n-0.331487\n-0.069644\n0.183987\n-0.618678\n0.089015\n0.521419\n0.086390\n0.004782\n1.00\n\n\n139790\n23.155278\n-1.786262\n1.118886\n1.347969\n-0.379954\n-1.240680\n0.467667\n0.081125\n0.964933\n0.042585\n-1.275754\n-1.871478\n0.375166\n0.692938\n-0.149358\n-0.396145\n0.802805\n-0.405073\n0.153925\n-0.241419\n-0.099266\n-0.047902\n-0.182530\n-0.162509\n-0.405178\n0.512595\n0.299398\n-0.042882\n-0.059130\n141.73\n\n\n158758\n31.034167\n-0.683414\n0.679341\n2.615556\n2.362138\n-0.012716\n0.603826\n0.574245\n-0.679978\n-0.811409\n2.035115\n-0.564418\n-1.407557\n-0.094656\n-0.859411\n1.749530\n0.099132\n-0.035668\n-0.053624\n1.656191\n0.372610\n-0.007167\n0.463597\n-0.243134\n0.084557\n-0.453177\n2.687676\n-1.084269\n-0.511626\n36.19\n\n\n130845\n22.067778\n1.183540\n-0.493000\n0.755202\n-0.963160\n-0.850295\n0.145905\n-0.794616\n0.302199\n1.656943\n-0.939787\n1.101727\n1.138109\n-0.592931\n0.156943\n0.903035\n-0.419096\n-0.207755\n0.403235\n0.614310\n-0.168134\n0.039588\n0.339340\n-0.053125\n-0.298049\n0.423994\n-0.652284\n0.102582\n0.017292\n1.00\n\n\n88908\n17.318056\n1.137583\n0.105478\n0.784402\n1.254973\n-0.600870\n-0.360836\n-0.161727\n0.076092\n0.280587\n0.015787\n1.084154\n1.016011\n-0.666982\n0.250706\n-0.835022\n-0.130522\n-0.216624\n-0.058071\n0.265563\n-0.178887\n-0.195692\n-0.443664\n0.046270\n0.516246\n0.447943\n-0.554949\n0.031821\n0.018177\n7.60"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "",
    "text": "Código\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npd.options.display.max_columns=None #para que se despliegue todas las columnas\n\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom matplotlib import cm\nc4=cm.get_cmap('Set3')"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Frecuencia de los tipos de acciedentes",
    "text": "Frecuencia de los tipos de acciedentes\n\n\nCódigo\nsb.countplot(x=df['Collision Type'],palette='Set3')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Frecuencia de tipos de Accidentes\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que existe una mayor cantidad de accidentes entre dos vehículos, por otro lado la cantidad de accidentes de bicicletas es la menor."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Accidentes ocurridos entre semana vs fin de semana",
    "text": "Accidentes ocurridos entre semana vs fin de semana\n\n\nCódigo\nsb.countplot(x=df['Weekend?'],palette='Accent')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Entre semana vs Fin de semana\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el Fin de semana es cuando mas ocurren accidentes."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Porcentaje de lesiones por Categoria",
    "text": "Porcentaje de lesiones por Categoria\n\n\nCódigo\nles_val=df['Injury Type'].value_counts()\nles_val\n\n\nInjury Type\nNo injury/unknown     41603\nNon-incapacitating    11136\nIncapacitating         1089\nFatal                   115\nName: count, dtype: int64\n\n\n\n\nCódigo\nplt.pie(les_val,labels=les_val.index,startangle=30,shadow=True,autopct='%1.1f%%',rotatelabels=30,explode=(0.1,0.1,0.1,0.1),colors=[c4(0.9),c4(0.2),c4(0.3),c4(0.6)])\nplt.title('Porcentaje de lesiones por Categoria')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el \\(0.2\\%\\) de accidentes son Fatales. Por otro lado algo que sería de mucho interes saber de cuantos accidentes no se conoce la lesión ocasionada puesto que la probabilidad de no tener lesiones es muy baja."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Motivo mas comun por el cual suceden accidentes",
    "text": "Motivo mas comun por el cual suceden accidentes\nPrimero indagaremos cuantos Factores Primarios existen en la data\n\n\nCódigo\ndf['Primary Factor'].nunique()\n\n\n55\n\n\nDado que existen \\(55\\) Factores Primarios nos quedaremos con el top 20\n\n\nCódigo\npfdf=df['Primary Factor'].value_counts().head(20)\n\n\n\n\nCódigo\nfig=plt.figure(figsize=(10,10))\naxis=fig.add_axes([1,1,1,1])\nsb.swarmplot(x=pfdf,y=pfdf.index,ax=axis)\nfor i,j in enumerate(pfdf):\n  axis.text(j,i,j)\nplt.xlabel('Ocurrencia')\nplt.title('Motivos principales por los que ocurren accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Top 30 lugares mas frecuente donde ocurren accidentes",
    "text": "Top 30 lugares mas frecuente donde ocurren accidentes\n\n\nCódigo\nldf=df['Reported_Location'].value_counts().head(30)\n\n\n\n\nCódigo\nfig1=plt.figure()\naxis1=fig1.add_axes([1,1,1,1])\nsb.barplot(x=ldf,y=ldf.index,ax=axis1,palette=\"viridis\")\nfor i,j in enumerate(ldf):\n  axis1.text(j,i,j,va='top')\naxis1.set_xlabel('Frecuencia')\naxis1.set_title('Lugares con mayor frecuencia de Accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-años",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-años",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Tipos de Colisiones en diferentes años",
    "text": "Tipos de Colisiones en diferentes años\n\n\nCódigo\naños=df.groupby('Year')\nkeys=años.groups.keys()\n\n\n\n\nCódigo\ninfobox=[]\nfor i in range(2003,2016):\n  infobox.append(años.get_group(i)['Collision Type'].value_counts())\n\n\n\n\nCódigo\nc2=cm.get_cmap('terrain')\n\n\n\n\nCódigo\nfrom IPython.display import display, Markdown\n\n\nColisiones por Año\n\n2003200420052006200720082009201020112012201320142015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#gráfico-dinamico",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#gráfico-dinamico",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Gráfico Dinamico",
    "text": "Gráfico Dinamico\n\n\nCódigo\nimport plotly.express as px\n\n\n\n\nCódigo\npx.histogram(df,x='Collision Type', \nanimation_frame=df['Year'].sort_values(ascending=True),\ncolor='Collision Type', title='Accidentes por Tipo de Colisión'\n)"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "",
    "text": "Las medidas con fórmulas temporales en DAX (Data Analysis Expressions) y Power BI son herramientas poderosas que permiten a los analistas de datos realizar cálculos avanzados en base a períodos de tiempo. Estas medidas son esenciales para el análisis de series temporales, permitiendo comparar datos entre diferentes períodos y obtener insights sobre tendencias y patrones. En este artículo, exploraremos qué son estas medidas, para qué sirven, y veremos un ejemplo práctico utilizando Power BI."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 1: Cargar los Datos en Power BI",
    "text": "Paso 1: Cargar los Datos en Power BI\n\nAbre Power BI Desktop.\nHaz clic en “Obtener datos” y selecciona “Archivo CSV”\nCarga el dataset “Superstore Sales”"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 2: Crear un Modelo de Datos",
    "text": "Paso 2: Crear un Modelo de Datos\n\nAbre Transformar datos y verificamos que las columnas tengan el formato adecuado\nCrea una tabla de fechas “Calendario”\n\n\n\ndax\n\ncalendario = CALENDAR(MIN(Superstore[Order Date]), MAX(Superstore[Order Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 3: Definir Relaciones",
    "text": "Paso 3: Definir Relaciones\n\nEn la vista “Modelo”, crea una relación entre la columna “Order Date” de la tabla de Superstore y la columna “Date” de la tabla calendario."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 4: Crear Medidas Temporales",
    "text": "Paso 4: Crear Medidas Temporales\n\nVentas Totales (Total Sales):\n\n\n\ndax\n\ntotal_sales = SUM(Superstore[Sales])\n\n\nVentas Acumuladas Año a la Fecha (YTD Sales):\n\n\n\ndax\n\nYTD_Sales = TOTALYTD([total_sales],calendario[Date])\n\n\nVentas Acumuladas Mes a la Fecha (MTD Sales):\n\n\n\ndax\n\nMTD_Sales = TOTALMTD([total_sales], calendario[Date])\n\n\nVentas del Mismo Período el Año Pasado (Same Period Last Year Sales):\n\n\n\ndax\n\nSales_last_year = CALCULATE( [total_sales], SAMEPERIODLASTYEAR(calendario[Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Ventajas",
    "text": "Ventajas\n\nFacilidad de Uso: Power BI y DAX facilitan la creación y manipulación de medidas temporales.\nFlexibilidad: Permite realizar cálculos complejos y personalizados según las necesidades del análisis.\nVisualización Interactiva: Power BI ofrece herramientas visuales que permiten explorar datos de manera interactiva."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Desventajas",
    "text": "Desventajas\n\nCurva de Aprendizaje: Puede ser desafiante para principiantes sin experiencia previa en DAX.\nRendimiento: Las medidas complejas pueden afectar el rendimiento del informe, especialmente con grandes volúmenes de datos."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html",
    "href": "posts/2024-03-11-regresion-logistica/index.html",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística es una técnica de modelado estadístico utilizada para predecir el resultado de una variable categórica basada en una o más variables independientes. Es especialmente útil cuando queremos clasificar observaciones en dos o más clases. A diferencia de la regresión lineal, que predice valores continuos, la regresión logística predice probabilidades de pertenencia a una categoría.\n\n\nLa regresión logística utiliza la función logística, también conocida como función sigmoide, para transformar la salida de una combinación lineal de las variables independientes. La función sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa fórmula de la función sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinación lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)\n\n\n\n\n\n\n\n\n\n\n\nImaginemos que queremos predecir si un correo electrónico es spam o no, usando características como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nCódigo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nCódigo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nCódigo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nCódigo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nCódigo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nCódigo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nCódigo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nCódigo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nCódigo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nCódigo\n# Matriz de consfusión\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nCódigo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nCódigo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo.\n\n\n\n\n\n\n\nVentajas\n\nInterpretabilidad: Es fácil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros métodos complejos\n\nDesventajas\n\nLinealidad: Asume una relación lineal entre las variables independientes y la probabilidad de la clase.\nLimitación: La versión básica está diseñada para clasificación binaria.\n\n\n\n\nLa regresión logística es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificación. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagnóstico médico hasta la detección de fraude."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#comó-funciona",
    "href": "posts/2024-03-11-regresion-logistica/index.html#comó-funciona",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística utiliza la función logística, también conocida como función sigmoide, para transformar la salida de una combinación lineal de las variables independientes. La función sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa fórmula de la función sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinación lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)"
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-práctico",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-práctico",
    "title": "Regresión Logística",
    "section": "",
    "text": "Imaginemos que queremos predecir si un correo electrónico es spam o no, usando características como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nCódigo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nCódigo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nCódigo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nCódigo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nCódigo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nCódigo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nCódigo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nCódigo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nCódigo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nCódigo\n# Matriz de consfusión\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nCódigo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nCódigo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluación",
    "href": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluación",
    "title": "Regresión Logística",
    "section": "",
    "text": "En general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "title": "Regresión Logística",
    "section": "",
    "text": "Ventajas\n\nInterpretabilidad: Es fácil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros métodos complejos\n\nDesventajas\n\nLinealidad: Asume una relación lineal entre las variables independientes y la probabilidad de la clase.\nLimitación: La versión básica está diseñada para clasificación binaria."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#conclusión",
    "href": "posts/2024-03-11-regresion-logistica/index.html#conclusión",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificación. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagnóstico médico hasta la detección de fraude."
  },
  {
    "objectID": "posts/2023-12-20-Clustering/index.html",
    "href": "posts/2023-12-20-Clustering/index.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "En este post veremos un modelos de cluster\nComo ya es costumbre primero importamos las librerias necesarias\n\n\nCódigo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\n\nImportemos los datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nCódigo\nGauss=GaussianMixture(n_components=3,\ncovariance_type='full')\n\n\nAjustamos el modelo\n\n\nCódigo\nGauss.fit(X_iris)\n\n\nGaussianMixture(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=3)\n\n\nDeterminamos las etiquetas\n\n\nCódigo\ny_gmm=Gauss.predict(X_iris)\n\n\ngraficamente\n\n\nCódigo\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\nCódigo\niris['cluster']=y_gmm\nsb.lmplot(data=iris,x='sepal_length',y='petal_length',hue='species',col='cluster',fit_reg=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joel Burbano",
    "section": "",
    "text": "Ingeniero Matemático | Analista de Datos | Científico de Datos"
  },
  {
    "objectID": "index.html#perfil",
    "href": "index.html#perfil",
    "title": "Joel Burbano",
    "section": "Perfil",
    "text": "Perfil\nIngeniero Matemático con sólida experiencia en análisis de datos estadística y ciencia de datos. Especializado en la recolección, procesamiento y modelado de datos para facilitar la toma de decisiones estratégicas. Experto en el desarrollo de modelos estadísticos y econométricos, así como en la creación de dashboards interactivos y reportes en Power BI. Amplia experiencia en el uso de herramientas avanzadas como Python y R para realizar análisis predictivo y exploratorio. Apasionado por resolver problemas complejos mediante el análisis de datos y aportar soluciones innovadoras basadas en evidencia."
  },
  {
    "objectID": "index.html#competencias",
    "href": "index.html#competencias",
    "title": "Joel Burbano",
    "section": "Competencias",
    "text": "Competencias\nR Python SQL MongoDB Power BI Excel(VBA)"
  },
  {
    "objectID": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "href": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "title": "Curso Pronóstico Ventas",
    "section": "",
    "text": "Curso Pronoóstico Ventas"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          Título\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntegración de Python en Power BI\n\n\n\nPython\n\n\nPower BI\n\n\n\nTutorial para Integración de Python en Power BI, aprende a cargar datos desde scripts de python y a crear visualizaciones avanzadas.\n\n\n\nJoel Burbano\n\n\n10 feb 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas con Fórmulas Temporales DAX y Power BI\n\n\n\nPower BI\n\n\nDAX\n\n\n\n\n\n\n\nJoel Burbano\n\n\n25 may 2024\n\n\n\n\n\n\n\n\n\n\n\n\nK Vecinos más cercanos (KNN)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje Supervisado\n\n\nKNN\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Componentes Principales (PCA)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión Logística\n\n\n\nPython\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n11 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAccidentes automovilisticos Conjunto de datos EDA\n\n\n\nPython\n\n\n\nEn este apartado vamos a analizar datos de accidentes automovilisticos\n\n\n\nJoel burbano\n\n\n13 ene 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Mixture Models\n\n\n\nPython\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalisis de Componentes Principales\n\n\n\nPython\n\n\nPCA\n\n\nReducción de dimensiones\n\n\nAprendizaje No Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión Lineal Simple\n\n\n\nPython\n\n\nAprendizaje Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Clasificación\n\n\n\nAprendizaje supervisado\n\n\nPython\n\n\nNaive Bayes\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDetección de fraude con tarjetas de Crédito\n\n\n\nPython\n\n\n\nEn el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias.\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHiperparametros y Modelos de Validación\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTratando los datos Keane\n\n\n\nEconometría\n\n\nPython\n\n\n\nEn este post se realizara un analisis de los datos keane\n\n\n\nJoel Burbano\n\n\n7 dic 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "👋 ¡Hola! Soy Joel, graduado de Ingeniero Matemático en la Facultad de Ciencias de la Escuela Politécnica Nacional.\n🎓 Durante mi etapa universitaria he adquirido las bases para gestionar modelos de riesgo, modelos econométricos, modelos estadísticos, modelos de programación entera. Así también, he adquirido las bases de matemática actuarial, estadística matemática, investigación operativa. Además, me he capacitado en el manejo de lenguajes de programación tales como: C++, Matlab, R, Python, y también manejo de paquetes estadísticos Statgraphics y Gretl.\n📝 El desarrollo de mi trabajo de titulación se enfoca en desarrollar un modelo de consumo a partir de la Hipótesis de Renta Permanente de Friedman (Novel Economia,1976) y El Ciclo de Vida de Modigliani.\n🎯Mi objetivo es seguir desarrollando mis habilidades en el área estadística, econométrica, actuarial y ciencia de datos, por lo que me encuentro altamente interesado en trabajar en las mencionadas áreas."
  },
  {
    "objectID": "certific.html",
    "href": "certific.html",
    "title": "Certificados",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          Título\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDimplomado Técnico Big Data\n\n\n\nR\n\n\nPython\n\n\nMySQL\n\n\nBI\n\n\nPower BI\n\n\nML\n\n\nDL\n\n\nDBA\n\n\n\n\n\n\n\n\n\n\n12 jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Interactivo\n\n\n\nSQL\n\n\nConsultas\n\n\nCombinanciones y Subconsultas\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR para data scientist avanzado\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\nSeries Temporales\n\n\nArboles de decisión\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython para data science y big data esencial\n\n\n\nPython\n\n\nData Science\n\n\nBig Data\n\n\npandas\n\n\nPySpark\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n24 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos profesionales del análisis de datos, por Microsfot y LinkedIn\n\n\n\nAnalítica de datos\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n16 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprende data science Conceptos básicos\n\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n12 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducción a las habilidades profesionales en análisis de datos\n\n\n\nAnálisis de datos\n\n\nAnalítica de datos\n\n\nAptitudes para carreras tecnologicas\n\n\nExcel\n\n\nPower BI\n\n\n\n\n\n\n\n\n\n\n5 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Completo de Data Science en Python Desde Cero [2023]\n\n\n\nPython\n\n\nData Science\n\n\npandas\n\n\nsklearn\n\n\nstatsmodels\n\n\nBeautifulSoup\n\n\n\n\n\n\n\n\n\n\n22 jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Power BI\n\n\n\nPower BI\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n25 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Tableau Desktop\n\n\n\nTableau\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n19 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso PHP y MySQL\n\n\n\nMySQL\n\n\nPHP\n\n\n\n\n\n\n\n\n\n\n16 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Pronóstico Ventas\n\n\n\nPronósticos Ventas\n\n\nExcel\n\n\n\n\n\n\n\n\n\n\n16 abr 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso teórico práctico El ambiente de Programación R en el ámbito de la investigación científica\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\ngit\n\n\nRmarkdown\n\n\n\n\n\n\n\n\n\n\n3 sept 2020\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "href": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "title": "Curso PHP y MySQL",
    "section": "",
    "text": "Curso PHP y MySQL"
  },
  {
    "objectID": "certificados/25-05-2023_Curso_power-BI/index.html",
    "href": "certificados/25-05-2023_Curso_power-BI/index.html",
    "title": "Curso Power BI",
    "section": "",
    "text": "Curso Power BI"
  },
  {
    "objectID": "contacto.html#email-joelburbanooutlook.com",
    "href": "contacto.html#email-joelburbanooutlook.com",
    "title": "Contactos",
    "section": "Email : joelburbano@outlook.com",
    "text": "Email : joelburbano@outlook.com"
  },
  {
    "objectID": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "href": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "title": "Tratando los datos Keane",
    "section": "",
    "text": "En este post abordaremos un poco los datos keane obtenidos de Gretl\nEmpezaremos por importar las librerías necesarias\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt \nimport seaborn as sb\n\n\n\nA continuación nos disponemos a visualizar los datos\n\n\nCódigo\nkeane=pd.read_csv(\"keane.csv\")\nkeane=pd.DataFrame(keane)\n\n\n\n\nCódigo\nkeane.head(5)\n\n\n\n\n\n\n\n\n\nid\nnumyrs\nyear\nchoice\nwage\neduc\nexpwc\nexpbc\nexpser\nmanuf\nblack\nlwage\nenroll\nemploy\nattrit\nexper\nexpersq\nstatus\n\n\n\n\n0\n1\n9\n81\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n1\n1\n9\n82\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n2\n1\n9\n83\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n3\n1\n9\n84\n1.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n1\n0\n0\n0\n0\n1.0\n\n\n4\n1\n9\n85\n2.0\nNaN\n11\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n\n\n\n\n\n\nCrearemos etiquetas para las observaciones de acuerdo a “choice” estudiante=1, hogar=2, cualificado=3, no-cualificado=4, servicio=5\n\n\nCódigo\nkeane[\"choice\"]=np.where(keane[\"choice\"]==1,\"estudiante\",\n         np.where(keane[\"choice\"]==2,\"hogar\",\n                  np.where(keane[\"choice\"]==3,\"cualificado\",\n                           np.where(keane[\"choice\"]==4,\"no-cualificado\",\"servicio\"))))\n\n\n\nProcedemos a gráficar la evolución de salarios separado por color de piel\n\n\nCódigo\nsb.scatterplot(data=keane,x=\"year\",y=\"wage\",hue=\"black\",style=\"black\",style_order=[1,0])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nEn esta gráfica evidenciamos que a lo largo de los años aumenta la discriminación.\nVisualicemos lo siguiente: seleccionando sólo las personas que trabajan se realizara un gráfico de la evolución de los salarios separados por la variable choice\n\n\nCódigo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\", y=\"wage\",hue=\"choice\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nSe observa que la terciarización de la economía ha aumentado las diferencias entre trabajadores cualificados y no cualificados, así como, entre servicio e industria.\nAhora procedamos al análisis de la variable educ para ello primero la Codificaremos de acuerdo a educación básica=1, educación media=2, y educación superior=3.\n\n\nCódigo\nkeane[\"educCode\"]=np.where(keane[\"educ\"]&lt;=9,1,np.where(keane[\"educ\"]&lt;=12,2,3))\n\n\nSeleccionando solo las personas que trabajan tenemos lo siguiente:\n\n\nCódigo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\",y=\"wage\",hue=\"educCode\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2023-12-20-PCA/index.html",
    "href": "posts/2023-12-20-PCA/index.html",
    "title": "Analisis de Componentes Principales",
    "section": "",
    "text": "En este post se pretende reducir dimensiones de una cantidad de datos es decir encontrar una transformación en la cuál se represente de mejor manera los datos reduciendo así su dimensión\nComo ya es costumbre primero importamos las librerías necesarias\n\n\nCódigo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n\nImportemos los datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nCódigo\nACP=PCA(n_components=2)\n\n\nAjustamos el modelo\n\n\nCódigo\nACP.fit(X_iris)\n\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2)\n\n\ntransformamos la data\n\n\nCódigo\nX_2d=ACP.transform(X_iris)\n\n\nRealizamos una representación gráfica\n\n\nCódigo\n#para facilitarnos el trabajo vamos a extender sobre el data iris los nuevos ejes encontrados\niris['PCA1']=X_2d[:,0]\niris['PCA2']=X_2d[:,1]\nsb.lmplot(x=\"PCA1\",y=\"PCA2\",hue='species',data=iris,fit_reg=False)"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html",
    "href": "posts/2024-04-10-KNN/index.html",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos Más Cercanos (KNN) es uno de los métodos más simples y efectivos para clasificación y regresión en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los parámetros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#introducción",
    "href": "posts/2024-04-10-KNN/index.html#introducción",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos Más Cercanos (KNN) es uno de los métodos más simples y efectivos para clasificación y regresión en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los parámetros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#qué-es-k-vecinos-más-cercanos",
    "href": "posts/2024-04-10-KNN/index.html#qué-es-k-vecinos-más-cercanos",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "¿Qué es K Vecinos Más Cercanos?",
    "text": "¿Qué es K Vecinos Más Cercanos?\nKNN es un algoritmo de aprendizaje supervisado que clasifica una muestra en función de las categorías de sus \\(K\\) vecinos más cercanos. Para la clasificación, asigna la categoría más común entre sus vecinos, y para la regresión, predice el valor promedio de los vecinos más cercanos.\n\nFuncionamiento del KNN:\n\nSelección de K: Elige el número de vecinos más cercanos (\\(K\\)).\nDistancia: Calcula la distancia entre la muestra y todas las demás muestras en el conjunto de datos (comúnmente usando la distancia euclidiana).\nVecinos: Identifica los \\(K\\) vecinos más cercanos a la muestra.\nVotación: Asigna la clase más común (clasificación) o el promedio de los valores (regresión) entre los \\(K\\) vecinos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ejemplo",
    "href": "posts/2024-04-10-KNN/index.html#ejemplo",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\nPara ilustrar el uso de KNN, usaremos el dataset Iris.\n\nPaso 1: Instalación de Librerías\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd \npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nCódigo\nlibrary(class)\nlibrary(caret)\nlibrary(GGally)\nlibrary(ggplot2)\n\n\n\n\n\n\n\nPaso 2: Cargar y Preprocesar los Datos\n\nPythonR\n\n\n\n\nCódigo\ndata = pd.read_csv(\"Iris.csv\")\ndata = data.drop(['Id'], axis = 1)\nX = data.drop(['Species'], axis = 1)\ny = data['Species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n\n\n\n\n\nCódigo\ndata &lt;- read.csv(\"Iris.csv\")\ndata &lt;- data[,-1]\n\nany(is.na(data))\n\n\n[1] FALSE\n\n\nCódigo\nX &lt;- data[,-5]\ny &lt;- data$Species\n\nset.seed(42)\ntrainIndex &lt;- createDataPartition(y, p = .7, list = FALSE)\nX_train &lt;- X[trainIndex,]\nX_test &lt;- X[-trainIndex,]\ny_train &lt;- y[trainIndex]\ny_test &lt;- y[-trainIndex]\n\npreProcValues &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train &lt;- predict(preProcValues, X_train)\nX_test &lt;- predict(preProcValues, X_test)\n\n\n\n\n\n\n\nPaso 3: Entrenar el Modelo\n\nPythonR\n\n\n\n\nCódigo\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\n\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n\n\n\nCódigo\nknn_model &lt;- knn(train = X_train, test = X_test, cl = y_train, k = 3)\n\n\n\n\n\n\n\nPaso 4: Evaluar el Modelo\n\nPythonR\n\n\n\n\nCódigo\ny_pred = knn.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n\nAccuracy: 0.9111111111111111\n\n\nCódigo\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n\n\nConfusion Matrix:\n [[19  0  0]\n [ 0  9  4]\n [ 0  0 13]]\n\n\nCódigo\nprint('Classifcation Report:\\n', classification_report(y_test, y_pred))\n\n\nClassifcation Report:\n                  precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        19\nIris-versicolor       1.00      0.69      0.82        13\n Iris-virginica       0.76      1.00      0.87        13\n\n       accuracy                           0.91        45\n      macro avg       0.92      0.90      0.89        45\n   weighted avg       0.93      0.91      0.91        45\n\n\n\n\n\n\nCódigo\nconfusion &lt;- confusionMatrix(knn_model, as.factor(y_test))\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n                 Reference\nPrediction        Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              14               0              0\n  Iris-versicolor           1              14              2\n  Iris-virginica            0               1             13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9111          \n                 95% CI : (0.7878, 0.9752)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.467e-16       \n                                          \n                  Kappa : 0.8667          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Iris-setosa Class: Iris-versicolor\nSensitivity                      0.9333                 0.9333\nSpecificity                      1.0000                 0.9000\nPos Pred Value                   1.0000                 0.8235\nNeg Pred Value                   0.9677                 0.9643\nPrevalence                       0.3333                 0.3333\nDetection Rate                   0.3111                 0.3111\nDetection Prevalence             0.3111                 0.3778\nBalanced Accuracy                0.9667                 0.9167\n                     Class: Iris-virginica\nSensitivity                         0.8667\nSpecificity                         0.9667\nPos Pred Value                      0.9286\nNeg Pred Value                      0.9355\nPrevalence                          0.3333\nDetection Rate                      0.2889\nDetection Prevalence                0.3111\nBalanced Accuracy                   0.9167\n\n\n\n\n\n\n\nPaso 5: Visualización de Resultados\n\nPythonR\n\n\n\n\nCódigo\nsb.pairplot(data, hue = 'Species')\n\n\n\n\n\n\n\n\n\nCódigo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nggpairs(data, aes(color = Species, alpha = 0.5)) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "href": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nSimplicidad: Fácil de entender e implementar.\nNo Paramétrico: No asume ninguna distribución de datos.\n\nDesventajas\n\nCosto Computacional: Requiere almacenar todos los datos de entrenamiento y calcular distancias para cada predicción.\nSensibilidad al Ruido: Afectado por la escala y por valores atípicos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#conclusión",
    "href": "posts/2024-04-10-KNN/index.html#conclusión",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Conclusión",
    "text": "Conclusión\nEl algoritmo de K Vecinos Más Cercanos es una herramienta poderosa y fácil de usar para tareas de clasificación y regresión. A través de ejemplos prácticos en Python y R, hemos demostrado cómo implementar y evaluar este modelo en la práctica. Aunque tiene algunas limitaciones, KNN sigue siendo una opción valiosa para muchos problemas en ciencia de datos."
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html",
    "href": "posts/2024-07-12_Cross_validation/index.html",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "¿Sé puede procesar datos con un script de python en power BI?\nLa respuesta es sí, veamoslo con el siguiente ejemplo sencillo\n!()imag1.jpg"
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#validación-cruzada-k-fold",
    "href": "posts/2024-07-12_Cross_validation/index.html#validación-cruzada-k-fold",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Validación cruzada K-Fold",
    "text": "Validación cruzada K-Fold\nEn la validación cruzada K-Fold, el conjunto de datos se divide en k subconjuntos de igual tamaño. El modelo se entrena k veces, cada vez utilizando k-1 folds para entrenar y el fold restante para probar. El error de validación cruzada se calcula como el promedio de los errores en cada iteración."
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#leave-one-out-cross-validation-loocv",
    "href": "posts/2024-07-12_Cross_validation/index.html#leave-one-out-cross-validation-loocv",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Leave One Out Cross Validation (LOOCV)",
    "text": "Leave One Out Cross Validation (LOOCV)\nEn LOOCV, se utiliza un solo punto de datos para el conjunto de prueba y el resto para el conjunto de entrenamiento. Este proceso se repite para cada punto de datos, resultando en un número de iteraciones igual al número de observaciones en el conjunto de datos."
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#validación-cruzada-estratificada",
    "href": "posts/2024-07-12_Cross_validation/index.html#validación-cruzada-estratificada",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Validación Cruzada Estratificada",
    "text": "Validación Cruzada Estratificada\nEste método es similar a la validación cruzada K-Fold, pero los folds se crean de manera que la proporción de cada clase objetivo sea la misma en cada fold. Esto es especialmente útil en conjuntos de datos desbalanceados."
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#paso-1-carga-de-información",
    "href": "posts/2024-07-12_Cross_validation/index.html#paso-1-carga-de-información",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Paso 1: Carga de Información",
    "text": "Paso 1: Carga de Información\n\nPythonR\n\n\n\n\nCódigo\nimport pyreadr\nimport pandas as pd\n\ndf = pyreadr.read_r(\"house_prices.RData\")\n\ndata = df['house_prices'] \n\nprint(data.head())\n\n\n   Index  ... Plot Area\n0    0.0  ...       NaN\n1    1.0  ...       NaN\n2    2.0  ...       NaN\n3    3.0  ...       NaN\n4    4.0  ...       NaN\n\n[5 rows x 21 columns]\n\n\nCódigo\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 187531 entries, 0 to 187530\nData columns (total 21 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   Index              187531 non-null  float64\n 1   Title              187531 non-null  object \n 2   Description        184508 non-null  object \n 3   Amount(in rupees)  187531 non-null  object \n 4   Price (in rupees)  169866 non-null  float64\n 5   location           187531 non-null  object \n 6   Carpet Area        106858 non-null  object \n 7   Status             186916 non-null  object \n 8   Floor              180454 non-null  object \n 9   Transaction        187448 non-null  object \n 10  Furnishing         184634 non-null  object \n 11  facing             117298 non-null  object \n 12  overlooking        106095 non-null  object \n 13  Society            77853 non-null   object \n 14  Bathroom           186668 non-null  float64\n 15  Balcony            138574 non-null  float64\n 16  Car Parking        84174 non-null   object \n 17  Ownership          122014 non-null  object \n 18  Super Area         79846 non-null   object \n 19  Dimensions         0 non-null       object \n 20  Plot Area          0 non-null       object \ndtypes: float64(4), object(17)\nmemory usage: 30.0+ MB\nNone\n\n\n\n\n\n\nCódigo\nload(\"house_prices.RData\")\ndata &lt;- house_prices\nhead(data)\n\n\n  Index\n1     0\n2     1\n3     2\n4     3\n5     4\n6     5\n                                                                                Title\n1 1 BHK Ready to Occupy Flat for sale in Srushti Siddhi Mangal Murti Complex Bhiwandi\n2                     2 BHK Ready to Occupy Flat for sale in Dosti Vihar Pokhran Road\n3            2 BHK Ready to Occupy Flat for sale in Sunrise by Kalpataru Kolshet Road\n4                                         1 BHK Ready to Occupy Flat for sale Kasheli\n5     2 BHK Ready to Occupy Flat for sale in TenX Habitat Raymond Realty Pokhran Road\n6                         1 BHK Ready to Occupy Flat for sale in Virat Aangan Titwala\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Description\n1                                                                                                                                                                             Bhiwandi, Thane has an attractive 1 BHK Flat for sale. The property is ideally located in a strategic location in Srushti Siddhi Mangal Murti Complex township. This flat for resale is a choice property. This apartment ready to move in the Bhiwandi is available for an attractive price of INR 42 Lac. You will find it unfurnished.\n2 One can find this stunning 2 BHK flat for sale in Pokhran Road, Thane. It enjoys an excellent location within the Dosti Vihar. This flat for resale is a choice property. This ready to move flat in Pokhran Road can be availed at a reasonable price of INR 98 Lac. This semi-furnished flat is strategically designed with all the amenities to enhance the living experience. The property is strategically placed near prominent places as near singhaniya school which make for the smooth living of residents.\n3                           Up for immediate sale is a 2 BHK apartment in Kolshet Road, Thane. Don't miss this bargain flat for sale. Situated in the Sunrise By Kalpataru township, it has a prime location. This flat for resale has a desirable location. You can buy this ready to move flat in Kolshet Road at a reasonable price of INR 1.40 Cr. This unfurnished flat is strategically designed with all the amenities to enhance the living experience. Landmarks near the apartment include pokhran road no 2.\n4                                                                                                                                                                                                                                                                                              This beautiful 1 BHK Flat is available for sale in Kasheli, Thane. This flat for resale has a desirable location. This ready to move flat is offered at an economical price of INR 25 Lac. You will find it unfurnished.\n5                                                                                                        This lovely 2 BHK Flat in Pokhran Road, Thane is up for sale. This flat is situated in the Tenx Habitat Raymond Realty township and is equipped with premium facilities. This flat is an attractive property for resale. You can buy this ready to move flat in Pokhran Road at a reasonable price of INR 1.60 Cr. You will find it unfurnished. Some of the landmarks in the vicinity include pokhran road 2.\n6                                                                                              Creatively planned and constructed is a 1 BHK flat for sale in Titwala, Thane. It is housed in the well-planned Virat Aangan township in an advantageous location. This flat is available as a resale property. You can buy this ready to move flat in Titwala at a reasonable price of INR 45 Lac. The flat is uniquely designed to enhance the living style. It is unfurnished, studded with all the basic facilities.\n  Amount(in rupees) Price (in rupees) location Carpet Area        Status\n1            42 Lac              6000    thane    500 sqft Ready to Move\n2            98 Lac             13799    thane    473 sqft Ready to Move\n3           1.40 Cr             17500    thane    779 sqft Ready to Move\n4            25 Lac                NA    thane    530 sqft Ready to Move\n5           1.60 Cr             18824    thane    635 sqft Ready to Move\n6            45 Lac              6618    thane        &lt;NA&gt; Ready to Move\n         Floor Transaction     Furnishing facing            overlooking\n1 10 out of 11      Resale    Unfurnished   &lt;NA&gt;                   &lt;NA&gt;\n2  3 out of 22      Resale Semi-Furnished   East            Garden/Park\n3 10 out of 29      Resale    Unfurnished   East            Garden/Park\n4   1 out of 3      Resale    Unfurnished   &lt;NA&gt;                   &lt;NA&gt;\n5 20 out of 42      Resale    Unfurnished   West Garden/Park, Main Road\n6   2 out of 7      Resale    Unfurnished   East Garden/Park, Main Road\n                              Society Bathroom Balcony Car Parking\n1 Srushti Siddhi Mangal Murti Complex        1       2        &lt;NA&gt;\n2                         Dosti Vihar        2      NA      1 Open\n3                Sunrise by Kalpataru        2      NA   1 Covered\n4                                &lt;NA&gt;        1       1        &lt;NA&gt;\n5         TenX Habitat Raymond Realty        2      NA   1 Covered\n6                        Virat Aangan        1       1        &lt;NA&gt;\n             Ownership Super Area Dimensions Plot Area\n1                 &lt;NA&gt;       &lt;NA&gt;         NA        NA\n2             Freehold       &lt;NA&gt;         NA        NA\n3             Freehold       &lt;NA&gt;         NA        NA\n4                 &lt;NA&gt;       &lt;NA&gt;         NA        NA\n5 Co-operative Society       &lt;NA&gt;         NA        NA\n6 Co-operative Society   680 sqft         NA        NA"
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#paso-2-particionamiento-train-test",
    "href": "posts/2024-07-12_Cross_validation/index.html#paso-2-particionamiento-train-test",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Paso 2: Particionamiento (Train, Test)",
    "text": "Paso 2: Particionamiento (Train, Test)\n\nPythonR\n\n\n\n\nCódigo\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop('Price (in rupees)', axis =1)\ny = data['Price (in rupees)']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 95)\n\nprint(y_train.value_counts(normalize = True))\n\n\nPrice (in rupees)\n4000.0     0.014617\n10000.0    0.013600\n5000.0     0.013583\n3200.0     0.008663\n18000.0    0.008326\n             ...   \n13276.0    0.000008\n8285.0     0.000008\n22923.0    0.000008\n12658.0    0.000008\n1498.0     0.000008\nName: proportion, Length: 9760, dtype: float64"
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#desventajas",
    "href": "posts/2024-07-12_Cross_validation/index.html#desventajas",
    "title": "Validación Cruzada (Cross Validation)",
    "section": "Desventajas",
    "text": "Desventajas\nMayor Costo Computacional: Requiere múltiples entrenamientos y evaluaciones del modelo, lo que aumenta el tiempo de cómputo. Complejidad: La implementación y el análisis pueden ser más complejos que una simple división de datos en entrenamiento y prueba."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html",
    "href": "posts/2024-3-28-Componentes-principales/index.html",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica fundamental en ciencia de datos utilizada para la reducción de dimensionalidad. Este método transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales. Es especialmente útil cuando se trabaja con grandes volúmenes de datos y se busca simplificar el análisis sin perder información significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#introducción",
    "href": "posts/2024-3-28-Componentes-principales/index.html#introducción",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica fundamental en ciencia de datos utilizada para la reducción de dimensionalidad. Este método transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales. Es especialmente útil cuando se trabaja con grandes volúmenes de datos y se busca simplificar el análisis sin perder información significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#qué-es-el-análisis-de-componentes-principales",
    "href": "posts/2024-3-28-Componentes-principales/index.html#qué-es-el-análisis-de-componentes-principales",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "¿Qué es el Análisis de Componentes Principales?",
    "text": "¿Qué es el Análisis de Componentes Principales?\nPCA es un método estadístico que transforma los datos originales en nuevas variables no correlacionadas ordenadas según la cantidad de varianza explicada. Los primeros componentes principales capturan la mayor parte de la variabilidad en los datos, lo que permite una reducción significativa de la dimensionalidad mientras se preserva la mayor cantidad de información posible.\n\n\n\n\n\n\nImportante\n\n\n\nConceptos Clave:\n\nVarianza: Medida de la dispersión de los datos\nCovarianza: Indica la dirección de la relación entre dos variables.\nComponentes Principales: Nuevas variables no correlacionadas formadas por combinaciones lineales de las variables originales\nValores y Vectores propios: Los valores propios indican la cantidad de varianza capturada por cada componente principal, y los vectores propios definen la dirección de los componentes."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Ejemplo Practico",
    "text": "Ejemplo Practico\n\n\n\n\n\n\nPara ilustrar un ejemplo se utilizara el dataset de Wine Quality disponible en Kaggle.\n\nPaso 1: Instalación de librerias\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nCódigo\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\n\n\nPaso 2: Cara y Preprocesamiento de Datos\n\nPythonR\n\n\n\n\nCódigo\ndata = pd.read_csv(\"WineQT.csv\")\nfeatures = data.drop('quality', axis=1)\n\nfeatures = StandardScaler().fit_transform(features)\n\n\n\n\n\n\nCódigo\ndata &lt;- read.csv(\"WineQT.csv\")\nfeatures &lt;- data %&gt;% select(-quality)\n\nfeatures &lt;- scale(features)\n\n\n\n\n\n\n\nPaso 3: Aplicación de PCA\n\nPythonR\n\n\n\n\nCódigo\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(features)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\nfinalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n\n\n\n\n\n\nCódigo\npca &lt;- prcomp(features, center = TRUE, scale. = TRUE)\nprincipalComponents &lt;- data.frame(pca$x[,1:2])\nfinalDf &lt;- cbind(principalComponents, quality = data$quality)\n\n\n\n\n\n\n\nPaso 4: Visualización de Resultados\n\npythonR\n\n\n\n\nCódigo\nplt.figure(figsize = (8,6))\nsb.scatterplot(x = 'PC1', y = 'PC2', hue = 'quality', data = finalDf, palette = 'viridis')\nplt.title('PCA de Vinos')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nCódigo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(finalDf, aes(x = PC1, y = PC2, color = as.factor(quality))) +\n  geom_point(alpha=0.5) +\n  labs(title = \"PCA de Vinos\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nReducción de Dimensionalidad: Facilita la visualización y análisis de datos de alta dimensionalidad.\nEliminación de Redundancia: Reduce la redundancia al eliminar las correlaciones entre variables.\n\nDesventajas\n\nInterpretabilidad: Los componentes principales no siempre tienen un significado intuitivo.\nPérdida de Información: Aunque PCA preserva la mayor varianza posible, siempre hay alguna pérdida de información."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#conclusión",
    "href": "posts/2024-3-28-Componentes-principales/index.html#conclusión",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Conclusión",
    "text": "Conclusión\nEl PCA es una herramienta poderosa en el arsenal de un científico de datos. Su capacidad para simplificar conjuntos de datos complejos y reducir la dimensionalidad lo hace indispensable para la exploración y visualización de datos. A través de ejemplos prácticos en Python y R, podemos ver cómo esta técnica se aplica en la práctica, facilitando el análisis y la toma de decisiones basadas en datos."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html",
    "href": "posts/Hyperparameters and Model Validation/index.html",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta razón es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nCódigo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAquí, cada fila de la data se refiere a la observación de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#representación-de-la-data-en-scikit-learn",
    "href": "posts/Hyperparameters and Model Validation/index.html#representación-de-la-data-en-scikit-learn",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta razón es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nCódigo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAquí, cada fila de la data se refiere a la observación de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "href": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Matriz de caracteristicas (Features matrix)",
    "text": "Matriz de caracteristicas (Features matrix)\nEsta tabla contienene la información caracteeristica del conjunto de datos en nuestro ejemplo contiene información de flores. Matemáticamente estas caracteristicas pasan a representar las variables independientes de nuestro conjunto de datos generalmente denotado por \\(X\\)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "href": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Objetivo (Target Array)",
    "text": "Objetivo (Target Array)\nEs un arraglo que matemáticamente representa la variable dependiente generalmente notada por \\(y\\)\n\n\nCódigo\nimport matplotlib.pyplot as plt\n\n\n\n\nCódigo\n#plt.figure(figsize=(12,8))\nsb.set()\nsb.pairplot(iris,hue='species',size=1.5)\n\n\nC:\\Users\\JXBS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\nPara usar los datos en Scikit-Learn, tenemos que extraer los matriz \\(X\\) y el objetivo \\(y\\)\n\n\nCódigo\nX_iris=iris.drop('species',axis=1)\nprint(X_iris.shape)\ny_iris=iris['species']\nprint(y_iris.shape)\n\n\n(150, 4)\n(150,)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "href": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Scikit-Learn’s API estimador",
    "text": "Scikit-Learn’s API estimador\nLa API de Scikit-Learn esta diseñada con los siguientes principios en mente\n\nCoherencia.- Todos los objetos comparten unaa interfaz común extraida de un conunto limitado de métodos, con documentación consistente.\nInspección Todos los valores de parámetros especificados se exponen como atributos públicos. Jerarquía de objetos limitada Sólo los algoritmos están representados por clases de Python; los conjuntos de datos se representan en formatos estándar (matrices NumPy, Pandas DataFrames, matrices dispersas de SciPy) y los nombres de los parámetros utilizan cadenas estándar de Python.\nComposición Muchas tareas de aprendizaje automático se pueden expresar como secuencias de algoritmos más fundamentales, y ScikitLearn hace uso de esto siempre que es posible.\nValores predeterminados sensatos Cuando los modelos requieren parámetros especificados por el usuario, la biblioteca define un valor predeterminado apropiado."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#básicos-de-la-api",
    "href": "posts/Hyperparameters and Model Validation/index.html#básicos-de-la-api",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Básicos de la API",
    "text": "Básicos de la API\nPor lo general, los pasos para usar la API del estimador Scikit-Learn son los siguientes (repasaremos algunos ejemplos detallados en las secciones siguientes):\n\nElija una clase de modelo importando la clase de estimador adecuada de ScikitLearn.\nElija los hiperparámetros del modelo creando una instancia de esta clase con los valores deseados.\nOrganice los datos en una matriz de características y un vector objetivo siguiendo la discusión anterior.\nAjuste el modelo a sus datos llamando al método fit() de la instancia del modelo.\nAplique el modelo a nuevos datos:\n\n• Para el aprendizaje supervisado, a menudo predecimos etiquetas para datos desconocidos usando el método predict().\n• Para el aprendizaje no supervisado, a menudo transformamos o inferimos propiedades de los datos utilizando el método transform() o predict()."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresión-lineal-simple",
    "href": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresión-lineal-simple",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Ejemplo De Aprendizaje Supervisado: Regresión Lineal Simple",
    "text": "Ejemplo De Aprendizaje Supervisado: Regresión Lineal Simple\n\n\nCódigo\nimport numpy as np\n\n\n\n\nCódigo\nrng=np.random.RandomState(42)\nx=10+rng.rand(50)\ny=2*x-1+rng.rand(50)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1e7115f3c20&gt;\n\n\n\n\n\n\n\n\n\n\nEscojemos el modelo\n\n\n\nCódigo\nfrom sklearn.linear_model import LinearRegression\n\n\n\nEscogemos los hiperparametros\n\n\n\nCódigo\nmodel=LinearRegression(fit_intercept=True)\nmodel\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nOrganizando los datos en variables independientes y variable dependiente\n\n\n\nCódigo\nX=x[:,np.newaxis]\nX.shape\n\n\n(50, 1)\n\n\n\nAjustando el modelo\n\n\n\nCódigo\nmodel.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCódigo\nmodel.coef_\n\n\narray([2.06607665])\n\n\n\n\nCódigo\nmodel.intercept_\n\n\n-1.1957940680607742\n\n\n\nPredicciendo data desconocida\n\n\n\nCódigo\nxfit=np.linspace(-1,11)\n\n\n\n\nCódigo\nXfit=xfit[:,np.newaxis]\nyfit=model.predict(Xfit)\n\n\n\n\nCódigo\nplt.scatter(x,y)\nplt.plot(xfit,yfit)"
  },
  {
    "objectID": "posts/naive_bayes/index.html",
    "href": "posts/naive_bayes/index.html",
    "title": "Naive Bayes Clasificación",
    "section": "",
    "text": "En este post vamos a utilizar los datos iris para entrenar un modelo de clasificación y ver que tan bien se puede predecir las etiquetas\nEn este caso para evitarnos particionar el conjunto a “mano” utilizaremos la función train_test_split\nPrieramente importamos las librerias necesarias\n\n\nCódigo\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\n\nluego importamos el conjunto de datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nAhora creamos los datos de entrenamiento y validación\n\n\nCódigo\nXtrain,Xtest,ytrain,ytest =train_test_split(X_iris,y_iris,random_state=1)\n\n\nAhora importamos el modelo que utilizaremos\n\n\nCódigo\nfrom sklearn.naive_bayes import GaussianNB\n\n\nDefinimos un nombre\n\n\nCódigo\nNB=GaussianNB()\n\n\najustamos el modelo\n\n\nCódigo\nNB.fit(Xtrain,ytrain)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\npredecimos la data que no se utilizo para establecer los parámetros del modelo\n\n\nCódigo\ny_NB=NB.predict(Xtest)\n\n\nAhora utilizando la métrica de exactitud (accurancy score) evaluaremos que tan bien predice el modelo que se ha creado\n\n\nCódigo\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nCódigo\naccuracy_score(ytest,y_NB)\n\n\n0.9736842105263158\n\n\nVemos que con una exactitud del \\(97,4\\%\\) el modelo implementado puede etiquetar los datos requeridos."
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html",
    "href": "proyects/2024-08-27-Churn-Rate/index.html",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "",
    "text": "En este proyecto vamos a desarrollar un modelo predictivo para identificar clientes con alto riesgo de churn (abandono) en una institución bancaria, utilizando técnicas de Machine Learning"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#tipo-de-datos",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#tipo-de-datos",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Tipo de datos",
    "text": "Tipo de datos\n\n\nCódigo\nstr(df)\n\n\nClasses 'data.table' and 'data.frame':  10002 obs. of  15 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : chr  \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr  \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr  \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : num  42 41 42 39 43 44 50 29 44 NA ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 NA 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 NA 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n $ ModVal         : num  1 0 0 0 0 0 1 0 0 1 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\ncreamos una lista de variables para mejor manipulación\n\n\nCódigo\nindex &lt;- c(\"RowNumber\", \"CustomerId\",\"Surname\")\nvar_num &lt;- c(\"CreditScore\", \"Age\", \"Tenure\", \"Balance\",\"EstimatedSalary\")\nvar_cat &lt;- setdiff(names(df),c(index,var_num,\"Exited\"))"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#cambio-de-nombres-de-las-columnas",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#cambio-de-nombres-de-las-columnas",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Cambio de nombres de las columnas",
    "text": "Cambio de nombres de las columnas\nEn esta ocasión no vamos a cambiar ningun nombre"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#exploración-de-datos",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#exploración-de-datos",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Exploración de datos",
    "text": "Exploración de datos\nPrimero visualicemos un breve resumen de las variables\n\n\nCódigo\nsummary(df)\n\n\n   RowNumber       CustomerId         Surname           CreditScore   \n Min.   :    1   Min.   :15565701   Length:10002       Min.   :350.0  \n 1st Qu.: 2501   1st Qu.:15628525   Class :character   1st Qu.:584.0  \n Median : 5002   Median :15690732   Mode  :character   Median :652.0  \n Mean   : 5002   Mean   :15690933                      Mean   :650.6  \n 3rd Qu.: 7502   3rd Qu.:15753226                      3rd Qu.:718.0  \n Max.   :10000   Max.   :15815690                      Max.   :850.0  \n                                                                      \n  Geography            Gender               Age            Tenure      \n Length:10002       Length:10002       Min.   :18.00   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:32.00   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median :37.00   Median : 5.000  \n                                       Mean   :38.92   Mean   : 5.012  \n                                       3rd Qu.:44.00   3rd Qu.: 7.000  \n                                       Max.   :92.00   Max.   :10.000  \n                                       NA's   :1                       \n    Balance       NumOfProducts    HasCrCard      IsActiveMember  \n Min.   :     0   Min.   :1.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:     0   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 97199   Median :1.00   Median :1.0000   Median :1.0000  \n Mean   : 76491   Mean   :1.53   Mean   :0.7055   Mean   :0.5149  \n 3rd Qu.:127648   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :250898   Max.   :4.00   Max.   :1.0000   Max.   :1.0000  \n                                 NA's   :1        NA's   :1       \n EstimatedSalary         Exited           ModVal   \n Min.   :    11.58   Min.   :0.0000   Min.   :0.0  \n 1st Qu.: 50983.75   1st Qu.:0.0000   1st Qu.:0.0  \n Median :100185.24   Median :0.0000   Median :0.0  \n Mean   :100083.33   Mean   :0.2038   Mean   :0.3  \n 3rd Qu.:149383.65   3rd Qu.:0.0000   3rd Qu.:1.0  \n Max.   :199992.48   Max.   :1.0000   Max.   :1.0  \n                                                   \n\n\nNotamos que existen valores perdidos o NA’s, los mismos que serán tratados mas adelante"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#evaluación-de-valores-nulos",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#evaluación-de-valores-nulos",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Evaluación de valores nulos",
    "text": "Evaluación de valores nulos\n\n\nCódigo\n# tratamiento NA's \ndf &lt;- df[!(is.na(HasCrCard) == TRUE),]\ndf &lt;- df[!(is.na(IsActiveMember) == TRUE),]\ndf &lt;- df[!(is.na(Age) == TRUE),]\nsum(is.na(df))\n\n\n[1] 0"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#valores-duplicados",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#valores-duplicados",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Valores duplicados",
    "text": "Valores duplicados\n\n\nCódigo\ndf &lt;- unique(df)"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#proporción-del-variable-dependiente",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#proporción-del-variable-dependiente",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Proporción del variable dependiente",
    "text": "Proporción del variable dependiente\n\n\nCódigo\ndf[, round(.N / nrow(df),3), by = Exited]\n\n\n   Exited    V1\n    &lt;int&gt; &lt;num&gt;\n1:      1 0.204\n2:      0 0.796"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#creación-de-features-personalizada",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#creación-de-features-personalizada",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Creación de features personalizada",
    "text": "Creación de features personalizada\n\n\nCódigo\n# Construcción de variables y discretización \n\ndf[,prbm_NumOfProducts := ifelse(NumOfProducts &lt; 1.5, 0.2776061,\n                                 ifelse(NumOfProducts &lt; 2.5, 0.0800000, 0.8744770))]\n\ndf[,prbm_Age := ifelse(Age &lt; 42.5, 0.1208350,\n                       ifelse(Age &lt; 46.5, 0.3450704,\n                              ifelse(Age &lt; 57.5, 0.5261364, 0.3356808)))]\n\ndf[, prbm_IsActiveMember := ifelse(IsActiveMember &lt; 0.5, 0.1451298,0.2732064)]\n\ndf[, prbm_Geography := ifelse(Geography == \"France\", 0.1667147,\n                              ifelse(Geography == \"Spain\",0.1624077, 0.3327684))]\ndf[, prbm_Balance := ifelse(Balance &lt; 1884.5, 0.1368209, 0.2466209)]\n\ndf[, prbm_Gender := ifelse(Gender == \"Male\", 0.1714436 , 0.2507042)]\n\ndf[, prbm_CreditScore := ifelse(CreditScore &lt;= 407.5, 0.94444444 ,0.20573066)]\n\ndf[, prbm_EstimatedSalary := ifelse(EstimatedSalary &lt;= 25000, 0.2031063,\n                                    ifelse(EstimatedSalary &lt;= 35000, 0.2156863,\n                                           ifelse(EstimatedSalary &lt;= 60000, 0.1944134,\n                                                  ifelse(EstimatedSalary &lt;= 75000, 0.2189239,\n                                                         ifelse(EstimatedSalary &lt;= 85000, 0.1789474,\n                                                                ifelse(EstimatedSalary &lt;= 155000, 0.1990913,\n                                                                       ifelse(EstimatedSalary &lt;= 185000, 0.2301815, 0.1965649))))))  )]\n\n# Generación de muestras para el performance del modelo\nmod &lt;- df[ModVal == 0 & Exited %in% c(0,1)]\nval &lt;- df[ModVal ==1]\n\n\n\nRegresión Logística\n\n\nCódigo\n# Regresión logistica\nformula &lt;- \"Exited ~\n  prbm_NumOfProducts +\n  prbm_Age +\n  prbm_IsActiveMember +\n  prbm_Geography + \n  prbm_Gender +\n  prbm_EstimatedSalary\n\"\nmodelo &lt;- glm(formula = as.formula(formula), family = binomial(\"logit\"), data = mod)\nsummary(modelo)\n\n\n\nCall:\nglm(formula = as.formula(formula), family = binomial(\"logit\"), \n    data = mod)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           -6.1860     0.6008 -10.296  &lt; 2e-16 ***\nprbm_NumOfProducts     6.1156     0.2780  22.002  &lt; 2e-16 ***\nprbm_Age               5.5137     0.2196  25.113  &lt; 2e-16 ***\nprbm_IsActiveMember   -7.6616     0.5742 -13.343  &lt; 2e-16 ***\nprbm_Geography         5.2643     0.4509  11.674  &lt; 2e-16 ***\nprbm_Gender            5.9177     0.9004   6.572 4.95e-11 ***\nprbm_EstimatedSalary   6.3426     2.5981   2.441   0.0146 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7149.2  on 6997  degrees of freedom\nResidual deviance: 5134.2  on 6991  degrees of freedom\nAIC: 5148.2\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCódigo\nnom_vars &lt;- all.vars(terms(modelo))[-1]\nres &lt;- cor(setDT(mod)[, ..nom_vars])\nres\n\n\n                     prbm_NumOfProducts   prbm_Age prbm_IsActiveMember\nprbm_NumOfProducts          1.000000000 0.14015906         -0.03921337\nprbm_Age                    0.140159055 1.00000000          0.02311527\nprbm_IsActiveMember        -0.039213366 0.02311527          1.00000000\nprbm_Geography              0.076511468 0.06275108         -0.02691362\nprbm_Gender                 0.031159138 0.03948541         -0.02861081\nprbm_EstimatedSalary        0.004809131 0.01098787         -0.01646802\n                     prbm_Geography  prbm_Gender prbm_EstimatedSalary\nprbm_NumOfProducts       0.07651147  0.031159138          0.004809131\nprbm_Age                 0.06275108  0.039485408          0.010987871\nprbm_IsActiveMember     -0.02691362 -0.028610806         -0.016468018\nprbm_Geography           1.00000000  0.019422000          0.015802201\nprbm_Gender              0.01942200  1.000000000         -0.009303946\nprbm_EstimatedSalary     0.01580220 -0.009303946          1.000000000\n\n\nCódigo\neigen(res)\n\n\neigen() decomposition\n$values\n[1] 1.2103689 1.0254641 1.0085638 0.9703447 0.9400960 0.8451624\n\n$vectors\n            [,1]        [,2]        [,3]       [,4]        [,5]        [,6]\n[1,] -0.61329773  0.07464562  0.01739220  0.1542000  0.37626626  0.67278339\n[2,] -0.57690639  0.36902613  0.06410691 -0.1439743  0.24002392 -0.66973797\n[3,]  0.14872209  0.82019845  0.15353299 -0.3083363 -0.31987004  0.29016501\n[4,] -0.44372704 -0.11890199  0.16319040  0.3516848 -0.79861501 -0.02948562\n[5,] -0.25687639 -0.22070837 -0.60816689 -0.6674430 -0.24617892  0.09670143\n[6,] -0.07776557 -0.35024616  0.75862861 -0.5396852  0.01026557  0.06631192\n\n\nCódigo\nsqrt( head(eigen(res)$values,1) / tail(eigen(res)$values,1)) # calculo IC &gt; 5 indicios multicolinealidad\n\n\n[1] 1.19671\n\n\nCódigo\ndf[, Y:= modelo$coefficients[1] + Reduce(`+`, Map(function(var, coef) get(var) * coef, names(modelo$coefficients[-1]), modelo$coefficients[-1]))]\ndf[, RL := (1 / (1 + exp(Y)))]\n#quantile(df$RL, probs = seq(0 , 1, by = 0.01), na.rm = TRUE)\n\nmod &lt;- df[ModVal == 0 & Exited %in%  c(0,1)]\nval &lt;- df[ModVal == 1]\n\n# Estimación  Probabilidades\nres_mod &lt;- data.table(Var = mod$Exited, RL = mod$RL)\n\nres_val &lt;- data.table(Var = val$Exited, RL = val$RL)\n\n\n\n\nCódigo\n# Gráficas ROC\nlibrary(pROC)\nobjroc1 &lt;- roc(res_mod$Var, res_mod$RL, auc=T, ci=T)\nplot(objroc1, col=\"blue\", xlab=\"1 - Especificidad\", ylab=\"Sensibilidad\", main=\"Comparación curvas ROC\", legacy.axes = TRUE)\nobjroc2 &lt;- roc(res_val$Var, res_val$RL, auc=T, ci=T)\nplot(objroc2, col=\"red\", add=TRUE)\nlegend(\"bottomright\", legend=c(paste(\"Modelamiento\",round(objroc1$auc, 3)), paste(\"Validación\", round(objroc2$auc, 3))), col=c(\"blue\", \"red\"), lwd=0.5, title = \"AUC-ROC\")\n\n\n\n\n\n\n\n\n\nCódigo\n#legend(\"topright\", legend = c(round(objroc1$auc, 3), round(objroc2$auc, 3)), col = c(\"blue\", \"red\"), lwd = 0.2 , title = \"AUC-ROC\")\n\n\nCon estas features se ha alcanzado un AUC-ROC con la data de validación de 0.846"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#optimal-binning",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#optimal-binning",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Optimal binning",
    "text": "Optimal binning\nCreando los features y bins con la libreria scorecard\n\n\nCódigo\n#install.packages(\"scorecard\")\nlibrary(scorecard)\n\nvars &lt;- c(\"NumOfProducts\", \"Age\", \"IsActiveMember\", \"Geography\", \"Gender\", \"EstimatedSalary\")\nbins &lt;- woebin(df, y = \"Exited\", x = vars)\n\n\n✔ Binning on 9998 rows and 7 columns in 00:00:02\n\n\nCódigo\nprint(bins)\n\n\n$NumOfProducts\n        variable      bin count count_distr   neg   pos   posprob        woe\n          &lt;char&gt;   &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;      &lt;num&gt;\n1: NumOfProducts [-Inf,2)  5082   0.5083017  3673  1409 0.2772530  0.4043315\n2: NumOfProducts [2, Inf)  4916   0.4916983  4287   629 0.1279496 -0.5567511\n       bin_iv  total_iv breaks is_special_values\n        &lt;num&gt;     &lt;num&gt; &lt;char&gt;            &lt;lgcl&gt;\n1: 0.09296873 0.2209836      2             FALSE\n2: 0.12801486 0.2209836    Inf             FALSE\n\n$Age\n   variable       bin count count_distr   neg   pos   posprob        woe\n     &lt;char&gt;    &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;      &lt;num&gt;\n1:      Age [-Inf,35)  3678  0.36787357  3388   290 0.0788472 -1.0956541\n2:      Age   [35,42)  3106  0.31066213  2641   465 0.1497102 -0.3744154\n3:      Age   [42,45)   874  0.08741748   635   239 0.2734554  0.3852986\n4:      Age [45, Inf)  2340  0.23404681  1296  1044 0.4461538  1.1462370\n       bin_iv  total_iv breaks is_special_values\n        &lt;num&gt;     &lt;num&gt; &lt;char&gt;            &lt;lgcl&gt;\n1: 0.31043361 0.7642339     35             FALSE\n2: 0.03879657 0.7642339     42             FALSE\n3: 0.01444791 0.7642339     45             FALSE\n4: 0.40055578 0.7642339    Inf             FALSE\n\n$IsActiveMember\n         variable      bin count count_distr   neg   pos   posprob        woe\n           &lt;char&gt;   &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;      &lt;num&gt;\n1: IsActiveMember [-Inf,1)  4850    0.485097  3547  1303 0.2686598  0.3610272\n2: IsActiveMember [1, Inf)  5148    0.514903  4413   735 0.1427739 -0.4299794\n       bin_iv total_iv breaks is_special_values\n        &lt;num&gt;    &lt;num&gt; &lt;char&gt;            &lt;lgcl&gt;\n1: 0.06994876 0.153257      1             FALSE\n2: 0.08330821 0.153257    Inf             FALSE\n\n$Geography\n    variable              bin count count_distr   neg   pos   posprob\n      &lt;char&gt;           &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;\n1: Geography France%,%missing  5012   0.5013003  4202   810 0.1616121\n2: Geography            Spain  2476   0.2476495  2063   413 0.1668013\n3: Geography          Germany  2510   0.2510502  1695   815 0.3247012\n          woe     bin_iv  total_iv           breaks is_special_values\n        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;           &lt;char&gt;            &lt;lgcl&gt;\n1: -0.2838216 0.03702196 0.1687521 France%,%missing             FALSE\n2: -0.2460089 0.01390472 0.1687521            Spain             FALSE\n3:  0.6302102 0.11782546 0.1687521          Germany             FALSE\n\n$Gender\n   variable    bin count count_distr   neg   pos   posprob        woe\n     &lt;char&gt; &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;      &lt;num&gt;\n1:   Gender   Male  5456   0.5457091  4557   899 0.1647727 -0.2606767\n2:   Gender Female  4542   0.4542909  3403  1139 0.2507706  0.2679534\n       bin_iv   total_iv breaks is_special_values\n        &lt;num&gt;      &lt;num&gt; &lt;char&gt;            &lt;lgcl&gt;\n1: 0.03424476 0.06944544   Male             FALSE\n2: 0.03520068 0.06944544 Female             FALSE\n\n$EstimatedSalary\n          variable             bin count count_distr   neg   pos   posprob\n            &lt;char&gt;          &lt;char&gt; &lt;int&gt;       &lt;num&gt; &lt;int&gt; &lt;int&gt;     &lt;num&gt;\n1: EstimatedSalary    [-Inf,25000)  1217  0.12172434   975   242 0.1988496\n2: EstimatedSalary   [25000,35000)   501  0.05011002   388   113 0.2255489\n3: EstimatedSalary   [35000,60000)  1243  0.12432486  1013   230 0.1850362\n4: EstimatedSalary   [60000,75000)   759  0.07591518   589   170 0.2239789\n5: EstimatedSalary   [75000,85000)   523  0.05231046   436    87 0.1663480\n6: EstimatedSalary  [85000,155000)  3519  0.35197039  2807   712 0.2023302\n7: EstimatedSalary [155000,185000)  1499  0.14992999  1166   333 0.2221481\n8: EstimatedSalary   [185000, Inf)   737  0.07371474   586   151 0.2048847\n            woe       bin_iv    total_iv breaks is_special_values\n          &lt;num&gt;        &lt;num&gt;       &lt;num&gt; &lt;char&gt;            &lt;lgcl&gt;\n1: -0.031039680 1.161992e-04 0.008733362  25000             FALSE\n2:  0.128842544 8.636055e-04 0.008733362  35000             FALSE\n3: -0.120132130 1.730571e-03 0.008733362  60000             FALSE\n4:  0.119832318 1.128837e-03 0.008733362  75000             FALSE\n5: -0.249274060 3.012467e-03 0.008733362  85000             FALSE\n6: -0.009333600 3.057754e-05 0.008733362 155000             FALSE\n7:  0.109268188 1.848061e-03 0.008733362 185000             FALSE\n8:  0.006420112 3.044140e-06 0.008733362    Inf             FALSE\n\n\n\n\nCódigo\ndf_bineed &lt;- woebin_ply(df,bins = bins)\n\n\n✔ Woe transformating on 9998 rows and 6 columns in 00:00:01\n\n\nCódigo\nhead(df_bineed, 5)\n\n\n   RowNumber CustomerId  Surname CreditScore Tenure   Balance HasCrCard Exited\n       &lt;int&gt;      &lt;int&gt;   &lt;char&gt;       &lt;int&gt;  &lt;int&gt;     &lt;num&gt;     &lt;int&gt;  &lt;int&gt;\n1:         1   15634602 Hargrave         619      2      0.00         1      1\n2:         2   15647311     Hill         608      1  83807.86         0      0\n3:         3   15619304     Onio         502      8 159660.80         1      1\n4:         4   15701354     Boni         699      1      0.00         0      0\n5:         6   15574012      Chu         645      8 113755.78         1      1\n   ModVal prbm_NumOfProducts  prbm_Age prbm_IsActiveMember prbm_Geography\n    &lt;num&gt;              &lt;num&gt;     &lt;num&gt;               &lt;num&gt;          &lt;num&gt;\n1:      1          0.2776061 0.1208350           0.2732064      0.1667147\n2:      0          0.2776061 0.1208350           0.2732064      0.1624077\n3:      0          0.8744770 0.1208350           0.1451298      0.1667147\n4:      0          0.0800000 0.1208350           0.1451298      0.1667147\n5:      0          0.0800000 0.3450704           0.1451298      0.1624077\n   prbm_Balance prbm_Gender prbm_CreditScore prbm_EstimatedSalary         Y\n          &lt;num&gt;       &lt;num&gt;            &lt;num&gt;                &lt;num&gt;     &lt;num&gt;\n1:    0.1368209   0.2507042        0.2057307            0.1990913 -2.291211\n2:    0.2466209   0.2507042        0.2057307            0.1990913 -2.313884\n3:    0.2466209   0.2507042        0.2057307            0.1990913  2.340290\n4:    0.1368209   0.2507042        0.2057307            0.1990913 -2.518418\n5:    0.2466209   0.1714436        0.2057307            0.1990913 -1.773778\n           RL NumOfProducts_woe    Age_woe IsActiveMember_woe Geography_woe\n        &lt;num&gt;             &lt;num&gt;      &lt;num&gt;              &lt;num&gt;         &lt;num&gt;\n1: 0.90814650         0.4043315  0.3852986         -0.4299794    -0.2838216\n2: 0.91002041         0.4043315 -0.3744154         -0.4299794    -0.2460089\n3: 0.08784071        -0.5567511  0.3852986          0.3610272    -0.2838216\n4: 0.92542293        -0.5567511 -0.3744154          0.3610272    -0.2838216\n5: 0.85492687        -0.5567511  0.3852986          0.3610272    -0.2460089\n   Gender_woe EstimatedSalary_woe\n        &lt;num&gt;               &lt;num&gt;\n1:  0.2679534          -0.0093336\n2:  0.2679534          -0.0093336\n3:  0.2679534          -0.0093336\n4:  0.2679534          -0.0093336\n5: -0.2606767          -0.0093336\n\n\n\n\nCódigo\n# Generación de muestras para el performance del modelo\nmod &lt;- df_bineed[ModVal == 0 & Exited %in% c(0,1)]\nval &lt;- df_bineed[ModVal ==1]\n# Regresión logistica\nformula &lt;- \"Exited ~\n  NumOfProducts_woe +\n  Age_woe +\n  IsActiveMember_woe +\n  Geography_woe + \n  Gender_woe +\n  EstimatedSalary_woe\n\"\nmodelo &lt;- glm(formula = as.formula(formula), family = binomial(\"logit\"), data = mod)\n\nsummary(modelo)\n\n\n\nCall:\nglm(formula = as.formula(formula), family = binomial(\"logit\"), \n    data = mod)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.32127    0.03436 -38.457  &lt; 2e-16 ***\nNumOfProducts_woe    0.88080    0.07163  12.297  &lt; 2e-16 ***\nAge_woe              1.03291    0.03853  26.806  &lt; 2e-16 ***\nIsActiveMember_woe   1.23838    0.08731  14.184  &lt; 2e-16 ***\nGeography_woe        0.99349    0.07884  12.602  &lt; 2e-16 ***\nGender_woe           0.86631    0.12656   6.845 7.65e-12 ***\nEstimatedSalary_woe  0.77650    0.35711   2.174   0.0297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7149.2  on 6997  degrees of freedom\nResidual deviance: 5690.5  on 6991  degrees of freedom\nAIC: 5704.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCódigo\nnom_vars &lt;- all.vars(terms(modelo))[-1]\nres &lt;- cor(setDT(mod)[, ..nom_vars])\nres\n\n\n                    NumOfProducts_woe      Age_woe IsActiveMember_woe\nNumOfProducts_woe        1.0000000000  0.077205537         0.02528692\nAge_woe                  0.0772055374  1.000000000        -0.03600072\nIsActiveMember_woe       0.0252869155 -0.036000718         1.00000000\nGeography_woe            0.0362993628  0.062453734         0.02570862\nGender_woe              -0.0051035218  0.041384541         0.02861081\nEstimatedSalary_woe     -0.0002743988 -0.000688623         0.02210737\n                    Geography_woe   Gender_woe EstimatedSalary_woe\nNumOfProducts_woe      0.03629936 -0.005103522       -0.0002743988\nAge_woe                0.06245373  0.041384541       -0.0006886230\nIsActiveMember_woe     0.02570862  0.028610806        0.0221073663\nGeography_woe          1.00000000  0.018463506        0.0163394609\nGender_woe             0.01846351  1.000000000       -0.0015705987\nEstimatedSalary_woe    0.01633946 -0.001570599        1.0000000000\n\n\nCódigo\neigen(res)\n\n\neigen() decomposition\n$values\n[1] 1.1293676 1.0415320 1.0050447 0.9873191 0.9563795 0.8803571\n\n$vectors\n            [,1]        [,2]        [,3]       [,4]        [,5]        [,6]\n[1,] -0.51994721 -0.08399426  0.35618599 -0.4434461  0.43423271  0.45883434\n[2,] -0.61446110 -0.35014917 -0.06401573  0.1887344  0.14251679 -0.66317673\n[3,] -0.10513934  0.75866654 -0.02132252 -0.4742214  0.01207762 -0.43345625\n[4,] -0.51162371  0.15684900  0.08886514  0.1162957 -0.79514618  0.24486818\n[5,] -0.27141610  0.23627091 -0.81765353  0.1930863  0.25525188  0.31546188\n[6,] -0.07491766  0.46296874  0.43832016  0.7014570  0.30589882  0.04802835\n\n\nCódigo\nsqrt( head(eigen(res)$values,1) / tail(eigen(res)$values,1)) # calculo IC &gt; 5 indicios multicolinealidad\n\n\n[1] 1.13263\n\n\nCódigo\ndf_bineed[, Y:= modelo$coefficients[1] + Reduce(`+`, Map(function(var, coef) get(var) * coef, names(modelo$coefficients[-1]), modelo$coefficients[-1]))]\ndf_bineed[, RL := (1 / (1 + exp(Y)))]\n#quantile(df_bineed$RL, probs = seq(0 , 1, by = 0.01))\n\nmod &lt;- df_bineed[ModVal == 0 & Exited %in%  c(0,1)]\nval &lt;- df_bineed[ModVal == 1]\n\n# Estimación  Probabilidades\nres_mod &lt;- data.table(Var = mod$Exited, RL = mod$RL)\n\nres_val &lt;- data.table(Var = val$Exited, RL = val$RL)\n\n\n\n\nCódigo\n# Gráficas ROC\n#library(pROC)\nobjroc1 &lt;- roc(res_mod$Var, res_mod$RL, auc=T, ci=T)\nplot(objroc1, col=\"blue\", xlab=\"1 - Especificidad\", ylab=\"Sensibilidad\", main=\"Comparación curvas ROC\", legacy.axes = TRUE)\nobjroc2 &lt;- roc(res_val$Var, res_val$RL, auc=T, ci=T)\nplot(objroc2, col=\"red\", add=TRUE)\nlegend(\"bottomright\", legend=c(paste(\"Modelamiento\",round(objroc1$auc, 3)), paste(\"Validación\", round(objroc2$auc, 3))), col=c(\"blue\", \"red\"), lwd=0.5, title = \"AUC-ROC\")\n\n\n\n\n\n\n\n\n\nCon la librería scorecard se ha alcanzado un AUC-ROC de validación de 0.815"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#ensamble",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#ensamble",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Ensamble",
    "text": "Ensamble\nCreamos y damos formato necesario para utilizar h2o, nota en esta ocasión trabajaremos con las variables binneadas de forma personalizada\n\n\nCódigo\n# Ejecución del Ensamble\n\nlibrary(h2o)\nh2o.init(ip = \"localhost\", nthreads = 2, max_mem_size = \"5G\")\n\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         28 minutes 41 seconds \n    H2O cluster timezone:       America/Guayaquil \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    8 months and 13 days \n    H2O cluster name:           H2O_started_from_R_joelb_nrn194 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   4.91 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  2 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.3 (2024-02-29 ucrt) \n\n\nCódigo\n# Establecemos los datos en el formato adecuado\nvars &lt;- c(\"Exited\", \"prbm_NumOfProducts\", \"prbm_Age\", \"prbm_IsActiveMember\", \"prbm_Geography\", \"prbm_Gender\", \"prbm_EstimatedSalary\")\n\nmod_em &lt;- as.h2o(x = setDT(mod)[, vars, with = FALSE])\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\n# Identificamos predictores y respuesta\ny_em &lt;- \"Exited\"; x_em &lt;- setdiff(names(mod_em), y_em)\n\n# Para la clasificación binaria, la respuesta debe ser un factor \nmod_em[,y_em] &lt;- as.factor(mod_em[,y_em])\n\n# Numero de CV folds \nnfolds &lt;- 5\n\n# Generaremos el ensamble con 3 modelos (RF + GLM + GBM)"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#random-forest",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#random-forest",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Random Forest",
    "text": "Random Forest\n\n\nCódigo\n# Train & Cross- validate a RF\nmy_rf &lt;- h2o.randomForest(x = x_em,\n                          y = y_em,\n                          model_id = \"RF\",\n                          training_frame = mod_em,\n                          ntrees = 300,\n                          min_rows = 50,\n                          mtries = 5,\n                          nfolds = nfolds,\n                          fold_assignment = \"Stratified\",\n                          keep_cross_validation_predictions = TRUE,\n                          seed = 95)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nh2o.confusionMatrix(my_rf)\n\n\nConfusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.239020037913721:\n          0    1    Error        Rate\n0      4556  989 0.178359   =989/5545\n1       425 1028 0.292498   =425/1453\nTotals 4981 2017 0.202058  =1414/6998"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#gradient-boosting-machine",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#gradient-boosting-machine",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Gradient Boosting Machine",
    "text": "Gradient Boosting Machine\n\n\nCódigo\n# Train & Cross-validate a GBM\nmy_gbm &lt;- h2o.gbm(x = x_em,\n                  y = y_em,\n                  model_id = \"GBM\",\n                  training_frame = mod_em,\n                  distribution = \"bernoulli\",\n                  ntrees = 300,\n                  max_depth = 5,\n                  min_rows = 50,\n                  learn_rate = 0.02,\n                  nfolds = nfolds,\n                  fold_assignment = \"Stratified\",\n                  keep_cross_validation_predictions = TRUE,\n                  seed = 95)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nh2o.confusionMatrix(my_gbm)\n\n\nConfusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.324439576278591:\n          0    1    Error        Rate\n0      5050  495 0.089270   =495/5545\n1       587  866 0.403992   =587/1453\nTotals 5637 1361 0.154616  =1082/6998"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#regresión-logística-1",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#regresión-logística-1",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Regresión Logística",
    "text": "Regresión Logística\n\n\nCódigo\n# Train & Cross-validate a GLM\nmy_glm &lt;- h2o.glm(x = x_em,\n                  y = y_em,\n                  model_id = \"GLM\",\n                  training_frame = mod_em,\n                  alpha = 0.1, # penaliza inclusión excesiva de variables\n                  remove_collinear_columns = TRUE,\n                  nfolds = nfolds,\n                  fold_assignment = \"Stratified\",\n                  keep_cross_validation_predictions = TRUE,\n                  seed = 95)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nh2o.confusionMatrix(my_glm)\n\n\nConfusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.297621966238341:\n          0    1    Error        Rate\n0      4920  625 0.112714   =625/5545\n1       569  884 0.391604   =569/1453\nTotals 5489 1509 0.170620  =1194/6998"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#ensamble-rf-gbm",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#ensamble-rf-gbm",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Ensamble RF + GBM",
    "text": "Ensamble RF + GBM\n\n\nCódigo\n# Train ensamble\n\ne1m &lt;- h2o.stackedEnsemble(x = x_em,\n                           y = y_em,\n                           training_frame = mod_em,\n                           model_id = \"Ensamble_1m\",\n                           metalearner_algorithm = \"deeplearning\",\n                           base_models = list(my_rf, my_gbm))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#ensamble-rf-rl",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#ensamble-rf-rl",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Ensamble RF + RL",
    "text": "Ensamble RF + RL\n\n\nCódigo\ne2m &lt;- h2o.stackedEnsemble(x = x_em,\n                           y = y_em,\n                           training_frame = mod_em,\n                           model_id = \"Ensamble_2m\",\n                           metalearner_algorithm = \"deeplearning\",\n                           base_models = list(my_rf, my_glm))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nEnsamble RF + RL + GBM\n\n\nCódigo\ne3m &lt;- h2o.stackedEnsemble(x = x_em,\n                           y = y_em,\n                           training_frame = mod_em,\n                           model_id = \"Ensamble3m\",\n                           metalearner_algorithm = \"deeplearning\",\n                           base_models = list(my_rf, my_gbm, my_glm))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#predicciones",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#predicciones",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Predicciones",
    "text": "Predicciones\n\n\nCódigo\n# Realizamos los predicciones sobre la muestra de modelamiento / validación\nmod_em &lt;- as.h2o(x = setDT(mod)[, vars, with = FALSE])\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_em &lt;- as.h2o(x = setDT(val)[, vars, with = FALSE])\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nmod_em[,y_em] &lt;- as.factor(mod_em[, y_em])\nval_em[,y_em] &lt;- as.factor(val_em[, y_em])\n\nres_f &lt;- function(valida, resul){\n  res &lt;- data.frame(Exited = valida$Exited,\n                    Exited_p = as.data.frame(resul)[,3],0)\n  return(res)\n} \n\nroc_graf &lt;- function(res_mod, res_val, name=\"\"){\nobjroc1 &lt;- roc(res_mod$Exited, res_mod$Exited_p, auc=T, ci=T)\nplot(objroc1, col=\"blue\", xlab=\"1 - Especificidad\", ylab=\"Sensibilidad\", main=paste(\"Comparación curvas ROC\",name), legacy.axes = TRUE)\nobjroc2 &lt;- roc(res_val$Exited, res_val$Exited_p, auc=T, ci=T)\nplot(objroc2, col=\"red\", add=TRUE)\nlegend(\"bottomright\", legend=c(paste(\"Modelamiento\",round(objroc1$auc, 3)), paste(\"Validación\", round(objroc2$auc, 3))), col=c(\"blue\", \"red\"), lwd=0.5, title = \"AUC-ROC\")\n}"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC RF",
    "text": "Curva ROC RF\n\n\nCódigo\n# Random Forest\nmod_rf &lt;- setDT(res_f(mod, h2o.predict(my_rf, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_rf &lt;- setDT(res_f(val, h2o.predict(my_rf, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_rf,val_rf, \"RF\")"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rl",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rl",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC RL",
    "text": "Curva ROC RL\n\n\nCódigo\n# Regresión Logística\nmod_glm &lt;- setDT(res_f(mod, h2o.predict(my_glm, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_glm &lt;- setDT(res_f(val, h2o.predict(my_glm, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_glm,val_glm, \"Regresión Logística\")"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-gbm",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-gbm",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC GBM",
    "text": "Curva ROC GBM\n\n\nCódigo\n# Gradient Boosting \nmod_gbm &lt;- setDT(res_f(mod, h2o.predict(my_gbm, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_gbm &lt;- setDT(res_f(val, h2o.predict(my_gbm, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_gbm,val_gbm, \"Gradient Boosting\")"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf-gbm",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf-gbm",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC RF + GBM",
    "text": "Curva ROC RF + GBM\n\n\nCódigo\n# Ensamble RF + GBM\nmod_e1m &lt;- setDT(res_f(mod, h2o.predict(e1m, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_e1m &lt;- setDT(res_f(val, h2o.predict(e1m, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_e1m,val_e1m, \"Ensamble RF + GBM\")"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf-rl",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rf-rl",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC RF + RL",
    "text": "Curva ROC RF + RL\n\n\nCódigo\n# Ensamble GLM +RF\nmod_e2m &lt;- setDT(res_f(mod, h2o.predict(e2m, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_e2m &lt;- setDT(res_f(val, h2o.predict(e2m, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_e2m,val_e2m, \"Ensamble RF + GLM(RL)\")"
  },
  {
    "objectID": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rl-rf-gbm",
    "href": "proyects/2024-08-27-Churn-Rate/index.html#curva-roc-rl-rf-gbm",
    "title": "Churn Rate (Tasa de Abandono)",
    "section": "Curva ROC RL + RF + GBM",
    "text": "Curva ROC RL + RF + GBM\n\n\nCódigo\n# Ensamble GLM + RF + GBM\nmod_e3m &lt;- setDT(res_f(mod, h2o.predict(e3m, newdata = mod_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nval_e3m &lt;- setDT(res_f(val, h2o.predict(e3m, newdata = val_em)))\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCódigo\nroc_graf(mod_e3m,val_e3m, \"Ensamble RF + GBM + GLM(RL)\")"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html",
    "href": "proyects/Tesis_JXBS/index.html",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "El modelo de consumo y renta surge de manera casi natural cuando un individuo se plantea un escenario del tipo ¿Qué pasaría si se destinara una parte de su renta a un fondo de ahorro?. Es así que se empieza con un planteamiento muy básico en el que el individuo esperaría que su ahorro lo ayude a afrontar crisis futuras. Sin embargo, esto es un poco ingenuo puesto que el individuo no tiene certeza de cuanto tiempo va a vivir, así como tampoco de si su ahorro lo ayudara en un momento de gran crisis. Es así que de manera natural se procede a generalizar la idea a un grupo de individuos, lo cual desencadena en lo siguiente ¿Todos los individuos se comportan de una manera similar?. A partir de esto se ramifica dos situaciones, la primera, considerar que todos los individuos tienen un comportamiento similar y por tanto realizar modelos con datos de medias; la segunda, es considerar que los individuos tiene comportamientos diferentes y por tanto buscar cual es el modelo que mejor describe esta realidad.\n\n\nAl inicio de los años \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la “ley fundamental de la psicología” mencionada por (Keynes 1936) en la Teoría general. A ese momento, las limitaciones empíricas y teóricas del mencionado modelo se hicieron cada vez más notorias. Desde un punto de vista teórico, es difícil construir modelos coherentes basados en la optimización intertemporal del comportamiento que sean consistentes con la descripción de (Keynes 1936) en la “Ley fundamental de la psicología”. Desde el punto de vista empírico, parecía que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, según (Orazio P. Attanasio y Weber 2010) se observó que la propensión marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistemáticamente con el nivel de rentas. Además, se observó que grupos de individuos con, niveles más bajos de rentas en media, tenían tasas de ahorro más altas que otros grupos con niveles más altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observó que las tasas de ahorro están sistemáticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradecían claramente las implicaciones del modelo keynesiano y condujeron a la formulación de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia teórica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimización coherente con la capacidad de ajustar la mayoría de los hechos mencionados en el párrafo anterior.\nA nivel macro, es más probable que las fluctuaciones a corto plazo de la renta disponible estén dominadas por la varianza de las perturbaciones temporales que se promediarían a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) también llevó a la realización de otras implicaciones. En una versión simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilación, los consumidores ahorraran cuando sean jóvenes para respaldar el consumo en la última parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlación entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un año dado, las cohortes más jóvenes, que están ahorrando, son ``más ricas’’ en términos de vida que las de mayor edad, que están desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor será la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor será la tasa agregada de ahorro.\nDespués de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo estándar de la macroeconomía moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la década de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explotó la idea de usar las condiciones de primer orden del problema de optimización intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuación de Euler1, hace posible el análisis empírico de un problema que es analíticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centrándose en la esencia económica del modelo: los consumidores, en el momento óptimo, actuarán para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estadística suficiente para las elecciones de los consumidores y, dadas sus propiedades dinámicas, puede “diferenciarse” de manera análoga al tratamiento de los efectos fijos en la econometría (Orazio P. Attanasio y Weber 2010).\n\n\n\n\n\nComo se mencionó en la introducción, el modelo de ciclo de vida/renta permanente se desarrolló para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos volátil que la renta y la propensión marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos “macro acontecimientos” siguen siendo válidos y algunos también se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es más alta que la de los blancos en cualquier nivel de rentas, como señaló (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta “permanente”, como los hogares encabezados por personas con diferentes niveles de educación.\nAnálogamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos últimos es menor que la de los primeros, como señalaron hace varios años (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo económico (J. Y. Campbell y Mankiw 1989) encontraron que la regresión de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estadísticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran número de consumidores que siguen una “regla general” y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlación significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo está mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y además se identifica las innovaciones permanentes a está ultima variable, el modelo predice que estas innovaciones deberían traducirse uno a uno en consumo. Esto implica restricciones paramétricas de ecuaciones cruzadas sobre la representación \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), señalaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepción es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene raíz unitaria (i.e es \\(I(1)\\). 2) la sección cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribución transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad empírica: la distribución de la sección cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versión estándar del modelo de ciclo de vida, a cualquier edad el “\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)” 4 y por lo tanto, por sustitución recursiva, se obtiene que el \\(log(consumo)\\) está dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado específico, muestran que la evolución relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la sección cruzada de la renta y la del consumo identificará los cambios en la varianza de la sección cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribución inicial del consumo (sin importar cómo se determine) en presencia de un riesgo compartido perfecto esa distribución debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evolución de sección cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar explícitamente los movimientos en la clasificación relativa en la distribución del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evolución del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlación en bajas frecuencias entre estas dos variables como evidencia en contra de la hipótesis del mercado completo. No pueden rechazar la hipótesis de que a frecuencias relativamente altos (como un año) no existe una relación entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisión-bibliográfica",
    "href": "proyects/Tesis_JXBS/index.html#revisión-bibliográfica",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "Al inicio de los años \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la “ley fundamental de la psicología” mencionada por (Keynes 1936) en la Teoría general. A ese momento, las limitaciones empíricas y teóricas del mencionado modelo se hicieron cada vez más notorias. Desde un punto de vista teórico, es difícil construir modelos coherentes basados en la optimización intertemporal del comportamiento que sean consistentes con la descripción de (Keynes 1936) en la “Ley fundamental de la psicología”. Desde el punto de vista empírico, parecía que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, según (Orazio P. Attanasio y Weber 2010) se observó que la propensión marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistemáticamente con el nivel de rentas. Además, se observó que grupos de individuos con, niveles más bajos de rentas en media, tenían tasas de ahorro más altas que otros grupos con niveles más altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observó que las tasas de ahorro están sistemáticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradecían claramente las implicaciones del modelo keynesiano y condujeron a la formulación de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia teórica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimización coherente con la capacidad de ajustar la mayoría de los hechos mencionados en el párrafo anterior.\nA nivel macro, es más probable que las fluctuaciones a corto plazo de la renta disponible estén dominadas por la varianza de las perturbaciones temporales que se promediarían a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) también llevó a la realización de otras implicaciones. En una versión simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilación, los consumidores ahorraran cuando sean jóvenes para respaldar el consumo en la última parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlación entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un año dado, las cohortes más jóvenes, que están ahorrando, son ``más ricas’’ en términos de vida que las de mayor edad, que están desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor será la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor será la tasa agregada de ahorro.\nDespués de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo estándar de la macroeconomía moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la década de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explotó la idea de usar las condiciones de primer orden del problema de optimización intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuación de Euler1, hace posible el análisis empírico de un problema que es analíticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centrándose en la esencia económica del modelo: los consumidores, en el momento óptimo, actuarán para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estadística suficiente para las elecciones de los consumidores y, dadas sus propiedades dinámicas, puede “diferenciarse” de manera análoga al tratamiento de los efectos fijos en la econometría (Orazio P. Attanasio y Weber 2010)."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "href": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "Como se mencionó en la introducción, el modelo de ciclo de vida/renta permanente se desarrolló para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos volátil que la renta y la propensión marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos “macro acontecimientos” siguen siendo válidos y algunos también se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es más alta que la de los blancos en cualquier nivel de rentas, como señaló (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta “permanente”, como los hogares encabezados por personas con diferentes niveles de educación.\nAnálogamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos últimos es menor que la de los primeros, como señalaron hace varios años (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo económico (J. Y. Campbell y Mankiw 1989) encontraron que la regresión de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estadísticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran número de consumidores que siguen una “regla general” y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlación significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo está mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y además se identifica las innovaciones permanentes a está ultima variable, el modelo predice que estas innovaciones deberían traducirse uno a uno en consumo. Esto implica restricciones paramétricas de ecuaciones cruzadas sobre la representación \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), señalaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepción es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene raíz unitaria (i.e es \\(I(1)\\). 2) la sección cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribución transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad empírica: la distribución de la sección cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versión estándar del modelo de ciclo de vida, a cualquier edad el “\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)” 4 y por lo tanto, por sustitución recursiva, se obtiene que el \\(log(consumo)\\) está dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado específico, muestran que la evolución relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la sección cruzada de la renta y la del consumo identificará los cambios en la varianza de la sección cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribución inicial del consumo (sin importar cómo se determine) en presencia de un riesgo compartido perfecto esa distribución debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evolución de sección cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar explícitamente los movimientos en la clasificación relativa en la distribución del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evolución del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlación en bajas frecuencias entre estas dos variables como evidencia en contra de la hipótesis del mercado completo. No pueden rechazar la hipótesis de que a frecuencias relativamente altos (como un año) no existe una relación entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#ecuación-de-euler-del-consumo",
    "href": "proyects/Tesis_JXBS/index.html#ecuación-de-euler-del-consumo",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Ecuación de Euler del consumo",
    "text": "Ecuación de Euler del consumo\n(Parker 2007) Dice que, considerando un agente de vida infinita que elige una variable de control \\(C\\) en cada período \\(t\\) para maximizar un objetivo intertemporal: \\(\\sum\\limits_{t=1}^\\infty \\beta u(C_t)\\), donde \\(u(C_t)\\) representa el flujo de pago en \\(t\\), \\(u'&gt;0,~u''&lt;0\\), y \\(\\beta\\) es el factor de descuento, \\(0&lt;\\beta&lt;1\\). El agente se enfrenta a una restricción presupuestaria de valor presente: \\(\\sum\\limits_{t=1} R^{1-t}C_t\\leq W_1\\), donde \\(R\\) es la tasa de interés bruta (\\(R=1+r\\), donde \\(r\\) es la tasa de interés) y \\(W_1\\) es dado (mas adelante veremos que es el patrimonio). Por la teoría de optimización, si una trayectoria de tiempo del control es óptima, un aumento marginal en el control en cualquier \\(t\\), \\(dC_t\\), debe tener beneficios en \\(t+1\\) de la misma cantidad de valor presente, \\(-RdC_t\\), así: \\(\\beta^{t-1}u'(C_t)dC_t-\\beta^tu'(C_{t+1})RdC_t=0\\). Reorganizando obtenemos las ecuaciones de Euler: \\(u'(C_t)=\\beta Ru'(C_{t+1}),\\) para \\(t=1,2,3,\\dots.\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#modelo-teórico-el-modelo-de-ciclo-de-vida",
    "href": "proyects/Tesis_JXBS/index.html#modelo-teórico-el-modelo-de-ciclo-de-vida",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Modelo Teórico “EL modelo de ciclo de vida”",
    "text": "Modelo Teórico “EL modelo de ciclo de vida”\n\nPreferencias\nLa versión del modelo que se considera es aquella en la que una unidad de consumo maximiza la utilidad esperada en un intervalo finito sujeto a un conjunto de restricciones. \\[\n\\max E_t \\left[ \\sum_{j=0}^{T-t}\\beta_{t+j}U\\left(C_{t+j},z_{t+j},v_{t+j}\\right) \\right]\n\\tag{1}\\] donde, \\(C\\) representa el “consumo”, \\(z\\) es un vector de variables observables que afecta a la utilidad, \\(v\\) es un vector para factores no observables que afectan a la utilidad, y \\(\\beta\\) es un factor de descuento.\nsujeto a las siguientes restricciones\n\\[\nW_{t+j+1}=W_{t+j}\\left(1+R_{t+j}^\\ast\\right) +y_{t+j} -C_{t+j},\n\\tag{2}\\]\n\\[\nW_{t+j}=\\sum_{i=1}^N A_{t+j}^i,\n\\tag{3}\\]\n\\[\nR_{t+j}^\\ast=\\sum_{i=0}^N \\omega_{t+j}^i R_{t+j}^i,\n\\tag{4}\\]\n\\[\nW_T\\geq0\n\\tag{5}\\]\ndonde, \\(W\\) es el patrimonio neto y su rendimiento, \\(\\omega\\) son las ponderaciones de la cartera, \\(R\\) son los rendimientos, \\(A\\) son los activos y \\(y\\) es el ingreso.\nLa restricción Ecuación 2 es una restricción presupuestaria genérica donde el valor neto aparece junto con su retorno, ingreso y consumo8. Las restricciones Ecuación 3 y Ecuación 4 definen el patrimonio neto, \\(W\\), y su rendimiento \\(-\\omega_{t+j}^i\\): son las acciones (o ponderaciones) de la cartera. El rendimiento del patrimonio neto está dado por el promedio ponderado de los rendimientos individuales, \\(R_{t+j}^i\\). Se supone que estos rendimientos no dependen de la posición neta que tome el consumidor sobre cada uno de estos activos, \\(A_{t+j}^i\\). (ver , Orazio P. Attanasio y Weber 2010)\nLa restricción Ecuación 5 da el límite para el patrimonio neto total en el periodo \\(T\\). El consumidor tiene que morir sin deuda, es decir tiene que pagar su deuda con probabilidad uno9.\nEn esta formulación se supone varias restricciones importantes: * El consumidor maximiza la utilidad esperada. * Las preferencias son aditivamente separables a lo largo del tiempo * Implícitamente es posible anotar la utilidad como una función de una sola mercancía. Esta practica presupone un teorema de agregación del tipo estudiado por (Gorman 1959).\nEl problema formulado anteriormente es capaz de abarcar diferentes versiones del modelo que se han considerado en la literatura. En particular, tratamos como casos especiales el modelo estándar de ingresos permanentes/ciclo de vida con preferencias cuadráticas, el llamado ahorro de existencias reguladoras, así como versiones flexibles del modelo (con un papel importante para la demografía y la oferta laboral) que se han ajustado a los datos.\nComencemos con un caso en el que la función de consumo se puede derivar analíticamente. Sea la utilidad cuadrática en \\(C\\) (y aditivamente separable en sus otros argumentos \\(z\\)), y suponga que al menos un activo financiero se negocia libremente y produce un rendimiento real fijo, igual al parámetro de referencia temporal constante \\(\\frac{1-\\beta}{\\beta}\\). La condición de primer orden con respecto al consumo, o ecuación de Euler, implica que el consumo es paseo aleatorio. \\[\n     E(C_{t+1}|I_t )=C_t\n\\tag{6}\\]\ndonde \\(I_t\\), denota la información disponible al instante \\(t\\) . En efecto notemos que al ser la función de utilidad cuadrática en \\(C\\) y aditivamente separable en sus otros argumentos (Hall 1978) no dice que se cumple exactamente \\(C_{t+1}=\\beta_0 +\\gamma C_t -\\varepsilon_{t+1}\\) de donde tomando la Esperanza al tiempo \\(t\\) y dado que se produce un rendimiento real fijo, tenemos la Ecuación 6.\nen \\(t\\), el consumidor escoge \\(C_t\\) tal que maximiza \\[\n     \\beta_0 U(C_t,z_t,v_t)+ E_t \\sum_{\\tau=t+1}^{T-t} \\beta_{\\tau+j}U(C_\\tau,z_\\tau,v_\\tau)\n\\tag{7}\\] sujeto a \\[\n    W_{\\tau+j}=W_{\\tau-1+j} \\left(1+R_{\\tau-1+j}^\\ast \\right) + y_{\\tau-1+j}-C_{\\tau-1+j}\n\\tag{8}\\] la estrategia secuencial óptima tiene la forma \\[\n    C_t=g_t(w_\\tau,w_{\\tau-1},\\dots,w_0,A_0)\n\\]\nconsiderando una variación desde esta estrategia\nSi los consumidores tienen expectativas racionales, entonces: \\[\n     C_{t+1}=C_t+\\varepsilon_{t+1} \\qquad E\\left( \\varepsilon_{t+1}|W_t \\right)=0\n\\tag{9}\\] para todas las variables \\(W\\) conocidas al instante \\(t\\). La ecuación Ecuación 9 se puede utilizar para derivar una función de consumo, en el caso de que no exista ningún tipo otro activo disponible para el consumidor (como en, Bewley 1977) y la única variable estocástica es la renta del trabajo. Sustituyendo en Ecuación 9 en las restricciones presupuestarias, (Flavin 1981) muestra que el consumo se iguala a la renta permanente, definido como la tasa de interés multiplicada por el valor presente de los ingresos actuales y futuros esperados:\n\\[\n     C_t=\\frac{r}{1+r}A_t +\\frac{r}{1+r}\\sum_{k=0}^\\infty E\\left(y_{t+k} | I_t\\right)\n\\tag{10}\\]\nLa ecuación Ecuación 10 se deriva para el caso especial de vida infinita, pero se puede derivar una extensión a la vida finita."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "href": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Otro Modelo de Consumo",
    "text": "Otro Modelo de Consumo\nUna vez que tenemos estas definiciones podemos entonces incluir que si se sigue a (J. Y. Campbell 1987) y se define el ahorro como \\[\n    s_t=\\frac{rA_t}{1+r} +y_t -C_t\n\\tag{11}\\]\nAhora si \\(P(L) y_t=a+\\zeta_t\\), donde \\(P(L)\\) es un polinomio en el operador de retardos y \\(\\zeta_t\\) es un ruido blanco. En este caso la ecuación Ecuación 10 implica (Flavin 1981) que: \\[\n    P\\left(\\frac{1}{1+r}\\right)\\Delta C_{t+1}= \\frac{r}{1+r}\\zeta_{t+1}\n\\tag{12}\\]\nluego podemos reescribir Ecuación 12 como: \\[\n    s_t=- \\sum_{k=1}^\\infty (1+r)^{-k} E\\left(\\Delta y_{t+k}|I_t \\right)\n\\tag{13}\\]\nLa ecuación Ecuación 13 muestra que los individuos deberían “ahorrar para tiempos difíciles” (los ingresos futuros caen), y se mantiene (por la ley de las proyecciones iteradas) incluso si consideramos las expectativas condicionadas a un subconjunto de la información utilizada por los agentes económicos, como el pasado. ingreso y ahorro.\n\nProposición 1 Si \\(C_t\\) es un paseo aleatorio y además los consumidores tienen expectativas racionales y por otro lado \\(Y_t\\sim AR(1)\\) y además \\(Y_t\\sim I(1)\\) entonces: \\[\nC_{t+1}=\\frac{1}{1- \\eta}C_t +\\frac{\\eta}{1-\\eta} y_{t+1} +\\frac{\\eta^2}{1-\\eta}A_{t+1}\n\\tag{14}\\] en el caso de no tener información disponible sobre \\(\\eta\\), donde \\(\\eta\\) es la razón entre la tasa de interés pasada y la tasa de interés actual, y no disponer información del activo \\(A_{t+1}\\), el modelo puedo escribirse como \\[\n    C_{t+1}=aC_t+bY_{t+1}+ u_{t+1}\n\\tag{15}\\] donde \\(u_{t+1}\\) denotaría la innovación al tiempo \\(t+1\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisión-de-los-datos",
    "href": "proyects/Tesis_JXBS/index.html#revisión-de-los-datos",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Revisión de los datos",
    "text": "Revisión de los datos\nEn esta sección se empezara con el análisis de las series de datos obtenidos del Banco Mundial las cuales son Households and NPISHs Final consumption expenditure per capita (constant 2015 US$) [NE.CON.PRVT.PC.KD] de los cuales se seleccionaran 14 países los cuales no tienen perdida de información. Estos países son: Bolivia, Brasil, Chile, Colombia, Costa Rica, Ecuador, Guatemala, Honduras, México, Nicaragua, Panamá, Paraguay, Perú, y República Dominicana.\nEn esta sección Hablaremos un poco de la comunidad andina que es de la cual se tiene conocimiento relevante en cuanto a su historia. La comunidad andina (CAN) esta integrada por los siguientes países: Bolivia, Colombia, Ecuador y Perú\n\nBolivia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 1: Serie para Bolivia original y corregida por software TRAMO/SEATS10\n\n\n\n\n\n\n\n\n\nFigura 2: Serie de irregularidades para Bolivia\n\n\n\nEn la figura Figura 2 se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1962\\)\n\\(1964\\) El gobierno se vio interrumpido por medio de un golpe militar, a partir de lo cual Bolivia habría de vivir dictaduras.\n\\(1972\\) Presencia de una dictadura en el gobierno y contrato de venta de gas a Argentina.\n\\(1977\\) Extraordinario nivel de precios de las materias primas (el estaño llegó a cotizarse en ocho dólares la libra ﬁna) y una gran apertura de créditos internacionales.\n\nEn la figura Figura 1 (b) se puede observar que la serie posee cierta tendencia que en principio debe ser estocástica puesto que la población y el desarrollo sigue en crecimiento.\n\n\nColombia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 3: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 4: Serie de irregularidades para Colombia\n\n\n\nEn la figura Figura 4} se observa que la serie presenta irregularidades que se describen a continuación\n\n\\(1964\\) Aparecen las FARC ,quienes aprobaron en su constitución un programa agrario que pretende la entrega gratuita de las tierras a los campesinos\n\\(1965\\) Empiezan los ataques de las guerrillas mismo que se extienden hasta estos días.\n\\(1992\\) Drásticos recortes de energía en este país, mismos que acarrearon perdidas millonarias.\n\\(1998\\) Se crea la “Zona de distensión” e inicia el proceso de paz con las guerrillas.\n\\(1999\\) Debido a la gran cantidad de demandas que se interpusieron frente a la voracidad de los bancos y contra el refinado mecanismo del UPAC, el cual estafó a quienes se arriesgaron a endeudarse para tener casa, la Corte Constitucional resolvió acabarlo con la sentencia C-700\n\\(2019\\)\n\\(2020\\) COVID\n\n\n\nEcuador\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 5: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 6: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura 6} se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1973\\) La apropiación de los beneficios del petróleo “la renta petrolera” se constituyó en objetivo de disputa de grupos sociales y organizaciones políticas.\n\\(1998\\) La permisividad social a favor de monopolios y oligopolios11\n\\(1999\\) Feriado Bancario\n\n\n\nPerú\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 7: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 8: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura 8 se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1983\\) Fenómeno del niño y caída del precio de los metales.\n\\(1985\\) El sol fue reemplazado por el inti con un valor de \\(1000\\) soles (devaluación de la moneda).\n\\(1987\\) Se empieza a sentir los efectos de las políticas intervencionistas implementadas un años atrás.\n\\(1989\\) Devaluación del inti\n\nA continuación en la figura Figura 9 presentamos las gráficas de Consumo y Renta para cada uno de los países de Latinoamérica y Caribe seleccionados para este estudio.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\nFigura 9: Gráfica de Series Consumo y Renta para los 14 países de Latinoámerica y Caribe\n\n\n\nEn la tabla Tabla 1 se realiza el contraste de Dickey-fuller aumentado (ADF) para las series corregidas.\n\n\n\nTabla 1: Contraste de raíz unitaria Dickey Fuller aumentado\n\n\n\n\n\nPaís\nRaiz Unitaria\n\\(p\\)-value\nlags\n\n\n\n\n\nRenta\n\n\n\n\nBolivia\nsi\n0,9861\n1\n\n\nBrasil\nsi\n0,9818\n1\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n1,0000\n0\n\n\nEcuador\nsi\n0,9680\n1\n\n\nGuatemala\nsi\n0,9965\n1\n\n\nHonduras\nsi\n0,9979\n0\n\n\nMéxico\nsi\n0,9945\n0\n\n\nNicaragua\nsi\n0,6621\n1\n\n\nPanamá\nsi\n0,9995\n0\n\n\nParaguay\nsi\n0,9966\n1\n\n\nPerú\nsi\n0,9830\n1\n\n\nR. Dominicana\nsi\n1,0000\n1\n\n\n\nConsumo\n\n\n\n\nBolivia\nsi\n0,9947\n1\n\n\nBrasil\nsi\n0,9982\n0\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n0,9978\n1\n\n\nEcuador\nsi\n0,9902\n0\n\n\nGuatemala\nsi\n0,9990\n1\n\n\nHonduras\nsi\n0,9998\n1\n\n\nMéxico\nsi\n0,9789\n1\n\n\nNicaragua\nsi\n0,6414\n1\n\n\nPanamá\nsi\n0,9975\n0\n\n\nParaguay\nsi\n1,0000\n0\n\n\nPerú\nsi\n0,9604\n1\n\n\nR. Dominicana\nsi\n1,0000\n0\n\n\n\n\n\n\nEn la tabla Tabla 1 se ha realizado el contraste ADF para cada una de las series tanto de Renta (PIB) como de Consumo (Consumo familiar). Es así que de manera general tenemos que para todos las series.\nDada la hipótesis nula de no estacionariedad frente a la alternativa de estacionariedad, El estadístico de contraste de ADF para el caso sin constante para el cual su distribución se encuentra tabulada por Dickey Fuller. Entonces para un nivel de significancia del \\(\\alpha=0.05\\) puesto que el valor \\(p\\) del estadístico es mayor que \\(\\alpha\\), No se rechazo la hipótesis nula de no estacionariedad.\nAhora bien se procederá a realizar el contraste de Johansen para el caso sin constante.\n\n\n\nTabla 2: Contraste de Cointegración de Johansen\n\n\n\n\n\n\n\n\n\n\n\n\nPaís\nCointegración\n\\(p\\)-valor asint.\nboot \\(p\\)-valor\nrango\n\n\n\n\nBolivia\nsi\n0,041\n0,089\n1\n\n\nBrasil\nsi\n0,345\n0,431\n1\n\n\nChile\nsi\n0,077\n0,131\n1\n\n\nColombia\nsi\n0,571\n0,814\n1\n\n\nCosta Rica\nsi\n0,297\n0,639\n1\n\n\nEcuador\nno\n0,018\n0,01\n1\n\n\nGuatemala\nsi\n0,602\n0,906\n1\n\n\nHonduras\nno\n0,082\n0,085\n0\n\n\nMéxico\nno\n0,144\n0,307\n0\n\n\nNicaragua\nno\n0,063\n0,079\n0\n\n\nPanamá\nsi\n0,247\n0,354\n1\n\n\nParaguay\nsi\n0,026\n0,055\n1\n\n\nPerú\nsi\n0,333\n0,499\n1\n\n\nR. Dominicana\nsi\n0,139\n0,379\n1\n\n\n\n\n\n\nEn la tabla Tabla 2 se ha realizado el contraste de cointegración de Johansen entre las series tanto de Renta (PIB) como de Consumo (Consumo familiar) para cada país. Es así que de manera general tenemos que para cada país.\nDada la hipótesis nula de que el rango de la matriz de cointegración es \\(0\\) frente a la alternativa de que el rango de cointegración es \\(1\\). Entonces tenemos que para Honduras, México, Nicaragua, las series de consumo y renta no están cointegradas pues para un nivel de significancia \\(alpha=0.05\\) no se rechaza la hipótesis nula de que el rango de la matriz de cointegración es \\(0\\). Por otro lado, para el resto de países menos Ecuador se tiene que dada la hipótesis nula rango de la matriz de cointegración es \\(1\\) frente a la alternativa rango de la matriz de cointegración es \\(2\\), para un nivel de significancia \\(\\alpha=0.05\\) no se rechaza la hipótesis nula de que el rango de cointegración es \\(1\\) con valores \\(p\\) dados en la tabla Tabla 2. Finalmente, para Ecuador se tienen que para un nivel de significancia \\(\\alpha=0.05\\) Se rechaza la hipótesis nula de que el rango de cointegración es \\(1\\).\n\n\n\nTabla 3: Contraste de no Causalidad de Dumitrescu-Hurlin entre Consumo y Renta\n\n\n\n\n\nPaís\n\\(Y\\) no causa \\(C\\)\np-valor\ncausalidad\n\n\n\n\nBolivia\nsi\n0,6174\nno\n\n\nBrasil\nno\n0,0002\nsi\n\n\nChile\nno\n0,0027\nsi\n\n\nColombia\nsi\n0,9225\nno\n\n\nCosta Rica\nsi\n0,2856\nno\n\n\nEcuador\nno\n0,0172\nsi\n\n\nGuatemala\nno\n0,0001\nsi\n\n\nHonduras\nsi\n0,6704\nno\n\n\nMéxico\nsi\n0,4979\nno\n\n\nNicaragua\nsi\n0,3064\nno\n\n\nPanamá\nsi\n0,5101\nno\n\n\nParaguay\nsi\n0,8212\nno\n\n\nPerú\nno\n0,0002\nsi\n\n\nR. Dominicana\nsi\n0,7106\nno\n\n\n\n\n\n\nEn la tabla Tabla 3 se ha realizado un resumen del contraste de no causalidad de Dumitrescu-Hurlin para cada modelo de consumo de cada país. Dada la hipótesis nula de \\(Y\\) no causa \\(X\\) frente a la hipótesis alternativa \\(Y\\) causa \\(X\\). tenemos que para los siguientes países: Brasil, Chile, Ecuador, Guatemala, Perú. Para un nivel de significancia \\(\\alpha=0.05\\) se rechaza la hipótesis nula de no causalidad con valores \\(p\\) dados en la tabla Tabla 3. Para el resto de países no se rechaza la hipótesis nula de no causalidad."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#resultados",
    "href": "proyects/Tesis_JXBS/index.html#resultados",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Resultados",
    "text": "Resultados\nEn la tabla Tabla 1 se ha evidenciado que tanto las series de Consumo así como las series De Renta poseen una raíz unitaria es decir son series integradas de orden uno. Además se observa que para la mayoría de países el valor \\(p\\) es mayor a \\(0.95\\) es decir tenemos una gran certeza de que nuestras series son no estacionarias, lo cual nos permite realizar el análisis sobre el modelo Ecuación 15.\nAhora bien, una vez tenemos que las series son no estacionarias en la tabla Tabla 2 se ha realizado la prueba de cointegración de Johansen para el caso sin constante. De donde, se ha obtenido que los países de Ecuador, Honduras, México, y Nicaragua no se encuentra cointegrados esto puede ser debido a que para estos países las series tanto de Consumo y Renta al momento de analizar las intervenciones para esta se evidencio que las series tienen ciertas irregularidades que si bien se esperaba que quedaran enterradas en las innovaciones, estas no pueden absorberlas del todo.\nFinalmente, para los países cuyo modelo si esta cointegrado se ha realizado un contraste de causalidad viendo a estos datos como un panel de series apiladas. Es así que se ha obtenido que para países cuyas intervenciones han sido absorbidas por las innovaciones el modelo propuesto en este trabajo ayuda a mejorar la predicción del consumo."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#conclusiones",
    "href": "proyects/Tesis_JXBS/index.html#conclusiones",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Conclusiones",
    "text": "Conclusiones\nDado que uno de los objetivos de este trabajo fue revisar los modelos de ciclo de vida y funciones de consumo desde el punto de vista neoclásico, para los cuales si bien el lineamiento esta bastante acertado, en este trabajo se ha propuesto un modelo que mejora el modelo neoclásico. Esto se evidencia para países en los que los datos no tienen intervenciones de gran impacto es decir que su perturbaciones pueden ser absorbidas por las innovaciones al instante \\(t+1\\). El modelo Ecuación 15 ayuda a predecir de manera mas eficiente el Consumo; debido a que para estos países el tener información acerca de la renta en cada instante ayuda a predecir el consumo al instante \\(t+1\\). Por otro lado para los países, para los cuales las innovaciones causan impactos demasiado grandes, no se puede decir con certeza que el modelo propuesto no seria bueno. Puesto que, si se lograra mitigar los mencionados impactos en el tratamiento de la data podría darse cualquier resultado. Desde un punto de vista econométrico el modelo propuesto bajo las condiciones impuestas ayuda a establecer una relación entre la Renta y el Consumo esta relación es del tipo causal de Granger."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#footnotes",
    "href": "proyects/Tesis_JXBS/index.html#footnotes",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Notas",
    "text": "Notas\n\n\nUna ecuación de Euler del consumo, a grandes rasgos, es una condición matemática que describe el comportamiento de una senda óptima de consumo bajo los supuestos de elección intertemporal, expectativas racionales y agente representativo, entre otros, (Parker 2007)↩︎\nLas series integradas son un caso particular de series no estacionarias. Se dice que una serie temporal \\(x_t\\) es integrada de orden \\(d\\), \\(I(d)\\), cuando es necesario diferenciarla \\(d\\) veces para convertirla en estacionaria (Engle y Granger 1987) ↩︎\nEntonces se puede considerar como la varianza cruzada del consumo para una cohorte de individuos.↩︎\n\\(u_t\\) innovaciones en la renta permanente↩︎\nPor el TCL, \\(\\sum u_t \\overset{d}{\\sim} Normal\\) con \\(u_t\\) independientes , bajo supuestos de regularidad incluso si \\(u_t \\not\\sim Normal\\)↩︎\nAl igual que con otros documentos, rechazan enérgicamente la suposición de una perfecta distribución del riesgo.↩︎\nSegún (Orazio P. Attanasio y Weber 2010) esto parece indicar que, de alguna manera, en altas frecuencias los choques salariales son absorbidos y no se reflejan en el consumo.↩︎\nPor ejemplo, es posible que el ingreso esté dado por la tasa de salario multiplicada por el número de horas trabajadas, donde el número de horas es uno de los componentes de \\(z\\)↩︎\nEsta simple restricción impone limitaciones cuantitativamente importantes a la capacidad de suavizar el consumo↩︎\nTRAMO significa “Time series Regression with ARIMA noise, Missing values and Outliers” y SEATS “Signal Extraction in ARIMA Time Series”. Estos programas (que normalmente se usan juntos) han sido desarrollados por Víctor Gómez y Agustín Maravall del Banco de España.↩︎\nDado el fenómeno del niño (1995) el cual fue uno de los desastres naturales mas grandes que ha impactado al Ecuador seria el principio de la bola de nieve que desencadenaría en 1999 con el ya tan conocido “Feriado Bancario”↩︎"
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#importamos-datos-desde-un-script-en-python",
    "href": "posts/2024-07-12_Cross_validation/index.html#importamos-datos-desde-un-script-en-python",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "En esta ocasión el código que usaremos será el siguiente.\n\n\nCódigo\n# Librerias a utilizar\nimport pandas as pd\nimport numpy as np\n\n# Cargamos los datos desde el archivo csv\ndf = pd.read_csv(\"StudentsPerformance.csv\")\n\n# Procesamos los datos \n# Cambio de nombre de las columnas \ndf = df.rename(columns={'gender':'genero', 'race/ethnicity': 'raza',\n                        'parental level of education': 'educacion padres',\n                        'lunch': 'almuerzo', 'test preparation course': 'curso de preparacion',\n                        'math score': 'nota matematicas', 'reading score': 'nota lectura',\n                        'writing score': 'nota escritura'})\n#cambiamos los valores a español\ndf['genero'] = np.where(df['genero'] == 'male', 'masculino', 'femenino')\ndf['raza'] = df['raza'].str.replace('group','grupo')\ndf['educacion padres'] = np.where(df['educacion padres'] == 'some college', 'universidad incompleta',\n                                  np.where(df['educacion padres'] == \"associate's degree\", 'titulo de asociado',\n                                           np.where(df['educacion padres']=='high school', 'secundaria',\n                                                    np.where(df['educacion padres']== 'some high school', 'secundaria incompleta',\n                                                             np.where(df['educacion padres']== \"bachelor's degree\", \"licenciatura\",\n                                                                      \"maestria\")))))\n                                                                      \n                                                                      \ndf['almuerzo'] = np.where(df['almuerzo'] == 'standard', 'estandar', 'gratis/reducido')\ndf['curso de preparacion'] = np.where(df['curso de preparacion'] == 'none', 'ninguno', 'completado')\n\n\nescribimos el código y damos clic en OK\n ### Seleccionamos el df y damos clic en cargar y tenemos nuestros datos listo para usarlos o si deseamos podemos realizar otras transformaciones a los datos."
  },
  {
    "objectID": "posts/2024-07-12_Cross_validation/index.html#sé-puede-insertar-elementos-visuales-de-python-en-power-bi",
    "href": "posts/2024-07-12_Cross_validation/index.html#sé-puede-insertar-elementos-visuales-de-python-en-power-bi",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "Sí, veamos un ejemplo:\nEn el panel de visualizaciones seleccionamos Py\n Pasamos como valores las variables que vamos a graficar Alerta: como Power BI elimina duplicados automáticamente necesitamos pasar una variable index\n\nEn el editor de script de python escribimos el código para generar la gráfica\n\nEl código que vamos a colocar en este script en este caso es el siguiente:\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfig, ax = plt.subplots(nrows = 2, ncols =3, figsize=(20,7))\nsb.histplot(data=dataset, x='nota matematicas', ax= ax[0,0],color='lightblue')\nsb.histplot(data=dataset, x='nota escritura', ax = ax[0,1], color='lightgray')\nsb.histplot(data=dataset, x = 'nota lectura', ax =ax[0,2], color ='turquoise')\n# Añadimos la media a cada gráfico\np_mate =dataset['nota matematicas'].mean()\np_escr = dataset['nota escritura'].mean()\np_lect = dataset['nota lectura'].mean()\nax[0,0].axvline(p_mate, color='red', linestyle='--')\nax[0,1].axvline(p_escr, color='red', linestyle='--')\nax[0,2].axvline(p_lect, color='red', linestyle='--')\n#añadimos una etiqueta a la media\nax[0,0].annotate(f'{p_mate: .2f}', (p_mate,0), color='red',size= 18)\nax[0,1].annotate(f'{p_escr: .2f}', (p_escr,0), color='red',size= 18)\nax[0,2].annotate(f'{p_lect: .2f}', (p_lect,0), color='red',size= 18)\n# Añadimos los titulos\nax[0,0].set_title('Histograma de notas de Matemáticas')\nax[0,1].set_title('Histograma de notas de Escritura')\nax[0,2].set_title('Histograma de notas de Lectura')\n\n#Boxplots\nsb.boxplot(data=dataset, x='nota matematicas', ax=ax[1,0], showmeans=True, color = 'lightblue')\nsb.boxplot(data=dataset, x='nota lectura', ax=ax[1,1], showmeans=True, color ='lightgray')\nsb.boxplot(data=dataset, x='nota escritura', ax=ax[1,2], showmeans=True, color= 'turquoise')\n\nplt.tight_layout()\nplt.show()\n\n\nFinalmente creamos el dashboard de control añadiendo algunos segmentadores o lo que necesitemos\n\nDe esta manera podemos combinar python y Power BI para tener un tablero interactivo"
  },
  {
    "objectID": "posts/2025-02-10-Power-BI-python/index.html",
    "href": "posts/2025-02-10-Power-BI-python/index.html",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "La respuesta es sí, veamoslo con el siguiente ejemplo sencillo\n\n\n\n\n\n\n\n\n\nEn esta ocasión el código que usaremos será el siguiente.\n\n\nCódigo\n# Librerias a utilizar\nimport pandas as pd\nimport numpy as np\n\n# Cargamos los datos desde el archivo csv\ndf = pd.read_csv(\"StudentsPerformance.csv\")\n\n# Procesamos los datos \n# Cambio de nombre de las columnas \ndf = df.rename(columns={'gender':'genero', 'race/ethnicity': 'raza',\n                        'parental level of education': 'educacion padres',\n                        'lunch': 'almuerzo', 'test preparation course': 'curso de preparacion',\n                        'math score': 'nota matematicas', 'reading score': 'nota lectura',\n                        'writing score': 'nota escritura'})\n#cambiamos los valores a español\ndf['genero'] = np.where(df['genero'] == 'male', 'masculino', 'femenino')\ndf['raza'] = df['raza'].str.replace('group','grupo')\ndf['educacion padres'] = np.where(df['educacion padres'] == 'some college', 'universidad incompleta',\n                                  np.where(df['educacion padres'] == \"associate's degree\", 'titulo de asociado',\n                                           np.where(df['educacion padres']=='high school', 'secundaria',\n                                                    np.where(df['educacion padres']== 'some high school', 'secundaria incompleta',\n                                                             np.where(df['educacion padres']== \"bachelor's degree\", \"licenciatura\",\n                                                                      \"maestria\")))))\n                                                                      \n                                                                      \ndf['almuerzo'] = np.where(df['almuerzo'] == 'standard', 'estandar', 'gratis/reducido')\ndf['curso de preparacion'] = np.where(df['curso de preparacion'] == 'none', 'ninguno', 'completado')\n\n\nescribimos el código y damos clic en OK\n ### Seleccionamos el df y damos clic en cargar y tenemos nuestros datos listo para usarlos o si deseamos podemos realizar otras transformaciones a los datos.\n\n\n\n\n\nSí, veamos un ejemplo:\nEn el panel de visualizaciones seleccionamos Py\n\nPasamos como valores las variables que vamos a graficar Alerta: como Power BI elimina duplicados automáticamente necesitamos pasar una variable index\n\nEn el editor de script de python escribimos el código para generar la gráfica\n\nEl código que vamos a colocar en este script en este caso es el siguiente:\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfig, ax = plt.subplots(nrows = 2, ncols =3, figsize=(20,7))\nsb.histplot(data=dataset, x='nota matematicas', ax= ax[0,0],color='lightblue')\nsb.histplot(data=dataset, x='nota escritura', ax = ax[0,1], color='lightgray')\nsb.histplot(data=dataset, x = 'nota lectura', ax =ax[0,2], color ='turquoise')\n# Añadimos la media a cada gráfico\np_mate =dataset['nota matematicas'].mean()\np_escr = dataset['nota escritura'].mean()\np_lect = dataset['nota lectura'].mean()\nax[0,0].axvline(p_mate, color='red', linestyle='--')\nax[0,1].axvline(p_escr, color='red', linestyle='--')\nax[0,2].axvline(p_lect, color='red', linestyle='--')\n#añadimos una etiqueta a la media\nax[0,0].annotate(f'{p_mate: .2f}', (p_mate,0), color='red',size= 18)\nax[0,1].annotate(f'{p_escr: .2f}', (p_escr,0), color='red',size= 18)\nax[0,2].annotate(f'{p_lect: .2f}', (p_lect,0), color='red',size= 18)\n# Añadimos los titulos\nax[0,0].set_title('Histograma de notas de Matemáticas')\nax[0,1].set_title('Histograma de notas de Escritura')\nax[0,2].set_title('Histograma de notas de Lectura')\n\n#Boxplots\nsb.boxplot(data=dataset, x='nota matematicas', ax=ax[1,0], showmeans=True, color = 'lightblue')\nsb.boxplot(data=dataset, x='nota lectura', ax=ax[1,1], showmeans=True, color ='lightgray')\nsb.boxplot(data=dataset, x='nota escritura', ax=ax[1,2], showmeans=True, color= 'turquoise')\n\nplt.tight_layout()\nplt.show()\n\n\nFinalmente creamos el dashboard de control añadiendo algunos segmentadores o lo que necesitemos\n\nDe esta manera podemos combinar python y Power BI para tener un tablero interactivo"
  },
  {
    "objectID": "posts/2025-02-10-Power-BI-python/index.html#importamos-datos-desde-un-script-en-python",
    "href": "posts/2025-02-10-Power-BI-python/index.html#importamos-datos-desde-un-script-en-python",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "En esta ocasión el código que usaremos será el siguiente.\n\n\nCódigo\n# Librerias a utilizar\nimport pandas as pd\nimport numpy as np\n\n# Cargamos los datos desde el archivo csv\ndf = pd.read_csv(\"StudentsPerformance.csv\")\n\n# Procesamos los datos \n# Cambio de nombre de las columnas \ndf = df.rename(columns={'gender':'genero', 'race/ethnicity': 'raza',\n                        'parental level of education': 'educacion padres',\n                        'lunch': 'almuerzo', 'test preparation course': 'curso de preparacion',\n                        'math score': 'nota matematicas', 'reading score': 'nota lectura',\n                        'writing score': 'nota escritura'})\n#cambiamos los valores a español\ndf['genero'] = np.where(df['genero'] == 'male', 'masculino', 'femenino')\ndf['raza'] = df['raza'].str.replace('group','grupo')\ndf['educacion padres'] = np.where(df['educacion padres'] == 'some college', 'universidad incompleta',\n                                  np.where(df['educacion padres'] == \"associate's degree\", 'titulo de asociado',\n                                           np.where(df['educacion padres']=='high school', 'secundaria',\n                                                    np.where(df['educacion padres']== 'some high school', 'secundaria incompleta',\n                                                             np.where(df['educacion padres']== \"bachelor's degree\", \"licenciatura\",\n                                                                      \"maestria\")))))\n                                                                      \n                                                                      \ndf['almuerzo'] = np.where(df['almuerzo'] == 'standard', 'estandar', 'gratis/reducido')\ndf['curso de preparacion'] = np.where(df['curso de preparacion'] == 'none', 'ninguno', 'completado')\n\n\nescribimos el código y damos clic en OK\n ### Seleccionamos el df y damos clic en cargar y tenemos nuestros datos listo para usarlos o si deseamos podemos realizar otras transformaciones a los datos."
  },
  {
    "objectID": "posts/2025-02-10-Power-BI-python/index.html#sé-puede-insertar-elementos-visuales-de-python-en-power-bi",
    "href": "posts/2025-02-10-Power-BI-python/index.html#sé-puede-insertar-elementos-visuales-de-python-en-power-bi",
    "title": "Integración de Python en Power BI",
    "section": "",
    "text": "Sí, veamos un ejemplo:\nEn el panel de visualizaciones seleccionamos Py\n\nPasamos como valores las variables que vamos a graficar Alerta: como Power BI elimina duplicados automáticamente necesitamos pasar una variable index\n\nEn el editor de script de python escribimos el código para generar la gráfica\n\nEl código que vamos a colocar en este script en este caso es el siguiente:\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfig, ax = plt.subplots(nrows = 2, ncols =3, figsize=(20,7))\nsb.histplot(data=dataset, x='nota matematicas', ax= ax[0,0],color='lightblue')\nsb.histplot(data=dataset, x='nota escritura', ax = ax[0,1], color='lightgray')\nsb.histplot(data=dataset, x = 'nota lectura', ax =ax[0,2], color ='turquoise')\n# Añadimos la media a cada gráfico\np_mate =dataset['nota matematicas'].mean()\np_escr = dataset['nota escritura'].mean()\np_lect = dataset['nota lectura'].mean()\nax[0,0].axvline(p_mate, color='red', linestyle='--')\nax[0,1].axvline(p_escr, color='red', linestyle='--')\nax[0,2].axvline(p_lect, color='red', linestyle='--')\n#añadimos una etiqueta a la media\nax[0,0].annotate(f'{p_mate: .2f}', (p_mate,0), color='red',size= 18)\nax[0,1].annotate(f'{p_escr: .2f}', (p_escr,0), color='red',size= 18)\nax[0,2].annotate(f'{p_lect: .2f}', (p_lect,0), color='red',size= 18)\n# Añadimos los titulos\nax[0,0].set_title('Histograma de notas de Matemáticas')\nax[0,1].set_title('Histograma de notas de Escritura')\nax[0,2].set_title('Histograma de notas de Lectura')\n\n#Boxplots\nsb.boxplot(data=dataset, x='nota matematicas', ax=ax[1,0], showmeans=True, color = 'lightblue')\nsb.boxplot(data=dataset, x='nota lectura', ax=ax[1,1], showmeans=True, color ='lightgray')\nsb.boxplot(data=dataset, x='nota escritura', ax=ax[1,2], showmeans=True, color= 'turquoise')\n\nplt.tight_layout()\nplt.show()\n\n\nFinalmente creamos el dashboard de control añadiendo algunos segmentadores o lo que necesitemos\n\nDe esta manera podemos combinar python y Power BI para tener un tablero interactivo"
  }
]