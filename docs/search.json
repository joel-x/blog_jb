[
  {
    "objectID": "proyects.html",
    "href": "proyects.html",
    "title": "Proyectos",
    "section": "",
    "text": "Manejo de ETL’s\n\n\n\nPython\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\nJoel Burbano\n\n\n8 ago 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEstudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)\n\n\n\nRenta permanente\n\n\nModelo de Ciclo de vida\n\n\nEcuación de Euler\n\n\nCausalidad\n\n\nModelos de Consumo\n\n\n\nEl estudio del modelo teórico del consumo es un tema de relevancia tanto a nivel de individuo como nivel de hogares. Es así que, en este trabajo se realiza una nueva…\n\n\n\n\n\n\n23 feb 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyects/ETL/index.html",
    "href": "proyects/ETL/index.html",
    "title": "Manejo de ETL’s",
    "section": "",
    "text": "Automatización de Procesos ETL\nEn este proyecto vamos a automatizar un proceso ETL, el mismo que tiene los siguientes pasos\n\nExtraer los: En nuestro caso utilizaremos los datos de “World Happiness report”\nTransformar los datos\nCargar los datos: se los cargarara en una base de SQLite\n\nViaualización del dataset\n\n\n\n  \n\n\n\n\nPythonR\n\n\n\n\nCódigo\n\nimport pandas as pd\nimport sqlite3\nimport logging\n\n\n# Configuración básica de logging\nlogging.basicConfig(filename='etl_processP.log', level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef extract_data():\n    try:\n        df = pd.read_csv('world_happiness_report.csv')\n        logging.info(\"Extracción de datos completada con éxito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la extracción de datos: %s\", e)\n        raise\n\ndef transform_data(df):\n    try:\n        df = df.drop(columns=['Standard Error'])\n        df.iloc[:,2:].fillna(df.iloc[:,2:].mean(), inplace=True)\n        df = df.drop_duplicates()\n        df['Economy (GDP per Capita)'] = df['Economy (GDP per Capita)'] / df['Economy (GDP per Capita)'].max()\n        df['Happiness Level'] = df['Happiness Score'].apply(lambda x: 'High' if x &gt; df['Happiness Score'].mean() else 'Low')\n        logging.info(\"Transformación de datos completada con éxito.\")\n        return df\n    except Exception as e:\n        logging.error(\"Error durante la transformación de datos: %s\", e)\n        raise\n\ndef load_data(df):\n    try:\n        conn = sqlite3.connect('world_happiness.db')\n        cursor = conn.cursor()\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS happiness_data (\n            Country TEXT,\n            Region TEXT,\n            Happiness_Rank INTEGER,\n            Happiness_Score REAL,\n            GDP_per_capita REAL,\n            Happiness_Level TEXT\n        )\n        ''')\n        conn.commit()\n        df.to_sql('happiness_data', conn, if_exists='replace', index=False)\n        conn.close()\n        logging.info(\"Carga de datos completada con éxito.\")\n    except Exception as e:\n        logging.error(\"Error durante la carga de datos: %s\", e)\n        raise\n\ndef etl_process():\n    try:\n        logging.info(\"Inicio del proceso ETL.\")\n        data = extract_data()\n        transformed_data = transform_data(data)\n        load_data(transformed_data)\n        logging.info(\"Proceso ETL completado con éxito.\")\n    except Exception as e:\n        logging.critical(\"El proceso ETL falló: %s\", e)\n        raise\n\nif __name__ == \"__main__\":\n    etl_process()\n\n\n\n\n\n\nCódigo\n# Cargar las bibliotecas necesarias\n#install.packages(\"dplyr\")\n#install.packages(\"readr\")\n#install.packages(\"DBI\")\n#install.packages(\"RSQLite\")\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Configuración básica de logging\nlog_file &lt;- \"etl_processR.log\"\nlog_message &lt;- function(level, message) {\n  cat(sprintf(\"%s - %s - %s\\n\", Sys.time(), level, message), file = log_file, append = TRUE)\n}\n\n# Función para extraer datos\nextract_data &lt;- function() {\n  tryCatch({\n    df &lt;- read_csv('world_happiness_report.csv')\n    log_message(\"INFO\", \"Extracción de datos completada con éxito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la extracción de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función para transformar datos\ntransform_data &lt;- function(df) {\n  tryCatch({\n    df &lt;- df %&gt;% \n      select(-`Standard Error`) %&gt;%\n      mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))) %&gt;%\n      distinct() %&gt;%\n      mutate(`Economy (GDP per Capita)` = `Economy (GDP per Capita)` / max(`Economy (GDP per Capita)`, na.rm = TRUE)) %&gt;%\n      mutate(`Happiness Level` = ifelse(`Happiness Score` &gt; mean(`Happiness Score`, na.rm = TRUE), 'High', 'Low'))\n    \n    log_message(\"INFO\", \"Transformación de datos completada con éxito.\")\n    return(df)\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la transformación de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función para cargar datos en SQLite\nload_data &lt;- function(df) {\n  tryCatch({\n    conn &lt;- dbConnect(SQLite(), dbname = \"world_happiness.db\")\n    \n    dbExecute(conn, '\n    CREATE TABLE IF NOT EXISTS happiness_data (\n      Country TEXT,\n      Region TEXT,\n      Happiness_Rank INTEGER,\n      Happiness_Score REAL,\n      GDP_per_capita REAL,\n      Happiness_Level TEXT\n    )')\n    \n    dbWriteTable(conn, \"happiness_data\", df, overwrite = TRUE, row.names = FALSE)\n    dbDisconnect(conn)\n    log_message(\"INFO\", \"Carga de datos completada con éxito.\")\n  }, error = function(e) {\n    log_message(\"ERROR\", paste(\"Error durante la carga de datos:\", e$message))\n    stop(e)\n  })\n}\n\n# Función principal del proceso ETL\netl_process &lt;- function() {\n  tryCatch({\n    log_message(\"INFO\", \"Inicio del proceso ETL.\")\n    data &lt;- extract_data()\n    transformed_data &lt;- transform_data(data)\n    load_data(transformed_data)\n    log_message(\"INFO\", \"Proceso ETL completado con éxito.\")\n  }, error = function(e) {\n    log_message(\"CRITICAL\", paste(\"El proceso ETL falló:\", e$message))\n    stop(e)\n  })\n}\n\n# Ejecutar el proceso ETL\nif (interactive()) {\n  etl_process()\n}"
  },
  {
    "objectID": "posts/Linear_Regression/index.html",
    "href": "posts/Linear_Regression/index.html",
    "title": "Regresión Lineal Simple",
    "section": "",
    "text": "En este post presentamos un ejemplo básico de regresión lineal simple, es decir, el caso de ajustar una línea a datos \\(x,y\\).\nAntes que nada importamos las librerias necesarias\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\nPrimeramente generamos unos puntos para \\(x\\) y para \\(y\\)\n\n\nCódigo\nrng=np.random.RandomState(42)\nx=18*rng.rand(40)\ny=1.5*x-3+rng.rand(40)\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora definimos los hiperparámetros de nuestro modelo\n\n\nCódigo\nLineal=LinearRegression(fit_intercept=True) # La regresión lineal es de la forma y=ax+b, donde b!=0\n\n\nDefinimos la variable \\(X\\)\n\n\nCódigo\nX=x[:,np.newaxis]\nX.shape\n\n\n(40, 1)\n\n\nNota: en este caso como estamos trabajando con datos simulados no es necesario definir \\(y\\) puesto que sklearn si nos permite ingresar ese array\nAhor ajustamos el modelo\n\n\nCódigo\nLineal.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nObservemos el valor del coeficiente (\\(a\\))\n\n\nCódigo\nLineal.coef_\n\n\narray([1.50922213])\n\n\nObservemos el valor del intercepto (\\(b\\))\n\n\nCódigo\nLineal.intercept_\n\n\n-2.60022468062337\n\n\nProbemos el poder predictivo de nuestro modelo\n\n\nCódigo\nxfit=np.arange(17,22,0.5)\nXfit=xfit[:,np.newaxis]\nyfit=Lineal.predict(Xfit)\n\n\nAhora realizemos una visualización\n\n\nCódigo\ndef f(x):\n  return 1.51*x-2.6\n\nplt.scatter(x,y)\nplt.scatter(xfit,yfit,color='red')\nplt.plot(range(-1,22),[f(i) for i in range(-1,22)],color='cyan')\nplt.legend(['Datos modelo','Datos predichos','y=ax+b'])\nplt.show()"
  },
  {
    "objectID": "posts/Deteccion_fraude/index.html",
    "href": "posts/Deteccion_fraude/index.html",
    "title": "Detección de fraude con tarjetas de Crédito",
    "section": "",
    "text": "En el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias recolectadas durante dos días en el mes de de Septiembre del 2013 por European cardholders\nEmpezaremos por importar las librerias necesarias para realizar el análisis\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\nProcedemos a leer los datos con pandas\n\n\nCódigo\n#df: base de datos de las transacciones\ndf=pd.read_csv(\"creditcard.csv.zip\")\n\n\nVerificamos que cantidad de datos tenemos\n\n\nCódigo\ndf.shape\n\n\n(284807, 31)\n\n\nTenemos que existen 31 columnas (variables) y 284807 filas (registros)\nAhora bien procedemos a revisar la calidad de este conjunto de datos\n\n\nCódigo\ndf.isnull().any()\n\n\nTime      False\nV1        False\nV2        False\nV3        False\nV4        False\nV5        False\nV6        False\nV7        False\nV8        False\nV9        False\nV10       False\nV11       False\nV12       False\nV13       False\nV14       False\nV15       False\nV16       False\nV17       False\nV18       False\nV19       False\nV20       False\nV21       False\nV22       False\nV23       False\nV24       False\nV25       False\nV26       False\nV27       False\nV28       False\nAmount    False\nClass     False\ndtype: bool\n\n\nObservamos que no hay variables con datos nulos.\nAhora bien echemos un vistazo a la variable Class la cual contiene la información sobre las transacciones fraudulentas\n\n\nCódigo\ndf[\"Class\"].value_counts()\n\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nNotamos que solamente 492 transacciones son fraudulentas\n\n\nCódigo\ndf['Class'].value_counts(normalize=True)\n\n\nClass\n0    0.998273\n1    0.001727\nName: proportion, dtype: float64\n\n\nes decir solo el \\(0.17\\%\\) de transacciones son fraudulentas\nAhora bien empecemos a intentar predecir\n\n\nCódigo\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCódigo\nX=df.drop(labels='Class',axis=1)\ny=df.loc[:,'Class']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=1, stratify=y)\n\n\nAhora bien, se realiza un análisis exploratorio de los datos\n\n\nCódigo\nX_train['Time'].describe()\n\n\ncount    199364.000000\nmean      94675.212852\nstd       47536.519022\nmin           0.000000\n25%       54039.000000\n50%       84588.500000\n75%      139243.250000\nmax      172792.000000\nName: Time, dtype: float64\n\n\nRealicemos una conversión de la variable Time de segundos a horas para facilitar la interpretación\n\n\nCódigo\nX_train.loc[:,'Time']=X_train.Time/3600\nX_test.loc[:,'Time']=X_test.Time/3600\n\n\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Time'],bins=40,kde=False)\nplt.xlim([0,40])\nplt.xticks(np.arange(0,48,6))\nplt.xlabel('Tiempo despues de la primera transacción (h)')\nplt.ylabel('Conteo')\nplt.title('Tiempo de transacciones')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nNotamos que existen dos picos en la gráfica, el primero entre las 10 y 22 primeras horas y el segundo entre las 34 y 40 horas\nAnalicemos la variable Amount\n\n\nCódigo\nX_train['Amount'].describe()\n\n\ncount    199364.000000\nmean         88.659351\nstd         247.240287\nmin           0.000000\n25%           5.637500\n50%          22.000000\n75%          78.000000\nmax       25691.160000\nName: Amount, dtype: float64\n\n\nrealicemos una revisión gráfica\nprimero un histograma\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Amount'],bins=50,kde=False)\nplt.ylabel('Conteo')\nplt.title('Montos de Transacción')\nplt.show()\n\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nplt.figure(figsize=(12,4))\nsb.boxplot(x=X_train[\"Amount\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nObservemos que los datos se encuentran fuertemente sesgados a la derecha. Para asegurarnos calculamos la asimetría\n\n\nCódigo\nX_train['Amount'].skew()\n\n\n16.950540423177653\n\n\n\n\nCódigo\nX_train.head(5)\n\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\n\n\n\n\n105644\n19.340833\n1.135011\n-0.663898\n0.703924\n0.069871\n-0.488154\n1.312078\n-0.897198\n0.463148\n-0.478801\n0.396879\n0.268544\n0.752264\n0.264092\n-0.252977\n0.515272\n-3.211514\n1.648897\n-1.297012\n-2.246958\n-0.677938\n-0.331487\n-0.069644\n0.183987\n-0.618678\n0.089015\n0.521419\n0.086390\n0.004782\n1.00\n\n\n139790\n23.155278\n-1.786262\n1.118886\n1.347969\n-0.379954\n-1.240680\n0.467667\n0.081125\n0.964933\n0.042585\n-1.275754\n-1.871478\n0.375166\n0.692938\n-0.149358\n-0.396145\n0.802805\n-0.405073\n0.153925\n-0.241419\n-0.099266\n-0.047902\n-0.182530\n-0.162509\n-0.405178\n0.512595\n0.299398\n-0.042882\n-0.059130\n141.73\n\n\n158758\n31.034167\n-0.683414\n0.679341\n2.615556\n2.362138\n-0.012716\n0.603826\n0.574245\n-0.679978\n-0.811409\n2.035115\n-0.564418\n-1.407557\n-0.094656\n-0.859411\n1.749530\n0.099132\n-0.035668\n-0.053624\n1.656191\n0.372610\n-0.007167\n0.463597\n-0.243134\n0.084557\n-0.453177\n2.687676\n-1.084269\n-0.511626\n36.19\n\n\n130845\n22.067778\n1.183540\n-0.493000\n0.755202\n-0.963160\n-0.850295\n0.145905\n-0.794616\n0.302199\n1.656943\n-0.939787\n1.101727\n1.138109\n-0.592931\n0.156943\n0.903035\n-0.419096\n-0.207755\n0.403235\n0.614310\n-0.168134\n0.039588\n0.339340\n-0.053125\n-0.298049\n0.423994\n-0.652284\n0.102582\n0.017292\n1.00\n\n\n88908\n17.318056\n1.137583\n0.105478\n0.784402\n1.254973\n-0.600870\n-0.360836\n-0.161727\n0.076092\n0.280587\n0.015787\n1.084154\n1.016011\n-0.666982\n0.250706\n-0.835022\n-0.130522\n-0.216624\n-0.058071\n0.265563\n-0.178887\n-0.195692\n-0.443664\n0.046270\n0.516246\n0.447943\n-0.554949\n0.031821\n0.018177\n7.60"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "",
    "text": "Código\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npd.options.display.max_columns=None #para que se despliegue todas las columnas\n\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom matplotlib import cm\nc4=cm.get_cmap('Set3')"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#frecuencia-de-los-tipos-de-acciedentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Frecuencia de los tipos de acciedentes",
    "text": "Frecuencia de los tipos de acciedentes\n\n\nCódigo\nsb.countplot(x=df['Collision Type'],palette='Set3')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Frecuencia de tipos de Accidentes\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que existe una mayor cantidad de accidentes entre dos vehículos, por otro lado la cantidad de accidentes de bicicletas es la menor."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#accidentes-ocurridos-entre-semana-vs-fin-de-semana",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Accidentes ocurridos entre semana vs fin de semana",
    "text": "Accidentes ocurridos entre semana vs fin de semana\n\n\nCódigo\nsb.countplot(x=df['Weekend?'],palette='Accent')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=65)\nplt.title(\"Entre semana vs Fin de semana\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el Fin de semana es cuando mas ocurren accidentes."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#porcentaje-de-lesiones-por-categoria",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Porcentaje de lesiones por Categoria",
    "text": "Porcentaje de lesiones por Categoria\n\n\nCódigo\nles_val=df['Injury Type'].value_counts()\nles_val\n\n\nInjury Type\nNo injury/unknown     41603\nNon-incapacitating    11136\nIncapacitating         1089\nFatal                   115\nName: count, dtype: int64\n\n\n\n\nCódigo\nplt.pie(les_val,labels=les_val.index,startangle=30,shadow=True,autopct='%1.1f%%',rotatelabels=30,explode=(0.1,0.1,0.1,0.1),colors=[c4(0.9),c4(0.2),c4(0.3),c4(0.6)])\nplt.title('Porcentaje de lesiones por Categoria')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nObservamos que el \\(0.2\\%\\) de accidentes son Fatales. Por otro lado algo que sería de mucho interes saber de cuantos accidentes no se conoce la lesión ocasionada puesto que la probabilidad de no tener lesiones es muy baja."
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#motivo-mas-comun-por-el-cual-suceden-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Motivo mas comun por el cual suceden accidentes",
    "text": "Motivo mas comun por el cual suceden accidentes\nPrimero indagaremos cuantos Factores Primarios existen en la data\n\n\nCódigo\ndf['Primary Factor'].nunique()\n\n\n55\n\n\nDado que existen \\(55\\) Factores Primarios nos quedaremos con el top 20\n\n\nCódigo\npfdf=df['Primary Factor'].value_counts().head(20)\n\n\n\n\nCódigo\nfig=plt.figure(figsize=(10,10))\naxis=fig.add_axes([1,1,1,1])\nsb.swarmplot(x=pfdf,y=pfdf.index,ax=axis)\nfor i,j in enumerate(pfdf):\n  axis.text(j,i,j)\nplt.xlabel('Ocurrencia')\nplt.title('Motivos principales por los que ocurren accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#top-30-lugares-mas-frecuente-donde-ocurren-accidentes",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Top 30 lugares mas frecuente donde ocurren accidentes",
    "text": "Top 30 lugares mas frecuente donde ocurren accidentes\n\n\nCódigo\nldf=df['Reported_Location'].value_counts().head(30)\n\n\n\n\nCódigo\nfig1=plt.figure()\naxis1=fig1.add_axes([1,1,1,1])\nsb.barplot(x=ldf,y=ldf.index,ax=axis1,palette=\"viridis\")\nfor i,j in enumerate(ldf):\n  axis1.text(j,i,j,va='top')\naxis1.set_xlabel('Frecuencia')\naxis1.set_title('Lugares con mayor frecuencia de Accidentes')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-años",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#tipos-de-colisiones-en-diferentes-años",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Tipos de Colisiones en diferentes años",
    "text": "Tipos de Colisiones en diferentes años\n\n\nCódigo\naños=df.groupby('Year')\nkeys=años.groups.keys()\n\n\n\n\nCódigo\ninfobox=[]\nfor i in range(2003,2016):\n  infobox.append(años.get_group(i)['Collision Type'].value_counts())\n\n\n\n\nCódigo\nc2=cm.get_cmap('terrain')\n\n\n\n\nCódigo\nfrom IPython.display import display, Markdown\n\n\nColisiones por Año\n\n2003200420052006200720082009201020112012201320142015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2024-1-13-Accidentes automovilisticos/index.html#gráfico-dinamico",
    "href": "posts/2024-1-13-Accidentes automovilisticos/index.html#gráfico-dinamico",
    "title": "Accidentes automovilisticos Conjunto de datos EDA",
    "section": "Gráfico Dinamico",
    "text": "Gráfico Dinamico\n\n\nCódigo\nimport plotly.express as px\n\n\n\n\nCódigo\npx.histogram(df,x='Collision Type', \nanimation_frame=df['Year'].sort_values(ascending=True),\ncolor='Collision Type', title='Accidentes por Tipo de Colisión'\n)"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html",
    "href": "posts/2024-04-10-KNN/index.html",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos Más Cercanos (KNN) es uno de los métodos más simples y efectivos para clasificación y regresión en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los parámetros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#introducción",
    "href": "posts/2024-04-10-KNN/index.html#introducción",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "",
    "text": "El algoritmo de K Vecinos Más Cercanos (KNN) es uno de los métodos más simples y efectivos para clasificación y regresión en ciencia de datos. A pesar de su simplicidad, KNN puede ser muy poderoso, especialmente cuando se configuran adecuadamente los parámetros y se utiliza en los contextos apropiados."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#qué-es-k-vecinos-más-cercanos",
    "href": "posts/2024-04-10-KNN/index.html#qué-es-k-vecinos-más-cercanos",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "¿Qué es K Vecinos Más Cercanos?",
    "text": "¿Qué es K Vecinos Más Cercanos?\nKNN es un algoritmo de aprendizaje supervisado que clasifica una muestra en función de las categorías de sus \\(K\\) vecinos más cercanos. Para la clasificación, asigna la categoría más común entre sus vecinos, y para la regresión, predice el valor promedio de los vecinos más cercanos.\n\nFuncionamiento del KNN:\n\nSelección de K: Elige el número de vecinos más cercanos (\\(K\\)).\nDistancia: Calcula la distancia entre la muestra y todas las demás muestras en el conjunto de datos (comúnmente usando la distancia euclidiana).\nVecinos: Identifica los \\(K\\) vecinos más cercanos a la muestra.\nVotación: Asigna la clase más común (clasificación) o el promedio de los valores (regresión) entre los \\(K\\) vecinos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ejemplo",
    "href": "posts/2024-04-10-KNN/index.html#ejemplo",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\nPara ilustrar el uso de KNN, usaremos el dataset Iris.\n\nPaso 1: Instalación de Librerías\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd \npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nCódigo\nlibrary(class)\nlibrary(caret)\nlibrary(GGally)\nlibrary(ggplot2)\n\n\n\n\n\n\n\nPaso 2: Cargar y Preprocesar los Datos\n\nPythonR\n\n\n\n\nCódigo\ndata = pd.read_csv(\"Iris.csv\")\ndata = data.drop(['Id'], axis = 1)\nX = data.drop(['Species'], axis = 1)\ny = data['Species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n\n\n\n\n\nCódigo\ndata &lt;- read.csv(\"Iris.csv\")\ndata &lt;- data[,-1]\n\nany(is.na(data))\n\n\n[1] FALSE\n\n\nCódigo\nX &lt;- data[,-5]\ny &lt;- data$Species\n\nset.seed(42)\ntrainIndex &lt;- createDataPartition(y, p = .7, list = FALSE)\nX_train &lt;- X[trainIndex,]\nX_test &lt;- X[-trainIndex,]\ny_train &lt;- y[trainIndex]\ny_test &lt;- y[-trainIndex]\n\npreProcValues &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train &lt;- predict(preProcValues, X_train)\nX_test &lt;- predict(preProcValues, X_test)\n\n\n\n\n\n\n\nPaso 3: Entrenar el Modelo\n\nPythonR\n\n\n\n\nCódigo\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\n\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n\n\n\nCódigo\nknn_model &lt;- knn(train = X_train, test = X_test, cl = y_train, k = 3)\n\n\n\n\n\n\n\nPaso 4: Evaluar el Modelo\n\nPythonR\n\n\n\n\nCódigo\ny_pred = knn.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n\nAccuracy: 0.9111111111111111\n\n\nCódigo\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n\n\nConfusion Matrix:\n [[19  0  0]\n [ 0  9  4]\n [ 0  0 13]]\n\n\nCódigo\nprint('Classifcation Report:\\n', classification_report(y_test, y_pred))\n\n\nClassifcation Report:\n                  precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        19\nIris-versicolor       1.00      0.69      0.82        13\n Iris-virginica       0.76      1.00      0.87        13\n\n       accuracy                           0.91        45\n      macro avg       0.92      0.90      0.89        45\n   weighted avg       0.93      0.91      0.91        45\n\n\n\n\n\n\nCódigo\nconfusion &lt;- confusionMatrix(knn_model, as.factor(y_test))\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n                 Reference\nPrediction        Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              14               0              0\n  Iris-versicolor           1              14              2\n  Iris-virginica            0               1             13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9111          \n                 95% CI : (0.7878, 0.9752)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.467e-16       \n                                          \n                  Kappa : 0.8667          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Iris-setosa Class: Iris-versicolor\nSensitivity                      0.9333                 0.9333\nSpecificity                      1.0000                 0.9000\nPos Pred Value                   1.0000                 0.8235\nNeg Pred Value                   0.9677                 0.9643\nPrevalence                       0.3333                 0.3333\nDetection Rate                   0.3111                 0.3111\nDetection Prevalence             0.3111                 0.3778\nBalanced Accuracy                0.9667                 0.9167\n                     Class: Iris-virginica\nSensitivity                         0.8667\nSpecificity                         0.9667\nPos Pred Value                      0.9286\nNeg Pred Value                      0.9355\nPrevalence                          0.3333\nDetection Rate                      0.2889\nDetection Prevalence                0.3111\nBalanced Accuracy                   0.9167\n\n\n\n\n\n\n\nPaso 5: Visualización de Resultados\n\nPythonR\n\n\n\n\nCódigo\nsb.pairplot(data, hue = 'Species')\n\n\n\n\n\n\n\n\n\nCódigo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nggpairs(data, aes(color = Species, alpha = 0.5)) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "href": "posts/2024-04-10-KNN/index.html#ventajas-y-desventajas",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nSimplicidad: Fácil de entender e implementar.\nNo Paramétrico: No asume ninguna distribución de datos.\n\nDesventajas\n\nCosto Computacional: Requiere almacenar todos los datos de entrenamiento y calcular distancias para cada predicción.\nSensibilidad al Ruido: Afectado por la escala y por valores atípicos."
  },
  {
    "objectID": "posts/2024-04-10-KNN/index.html#conclusión",
    "href": "posts/2024-04-10-KNN/index.html#conclusión",
    "title": "K Vecinos más cercanos (KNN)",
    "section": "Conclusión",
    "text": "Conclusión\nEl algoritmo de K Vecinos Más Cercanos es una herramienta poderosa y fácil de usar para tareas de clasificación y regresión. A través de ejemplos prácticos en Python y R, hemos demostrado cómo implementar y evaluar este modelo en la práctica. Aunque tiene algunas limitaciones, KNN sigue siendo una opción valiosa para muchos problemas en ciencia de datos."
  },
  {
    "objectID": "posts/2023-12-20-PCA/index.html",
    "href": "posts/2023-12-20-PCA/index.html",
    "title": "Analisis de Componentes Principales",
    "section": "",
    "text": "En este post se pretende reducir dimensiones de una cantidad de datos es decir encontrar una transformación en la cuál se represente de mejor manera los datos reduciendo así su dimensión\nComo ya es costumbre primero importamos las librerías necesarias\n\n\nCódigo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n\nImportemos los datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nCódigo\nACP=PCA(n_components=2)\n\n\nAjustamos el modelo\n\n\nCódigo\nACP.fit(X_iris)\n\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2)\n\n\ntransformamos la data\n\n\nCódigo\nX_2d=ACP.transform(X_iris)\n\n\nRealizamos una representación gráfica\n\n\nCódigo\n#para facilitarnos el trabajo vamos a extender sobre el data iris los nuevos ejes encontrados\niris['PCA1']=X_2d[:,0]\niris['PCA2']=X_2d[:,1]\nsb.lmplot(x=\"PCA1\",y=\"PCA2\",hue='species',data=iris,fit_reg=False)"
  },
  {
    "objectID": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "href": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "title": "Tratando los datos Keane",
    "section": "",
    "text": "En este post abordaremos un poco los datos keane obtenidos de Gretl\nEmpezaremos por importar las librerías necesarias\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt \nimport seaborn as sb\n\n\n\nA continuación nos disponemos a visualizar los datos\n\n\nCódigo\nkeane=pd.read_csv(\"keane.csv\")\nkeane=pd.DataFrame(keane)\n\n\n\n\nCódigo\nkeane.head(5)\n\n\n\n\n\n\n\n\n\nid\nnumyrs\nyear\nchoice\nwage\neduc\nexpwc\nexpbc\nexpser\nmanuf\nblack\nlwage\nenroll\nemploy\nattrit\nexper\nexpersq\nstatus\n\n\n\n\n0\n1\n9\n81\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n1\n1\n9\n82\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n2\n1\n9\n83\n2.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n3\n1\n9\n84\n1.0\nNaN\n10\n0\n0\n0\n0.0\n1\nNaN\n1\n0\n0\n0\n0\n1.0\n\n\n4\n1\n9\n85\n2.0\nNaN\n11\n0\n0\n0\n0.0\n1\nNaN\n0\n0\n0\n0\n0\n2.0\n\n\n\n\n\n\n\n\nCrearemos etiquetas para las observaciones de acuerdo a “choice” estudiante=1, hogar=2, cualificado=3, no-cualificado=4, servicio=5\n\n\nCódigo\nkeane[\"choice\"]=np.where(keane[\"choice\"]==1,\"estudiante\",\n         np.where(keane[\"choice\"]==2,\"hogar\",\n                  np.where(keane[\"choice\"]==3,\"cualificado\",\n                           np.where(keane[\"choice\"]==4,\"no-cualificado\",\"servicio\"))))\n\n\n\nProcedemos a gráficar la evolución de salarios separado por color de piel\n\n\nCódigo\nsb.scatterplot(data=keane,x=\"year\",y=\"wage\",hue=\"black\",style=\"black\",style_order=[1,0])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nEn esta gráfica evidenciamos que a lo largo de los años aumenta la discriminación.\nVisualicemos lo siguiente: seleccionando sólo las personas que trabajan se realizara un gráfico de la evolución de los salarios separados por la variable choice\n\n\nCódigo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\", y=\"wage\",hue=\"choice\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nSe observa que la terciarización de la economía ha aumentado las diferencias entre trabajadores cualificados y no cualificados, así como, entre servicio e industria.\nAhora procedamos al análisis de la variable educ para ello primero la Codificaremos de acuerdo a educación básica=1, educación media=2, y educación superior=3.\n\n\nCódigo\nkeane[\"educCode\"]=np.where(keane[\"educ\"]&lt;=9,1,np.where(keane[\"educ\"]&lt;=12,2,3))\n\n\nSeleccionando solo las personas que trabajan tenemos lo siguiente:\n\n\nCódigo\nsb.scatterplot(data=keane[keane[\"employ\"]==1],x=\"year\",y=\"wage\",hue=\"educCode\")\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "contacto.html#email-joelburbanooutlook.com",
    "href": "contacto.html#email-joelburbanooutlook.com",
    "title": "Contactos",
    "section": "Email : joelburbano@outlook.com",
    "text": "Email : joelburbano@outlook.com"
  },
  {
    "objectID": "certificados/25-05-2023_Curso_power-BI/index.html",
    "href": "certificados/25-05-2023_Curso_power-BI/index.html",
    "title": "Curso Power BI",
    "section": "",
    "text": "Curso Power BI"
  },
  {
    "objectID": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "href": "certificados/16-05-2023-Curso_de_PHP-MySQL/index.html",
    "title": "Curso PHP y MySQL",
    "section": "",
    "text": "Curso PHP y MySQL"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          Título\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nMedidas con Fórmulas Temporales DAX y Power BI\n\n\n\nPower BI\n\n\nDAX\n\n\n\n\n\n\n\nJoel Burbano\n\n\n25 may 2024\n\n\n\n\n\n\n\n\n\n\n\n\nK Vecinos más cercanos (KNN)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje Supervisado\n\n\nKNN\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Componentes Principales (PCA)\n\n\n\nPython\n\n\nR\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n28 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión Logística\n\n\n\nPython\n\n\nR\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n11 mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAccidentes automovilisticos Conjunto de datos EDA\n\n\n\nPython\n\n\n\nEn este apartado vamos a analizar datos de accidentes automovilisticos\n\n\n\nJoel burbano\n\n\n13 ene 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Mixture Models\n\n\n\nPython\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalisis de Componentes Principales\n\n\n\nPython\n\n\nPCA\n\n\nReducción de dimensiones\n\n\nAprendizaje No Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión Lineal Simple\n\n\n\nPython\n\n\nAprendizaje Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Clasificación\n\n\n\nAprendizaje supervisado\n\n\nPython\n\n\nNaive Bayes\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDetección de fraude con tarjetas de Crédito\n\n\n\nPython\n\n\n\nEn el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias.\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHiperparametros y Modelos de Validación\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTratando los datos Keane\n\n\n\nEconometría\n\n\nPython\n\n\n\nEn este post se realizara un analisis de los datos keane\n\n\n\nJoel Burbano\n\n\n7 dic 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "👋 ¡Hola! Soy Joel, graduado de Ingeniero Matemático en la Facultad de Ciencias de la Escuela Politécnica Nacional.\n🎓 Durante mi etapa universitaria he adquirido las bases para gestionar modelos de riesgo, modelos econométricos, modelos estadísticos, modelos de programación entera. Así también, he adquirido las bases de matemática actuarial, estadística matemática, investigación operativa. Además, me he capacitado en el manejo de lenguajes de programación tales como: C++, Matlab, R, Python, y también manejo de paquetes estadísticos Statgraphics y Gretl.\n📝 El desarrollo de mi trabajo de titulación se enfoca en desarrollar un modelo de consumo a partir de la Hipótesis de Renta Permanente de Friedman (Novel Economia,1976) y El Ciclo de Vida de Modigliani.\n🎯Mi objetivo es seguir desarrollando mis habilidades en el área estadística, econométrica, actuarial y ciencia de datos, por lo que me encuentro altamente interesado en trabajar en las mencionadas áreas."
  },
  {
    "objectID": "certific.html",
    "href": "certific.html",
    "title": "Certificados",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          Título\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR para data scientist avanzado\n\n\n\nSQL\n\n\nConsultas\n\n\nCombinanciones y Subconsultas\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR para data scientist avanzado\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\nSeries Temporales\n\n\nArboles de decisión\n\n\n\n\n\n\n\n\n\n\n27 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython para data science y big data esencial\n\n\n\nPython\n\n\nData Science\n\n\nBig Data\n\n\npandas\n\n\nPySpark\n\n\nsklearn\n\n\n\n\n\n\n\n\n\n\n24 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos profesionales del análisis de datos, por Microsfot y LinkedIn\n\n\n\nAnalítica de datos\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n16 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprende data science Conceptos básicos\n\n\n\nCiencia de datos\n\n\n\n\n\n\n\n\n\n\n12 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducción a las habilidades profesionales en análisis de datos\n\n\n\nAnálisis de datos\n\n\nAnalítica de datos\n\n\nAptitudes para carreras tecnologicas\n\n\nExcel\n\n\nPower BI\n\n\n\n\n\n\n\n\n\n\n5 oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Completo de Data Science en Python Desde Cero [2023]\n\n\n\nPython\n\n\nData Science\n\n\npandas\n\n\nsklearn\n\n\nstatsmodels\n\n\nBeautifulSoup\n\n\n\n\n\n\n\n\n\n\n22 jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Power BI\n\n\n\nPower BI\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n25 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Tableau Desktop\n\n\n\nTableau\n\n\nBussiness Intelligence\n\n\n\n\n\n\n\n\n\n\n19 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso PHP y MySQL\n\n\n\nMySQL\n\n\nPHP\n\n\n\n\n\n\n\n\n\n\n16 may 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso Pronóstico Ventas\n\n\n\nPronósticos Ventas\n\n\nExcel\n\n\n\n\n\n\n\n\n\n\n16 abr 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurso teórico práctico El ambiente de Programación R en el ámbito de la investigación científica\n\n\n\nR\n\n\nData Science\n\n\ntidyverse\n\n\ngit\n\n\nRmarkdown\n\n\n\n\n\n\n\n\n\n\n3 sept 2020\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "href": "certificados/16-04-2023-Pronostico-Ventas/index.html",
    "title": "Curso Pronóstico Ventas",
    "section": "",
    "text": "Curso Pronoóstico Ventas"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joel Burbano",
    "section": "",
    "text": "Ingeniero Matemático | Analista de Datos | Científico de Datos"
  },
  {
    "objectID": "index.html#educación",
    "href": "index.html#educación",
    "title": "Joel Burbano",
    "section": "Educación",
    "text": "Educación\nEscuela Politécnica Nacional, Quito | Quito, Ec\nIngeniero Matemático | Meción Estadistica e Investigación Operativa\nTesis: Estudio Teórico del Comportamiento del Consumo en Latinoamérica y Caribe. En el trabajo de tesis se propuso un modelo teórico del consumo, basado en la hipótesis de renta permanente de Friedman y el modelo de Ciclo de Vida de Modigliani. Además, se realizo una comprobación con datos de panel para países de América Latina y Caribe."
  },
  {
    "objectID": "index.html#competencias",
    "href": "index.html#competencias",
    "title": "Joel Burbano",
    "section": "Competencias",
    "text": "Competencias\nR Python SQL MongoDB Power BI Excel(VBA)"
  },
  {
    "objectID": "posts/2023-12-20-Clustering/index.html",
    "href": "posts/2023-12-20-Clustering/index.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "En este post veremos un modelos de cluster\nComo ya es costumbre primero importamos las librerias necesarias\n\n\nCódigo\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\n\nImportemos los datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nDefinimos el modelo\n\n\nCódigo\nGauss=GaussianMixture(n_components=3,\ncovariance_type='full')\n\n\nAjustamos el modelo\n\n\nCódigo\nGauss.fit(X_iris)\n\n\nGaussianMixture(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=3)\n\n\nDeterminamos las etiquetas\n\n\nCódigo\ny_gmm=Gauss.predict(X_iris)\n\n\ngraficamente\n\n\nCódigo\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\nCódigo\niris['cluster']=y_gmm\nsb.lmplot(data=iris,x='sepal_length',y='petal_length',hue='species',col='cluster',fit_reg=False)"
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html",
    "href": "posts/2024-03-11-regresion-logistica/index.html",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística es una técnica de modelado estadístico utilizada para predecir el resultado de una variable categórica basada en una o más variables independientes. Es especialmente útil cuando queremos clasificar observaciones en dos o más clases. A diferencia de la regresión lineal, que predice valores continuos, la regresión logística predice probabilidades de pertenencia a una categoría.\n\n\nLa regresión logística utiliza la función logística, también conocida como función sigmoide, para transformar la salida de una combinación lineal de las variables independientes. La función sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa fórmula de la función sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinación lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)\n\n\n\n\n\n\n\n\n\n\n\nImaginemos que queremos predecir si un correo electrónico es spam o no, usando características como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nCódigo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nCódigo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nCódigo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nCódigo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nCódigo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nCódigo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nCódigo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nCódigo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nCódigo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nCódigo\n# Matriz de consfusión\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nCódigo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nCódigo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo.\n\n\n\n\n\n\n\nVentajas\n\nInterpretabilidad: Es fácil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros métodos complejos\n\nDesventajas\n\nLinealidad: Asume una relación lineal entre las variables independientes y la probabilidad de la clase.\nLimitación: La versión básica está diseñada para clasificación binaria.\n\n\n\n\nLa regresión logística es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificación. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagnóstico médico hasta la detección de fraude."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#comó-funciona",
    "href": "posts/2024-03-11-regresion-logistica/index.html#comó-funciona",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística utiliza la función logística, también conocida como función sigmoide, para transformar la salida de una combinación lineal de las variables independientes. La función sigmoide produce valores entre 0 y 1, que pueden interpretarse como probabilidades.\n\n\n\n\n\n\nImportante\n\n\n\nLa fórmula de la función sigmoide es:\n\\[\\sigma(z)=\\frac{1}{1+ e^{-z}}\\]\ndonde \\(z\\) es la combinación lineal de las variables independientes, es decir \\(z=\\beta_0 + \\sum_{i=1}^n \\beta_i z_i\\)"
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-práctico",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ejemplo-práctico",
    "title": "Regresión Logística",
    "section": "",
    "text": "Imaginemos que queremos predecir si un correo electrónico es spam o no, usando características como la frecuencia de ciertas palabras y la longitud del correo.\n\n\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\npd.options.display.max_columns=None\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n\n\n\n\nCódigo\nlibrary(dplyr)\nlibrary(caTools)\nlibrary(caret)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ncolums = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\"\n]\n\ndata = pd.read_csv(\"spambase.data\", header=None, names=colums)\n\n\nX = data.drop(columns=['spam'])\ny = data['spam']\n\n\n\n\n\n\nCódigo\ncolumns &lt;- c(\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n\"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n\"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n\"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\",\n\"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\",\n\"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n\"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n\"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\",\n\"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n\"capital_run_length_average\", \"capital_run_length_longest\",\"capital_run_length_total\",\"spam\")\n\ndata &lt;- read.csv(\"spambase.data\", header = FALSE, col.names = columns)\n\n# Convertimos la variable dependiente en factor\n\ndata$spam &lt;- as.factor(data$spam)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\n\nCódigo\nset.seed(42)\n\nsplit &lt;- sample.split(data$spam, SplitRatio = 0.7)\n\ntrain_data &lt;- subset(data, split == TRUE)\ntest_data &lt;- subset(data, split == FALSE)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n\n\nCódigo\nmodel &lt;- glm(spam ~ ., data = train_data, family = binomial)\n\n\n\n\n\n\n\n\n\nPythonR\n\n\n\n\nCódigo\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\nAccuracy: 0.9268645908761767\n\n\nCódigo\nprint('Confusion Matrix:')\n\n\nConfusion Matrix:\n\n\nCódigo\nprint(confusion_matrix(y_test, y_pred))\n\n\n[[757  47]\n [ 54 523]]\n\n\nCódigo\nprint('Clasification Report:')\n\n\nClasification Report:\n\n\nCódigo\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94       804\n           1       0.92      0.91      0.91       577\n\n    accuracy                           0.93      1381\n   macro avg       0.93      0.92      0.92      1381\nweighted avg       0.93      0.93      0.93      1381\n\n\n\n\n\n\nCódigo\npredictions &lt;- predict(model, test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\n\n# Calcular el accuracy del modelo\naccuracy &lt;- mean(predicted_classes == test_data$spam)\nprint(paste(\"Accuracy:\", round(accuracy * 100, 2),\"%\"))\n\n\n[1] \"Accuracy: 92.46 %\"\n\n\nCódigo\n# Matriz de consfusión\nconfusion &lt;- table(Predicted = predicted_classes, Actual = test_data$spam)\nprint(\"Confusion Matrix:\")\n\n\n[1] \"Confusion Matrix:\"\n\n\nCódigo\nprint(confusion)\n\n\n         Actual\nPredicted   0   1\n        0 793  61\n        1  43 483\n\n\nCódigo\n# Reporte de clasificacion\nconfusionMatrix(as.factor(predicted_classes), test_data$spam)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 793  61\n         1  43 483\n                                         \n               Accuracy : 0.9246         \n                 95% CI : (0.9094, 0.938)\n    No Information Rate : 0.6058         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.8413         \n                                         \n Mcnemar's Test P-Value : 0.09552        \n                                         \n            Sensitivity : 0.9486         \n            Specificity : 0.8879         \n         Pos Pred Value : 0.9286         \n         Neg Pred Value : 0.9183         \n             Prevalence : 0.6058         \n         Detection Rate : 0.5746         \n   Detection Prevalence : 0.6188         \n      Balanced Accuracy : 0.9182         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\n\n\n\n\nEn general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluación",
    "href": "posts/2024-03-11-regresion-logistica/index.html#resultados-y-evaluación",
    "title": "Regresión Logística",
    "section": "",
    "text": "En general se ha conseguido un accuracy de alrededor del \\(92\\%\\) lo cual indica que el modelo predice un resultado correcto 92 de cada 100 veces, lo que en general es un buen modelo pero se lo podria mejorar seleccionando mejor las variables en este caso se ha utilizado todas sin un analisis de su poder predicitivo."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "href": "posts/2024-03-11-regresion-logistica/index.html#ventajas-y-desventajas",
    "title": "Regresión Logística",
    "section": "",
    "text": "Ventajas\n\nInterpretabilidad: Es fácil de entender y explicar\nEficiencia: Es computacionalmente menos costosa que otros métodos complejos\n\nDesventajas\n\nLinealidad: Asume una relación lineal entre las variables independientes y la probabilidad de la clase.\nLimitación: La versión básica está diseñada para clasificación binaria."
  },
  {
    "objectID": "posts/2024-03-11-regresion-logistica/index.html#conclusión",
    "href": "posts/2024-03-11-regresion-logistica/index.html#conclusión",
    "title": "Regresión Logística",
    "section": "",
    "text": "La regresión logística es una herramienta poderosa y ampliamente utilizada en la ciencia de datos para problemas de clasificación. Su simplicidad y eficacia la hacen ideal para muchas aplicaciones, desde el diagnóstico médico hasta la detección de fraude."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "",
    "text": "Las medidas con fórmulas temporales en DAX (Data Analysis Expressions) y Power BI son herramientas poderosas que permiten a los analistas de datos realizar cálculos avanzados en base a períodos de tiempo. Estas medidas son esenciales para el análisis de series temporales, permitiendo comparar datos entre diferentes períodos y obtener insights sobre tendencias y patrones. En este artículo, exploraremos qué son estas medidas, para qué sirven, y veremos un ejemplo práctico utilizando Power BI."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-1-cargar-los-datos-en-power-bi",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 1: Cargar los Datos en Power BI",
    "text": "Paso 1: Cargar los Datos en Power BI\n\nAbre Power BI Desktop.\nHaz clic en “Obtener datos” y selecciona “Archivo CSV”\nCarga el dataset “Superstore Sales”"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-2-crear-un-modelo-de-datos",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 2: Crear un Modelo de Datos",
    "text": "Paso 2: Crear un Modelo de Datos\n\nAbre Transformar datos y verificamos que las columnas tengan el formato adecuado\nCrea una tabla de fechas “Calendario”\n\n\n\ndax\n\ncalendario = CALENDAR(MIN(Superstore[Order Date]), MAX(Superstore[Order Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-3-definir-relaciones",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 3: Definir Relaciones",
    "text": "Paso 3: Definir Relaciones\n\nEn la vista “Modelo”, crea una relación entre la columna “Order Date” de la tabla de Superstore y la columna “Date” de la tabla calendario."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#paso-4-crear-medidas-temporales",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Paso 4: Crear Medidas Temporales",
    "text": "Paso 4: Crear Medidas Temporales\n\nVentas Totales (Total Sales):\n\n\n\ndax\n\ntotal_sales = SUM(Superstore[Sales])\n\n\nVentas Acumuladas Año a la Fecha (YTD Sales):\n\n\n\ndax\n\nYTD_Sales = TOTALYTD([total_sales],calendario[Date])\n\n\nVentas Acumuladas Mes a la Fecha (MTD Sales):\n\n\n\ndax\n\nMTD_Sales = TOTALMTD([total_sales], calendario[Date])\n\n\nVentas del Mismo Período el Año Pasado (Same Period Last Year Sales):\n\n\n\ndax\n\nSales_last_year = CALCULATE( [total_sales], SAMEPERIODLASTYEAR(calendario[Date]))"
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#ventajas",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Ventajas",
    "text": "Ventajas\n\nFacilidad de Uso: Power BI y DAX facilitan la creación y manipulación de medidas temporales.\nFlexibilidad: Permite realizar cálculos complejos y personalizados según las necesidades del análisis.\nVisualización Interactiva: Power BI ofrece herramientas visuales que permiten explorar datos de manera interactiva."
  },
  {
    "objectID": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "href": "posts/2024-05-25-Medidas_Power_BI/index.html#desventajas",
    "title": "Medidas con Fórmulas Temporales DAX y Power BI",
    "section": "Desventajas",
    "text": "Desventajas\n\nCurva de Aprendizaje: Puede ser desafiante para principiantes sin experiencia previa en DAX.\nRendimiento: Las medidas complejas pueden afectar el rendimiento del informe, especialmente con grandes volúmenes de datos."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html",
    "href": "posts/2024-3-28-Componentes-principales/index.html",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica fundamental en ciencia de datos utilizada para la reducción de dimensionalidad. Este método transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales. Es especialmente útil cuando se trabaja con grandes volúmenes de datos y se busca simplificar el análisis sin perder información significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#introducción",
    "href": "posts/2024-3-28-Componentes-principales/index.html#introducción",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "",
    "text": "El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica fundamental en ciencia de datos utilizada para la reducción de dimensionalidad. Este método transforma un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales. Es especialmente útil cuando se trabaja con grandes volúmenes de datos y se busca simplificar el análisis sin perder información significativa."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#qué-es-el-análisis-de-componentes-principales",
    "href": "posts/2024-3-28-Componentes-principales/index.html#qué-es-el-análisis-de-componentes-principales",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "¿Qué es el Análisis de Componentes Principales?",
    "text": "¿Qué es el Análisis de Componentes Principales?\nPCA es un método estadístico que transforma los datos originales en nuevas variables no correlacionadas ordenadas según la cantidad de varianza explicada. Los primeros componentes principales capturan la mayor parte de la variabilidad en los datos, lo que permite una reducción significativa de la dimensionalidad mientras se preserva la mayor cantidad de información posible.\n\n\n\n\n\n\nImportante\n\n\n\nConceptos Clave:\n\nVarianza: Medida de la dispersión de los datos\nCovarianza: Indica la dirección de la relación entre dos variables.\nComponentes Principales: Nuevas variables no correlacionadas formadas por combinaciones lineales de las variables originales\nValores y Vectores propios: Los valores propios indican la cantidad de varianza capturada por cada componente principal, y los vectores propios definen la dirección de los componentes."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ejemplo-practico",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Ejemplo Practico",
    "text": "Ejemplo Practico\n\n\n\n\n\n\nPara ilustrar un ejemplo se utilizara el dataset de Wine Quality disponible en Kaggle.\n\nPaso 1: Instalación de librerias\n\nPythonR\n\n\n\n\nCódigo\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\n\n\n\n\nCódigo\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\n\n\nPaso 2: Cara y Preprocesamiento de Datos\n\nPythonR\n\n\n\n\nCódigo\ndata = pd.read_csv(\"WineQT.csv\")\nfeatures = data.drop('quality', axis=1)\n\nfeatures = StandardScaler().fit_transform(features)\n\n\n\n\n\n\nCódigo\ndata &lt;- read.csv(\"WineQT.csv\")\nfeatures &lt;- data %&gt;% select(-quality)\n\nfeatures &lt;- scale(features)\n\n\n\n\n\n\n\nPaso 3: Aplicación de PCA\n\nPythonR\n\n\n\n\nCódigo\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(features)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])\nfinalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n\n\n\n\n\n\nCódigo\npca &lt;- prcomp(features, center = TRUE, scale. = TRUE)\nprincipalComponents &lt;- data.frame(pca$x[,1:2])\nfinalDf &lt;- cbind(principalComponents, quality = data$quality)\n\n\n\n\n\n\n\nPaso 4: Visualización de Resultados\n\npythonR\n\n\n\n\nCódigo\nplt.figure(figsize = (8,6))\nsb.scatterplot(x = 'PC1', y = 'PC2', hue = 'quality', data = finalDf, palette = 'viridis')\nplt.title('PCA de Vinos')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nCódigo\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(finalDf, aes(x = PC1, y = PC2, color = as.factor(quality))) +\n  geom_point(alpha=0.5) +\n  labs(title = \"PCA de Vinos\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "href": "posts/2024-3-28-Componentes-principales/index.html#ventajas-y-desventajas",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\nVentajas\n\nReducción de Dimensionalidad: Facilita la visualización y análisis de datos de alta dimensionalidad.\nEliminación de Redundancia: Reduce la redundancia al eliminar las correlaciones entre variables.\n\nDesventajas\n\nInterpretabilidad: Los componentes principales no siempre tienen un significado intuitivo.\nPérdida de Información: Aunque PCA preserva la mayor varianza posible, siempre hay alguna pérdida de información."
  },
  {
    "objectID": "posts/2024-3-28-Componentes-principales/index.html#conclusión",
    "href": "posts/2024-3-28-Componentes-principales/index.html#conclusión",
    "title": "Análisis de Componentes Principales (PCA)",
    "section": "Conclusión",
    "text": "Conclusión\nEl PCA es una herramienta poderosa en el arsenal de un científico de datos. Su capacidad para simplificar conjuntos de datos complejos y reducir la dimensionalidad lo hace indispensable para la exploración y visualización de datos. A través de ejemplos prácticos en Python y R, podemos ver cómo esta técnica se aplica en la práctica, facilitando el análisis y la toma de decisiones basadas en datos."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html",
    "href": "posts/Hyperparameters and Model Validation/index.html",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta razón es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nCódigo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAquí, cada fila de la data se refiere a la observación de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#representación-de-la-data-en-scikit-learn",
    "href": "posts/Hyperparameters and Model Validation/index.html#representación-de-la-data-en-scikit-learn",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta razón es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\n\nCódigo\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAquí, cada fila de la data se refiere a la observación de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "href": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Matriz de caracteristicas (Features matrix)",
    "text": "Matriz de caracteristicas (Features matrix)\nEsta tabla contienene la información caracteeristica del conjunto de datos en nuestro ejemplo contiene información de flores. Matemáticamente estas caracteristicas pasan a representar las variables independientes de nuestro conjunto de datos generalmente denotado por \\(X\\)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "href": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Objetivo (Target Array)",
    "text": "Objetivo (Target Array)\nEs un arraglo que matemáticamente representa la variable dependiente generalmente notada por \\(y\\)\n\n\nCódigo\nimport matplotlib.pyplot as plt\n\n\n\n\nCódigo\n#plt.figure(figsize=(12,8))\nsb.set()\nsb.pairplot(iris,hue='species',size=1.5)\n\n\nC:\\Users\\JXBS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\nPara usar los datos en Scikit-Learn, tenemos que extraer los matriz \\(X\\) y el objetivo \\(y\\)\n\n\nCódigo\nX_iris=iris.drop('species',axis=1)\nprint(X_iris.shape)\ny_iris=iris['species']\nprint(y_iris.shape)\n\n\n(150, 4)\n(150,)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "href": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Scikit-Learn’s API estimador",
    "text": "Scikit-Learn’s API estimador\nLa API de Scikit-Learn esta diseñada con los siguientes principios en mente\n\nCoherencia.- Todos los objetos comparten unaa interfaz común extraida de un conunto limitado de métodos, con documentación consistente.\nInspección Todos los valores de parámetros especificados se exponen como atributos públicos. Jerarquía de objetos limitada Sólo los algoritmos están representados por clases de Python; los conjuntos de datos se representan en formatos estándar (matrices NumPy, Pandas DataFrames, matrices dispersas de SciPy) y los nombres de los parámetros utilizan cadenas estándar de Python.\nComposición Muchas tareas de aprendizaje automático se pueden expresar como secuencias de algoritmos más fundamentales, y ScikitLearn hace uso de esto siempre que es posible.\nValores predeterminados sensatos Cuando los modelos requieren parámetros especificados por el usuario, la biblioteca define un valor predeterminado apropiado."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#básicos-de-la-api",
    "href": "posts/Hyperparameters and Model Validation/index.html#básicos-de-la-api",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Básicos de la API",
    "text": "Básicos de la API\nPor lo general, los pasos para usar la API del estimador Scikit-Learn son los siguientes (repasaremos algunos ejemplos detallados en las secciones siguientes):\n\nElija una clase de modelo importando la clase de estimador adecuada de ScikitLearn.\nElija los hiperparámetros del modelo creando una instancia de esta clase con los valores deseados.\nOrganice los datos en una matriz de características y un vector objetivo siguiendo la discusión anterior.\nAjuste el modelo a sus datos llamando al método fit() de la instancia del modelo.\nAplique el modelo a nuevos datos:\n\n• Para el aprendizaje supervisado, a menudo predecimos etiquetas para datos desconocidos usando el método predict().\n• Para el aprendizaje no supervisado, a menudo transformamos o inferimos propiedades de los datos utilizando el método transform() o predict()."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresión-lineal-simple",
    "href": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresión-lineal-simple",
    "title": "Hiperparametros y Modelos de Validación",
    "section": "Ejemplo De Aprendizaje Supervisado: Regresión Lineal Simple",
    "text": "Ejemplo De Aprendizaje Supervisado: Regresión Lineal Simple\n\n\nCódigo\nimport numpy as np\n\n\n\n\nCódigo\nrng=np.random.RandomState(42)\nx=10+rng.rand(50)\ny=2*x-1+rng.rand(50)\nplt.scatter(x,y)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1e7115f3c20&gt;\n\n\n\n\n\n\n\n\n\n\nEscojemos el modelo\n\n\n\nCódigo\nfrom sklearn.linear_model import LinearRegression\n\n\n\nEscogemos los hiperparametros\n\n\n\nCódigo\nmodel=LinearRegression(fit_intercept=True)\nmodel\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nOrganizando los datos en variables independientes y variable dependiente\n\n\n\nCódigo\nX=x[:,np.newaxis]\nX.shape\n\n\n(50, 1)\n\n\n\nAjustando el modelo\n\n\n\nCódigo\nmodel.fit(X,y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCódigo\nmodel.coef_\n\n\narray([2.06607665])\n\n\n\n\nCódigo\nmodel.intercept_\n\n\n-1.1957940680607742\n\n\n\nPredicciendo data desconocida\n\n\n\nCódigo\nxfit=np.linspace(-1,11)\n\n\n\n\nCódigo\nXfit=xfit[:,np.newaxis]\nyfit=model.predict(Xfit)\n\n\n\n\nCódigo\nplt.scatter(x,y)\nplt.plot(xfit,yfit)"
  },
  {
    "objectID": "posts/naive_bayes/index.html",
    "href": "posts/naive_bayes/index.html",
    "title": "Naive Bayes Clasificación",
    "section": "",
    "text": "En este post vamos a utilizar los datos iris para entrenar un modelo de clasificación y ver que tan bien se puede predecir las etiquetas\nEn este caso para evitarnos particionar el conjunto a “mano” utilizaremos la función train_test_split\nPrieramente importamos las librerias necesarias\n\n\nCódigo\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\n\nluego importamos el conjunto de datos\n\n\nCódigo\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\n\nAhora creamos los datos de entrenamiento y validación\n\n\nCódigo\nXtrain,Xtest,ytrain,ytest =train_test_split(X_iris,y_iris,random_state=1)\n\n\nAhora importamos el modelo que utilizaremos\n\n\nCódigo\nfrom sklearn.naive_bayes import GaussianNB\n\n\nDefinimos un nombre\n\n\nCódigo\nNB=GaussianNB()\n\n\najustamos el modelo\n\n\nCódigo\nNB.fit(Xtrain,ytrain)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\npredecimos la data que no se utilizo para establecer los parámetros del modelo\n\n\nCódigo\ny_NB=NB.predict(Xtest)\n\n\nAhora utilizando la métrica de exactitud (accurancy score) evaluaremos que tan bien predice el modelo que se ha creado\n\n\nCódigo\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nCódigo\naccuracy_score(ytest,y_NB)\n\n\n0.9736842105263158\n\n\nVemos que con una exactitud del \\(97,4\\%\\) el modelo implementado puede etiquetar los datos requeridos."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html",
    "href": "proyects/Tesis_JXBS/index.html",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "El modelo de consumo y renta surge de manera casi natural cuando un individuo se plantea un escenario del tipo ¿Qué pasaría si se destinara una parte de su renta a un fondo de ahorro?. Es así que se empieza con un planteamiento muy básico en el que el individuo esperaría que su ahorro lo ayude a afrontar crisis futuras. Sin embargo, esto es un poco ingenuo puesto que el individuo no tiene certeza de cuanto tiempo va a vivir, así como tampoco de si su ahorro lo ayudara en un momento de gran crisis. Es así que de manera natural se procede a generalizar la idea a un grupo de individuos, lo cual desencadena en lo siguiente ¿Todos los individuos se comportan de una manera similar?. A partir de esto se ramifica dos situaciones, la primera, considerar que todos los individuos tienen un comportamiento similar y por tanto realizar modelos con datos de medias; la segunda, es considerar que los individuos tiene comportamientos diferentes y por tanto buscar cual es el modelo que mejor describe esta realidad.\n\n\nAl inicio de los años \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la “ley fundamental de la psicología” mencionada por (Keynes 1936) en la Teoría general. A ese momento, las limitaciones empíricas y teóricas del mencionado modelo se hicieron cada vez más notorias. Desde un punto de vista teórico, es difícil construir modelos coherentes basados en la optimización intertemporal del comportamiento que sean consistentes con la descripción de (Keynes 1936) en la “Ley fundamental de la psicología”. Desde el punto de vista empírico, parecía que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, según (Orazio P. Attanasio y Weber 2010) se observó que la propensión marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistemáticamente con el nivel de rentas. Además, se observó que grupos de individuos con, niveles más bajos de rentas en media, tenían tasas de ahorro más altas que otros grupos con niveles más altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observó que las tasas de ahorro están sistemáticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradecían claramente las implicaciones del modelo keynesiano y condujeron a la formulación de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia teórica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimización coherente con la capacidad de ajustar la mayoría de los hechos mencionados en el párrafo anterior.\nA nivel macro, es más probable que las fluctuaciones a corto plazo de la renta disponible estén dominadas por la varianza de las perturbaciones temporales que se promediarían a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) también llevó a la realización de otras implicaciones. En una versión simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilación, los consumidores ahorraran cuando sean jóvenes para respaldar el consumo en la última parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlación entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un año dado, las cohortes más jóvenes, que están ahorrando, son ``más ricas’’ en términos de vida que las de mayor edad, que están desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor será la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor será la tasa agregada de ahorro.\nDespués de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo estándar de la macroeconomía moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la década de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explotó la idea de usar las condiciones de primer orden del problema de optimización intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuación de Euler1, hace posible el análisis empírico de un problema que es analíticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centrándose en la esencia económica del modelo: los consumidores, en el momento óptimo, actuarán para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estadística suficiente para las elecciones de los consumidores y, dadas sus propiedades dinámicas, puede “diferenciarse” de manera análoga al tratamiento de los efectos fijos en la econometría (Orazio P. Attanasio y Weber 2010).\n\n\n\n\n\nComo se mencionó en la introducción, el modelo de ciclo de vida/renta permanente se desarrolló para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos volátil que la renta y la propensión marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos “macro acontecimientos” siguen siendo válidos y algunos también se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es más alta que la de los blancos en cualquier nivel de rentas, como señaló (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta “permanente”, como los hogares encabezados por personas con diferentes niveles de educación.\nAnálogamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos últimos es menor que la de los primeros, como señalaron hace varios años (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo económico (J. Y. Campbell y Mankiw 1989) encontraron que la regresión de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estadísticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran número de consumidores que siguen una “regla general” y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlación significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo está mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y además se identifica las innovaciones permanentes a está ultima variable, el modelo predice que estas innovaciones deberían traducirse uno a uno en consumo. Esto implica restricciones paramétricas de ecuaciones cruzadas sobre la representación \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), señalaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepción es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene raíz unitaria (i.e es \\(I(1)\\). 2) la sección cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribución transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad empírica: la distribución de la sección cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versión estándar del modelo de ciclo de vida, a cualquier edad el “\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)” 4 y por lo tanto, por sustitución recursiva, se obtiene que el \\(log(consumo)\\) está dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado específico, muestran que la evolución relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la sección cruzada de la renta y la del consumo identificará los cambios en la varianza de la sección cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribución inicial del consumo (sin importar cómo se determine) en presencia de un riesgo compartido perfecto esa distribución debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evolución de sección cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar explícitamente los movimientos en la clasificación relativa en la distribución del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evolución del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlación en bajas frecuencias entre estas dos variables como evidencia en contra de la hipótesis del mercado completo. No pueden rechazar la hipótesis de que a frecuencias relativamente altos (como un año) no existe una relación entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisión-bibliográfica",
    "href": "proyects/Tesis_JXBS/index.html#revisión-bibliográfica",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "Al inicio de los años \\(50\\), el modelo que predomino el comportamiento de consumo y que fue utilizado por los macroeconomistas se inspiro en la “ley fundamental de la psicología” mencionada por (Keynes 1936) en la Teoría general. A ese momento, las limitaciones empíricas y teóricas del mencionado modelo se hicieron cada vez más notorias. Desde un punto de vista teórico, es difícil construir modelos coherentes basados en la optimización intertemporal del comportamiento que sean consistentes con la descripción de (Keynes 1936) en la “Ley fundamental de la psicología”. Desde el punto de vista empírico, parecía que el punto de vista de Keynes era incompatible con una serie de hechos, tanto a macro y micro nivel. A nivel agregado, por ejemplo, según (Orazio P. Attanasio y Weber 2010) se observó que la propensión marginal a consumir de la renta disponible fue menor en el corto plazo que a la larga. Por otro lado, en secciones cruzadas, las tasas de ahorro parecieron cambiar sistemáticamente con el nivel de rentas. Además, se observó que grupos de individuos con, niveles más bajos de rentas en media, tenían tasas de ahorro más altas que otros grupos con niveles más altos de renta en media esto se da en cualquier nivel de renta. Finalmente, se observó que las tasas de ahorro están sistemáticamente relacionadas a los cambios en los rentas, siendo mayor para las personas que experimentas aumentos de rentas y menor para las personas que experimentan rentas que disminuyen (ver, Katona 1949).\nTodas estas observaciones contradecían claramente las implicaciones del modelo keynesiano y condujeron a la formulación de los modelos de ciclo de vida y de renta permanente (Modigliani y Brumberg 1980; Friedman 1957). Estos modelos combinaban consistencia teórica en el sentido de que las opciones intertemporales de consumo y ahorro se enmarcaban dentro de un problema de optimización coherente con la capacidad de ajustar la mayoría de los hechos mencionados en el párrafo anterior.\nA nivel macro, es más probable que las fluctuaciones a corto plazo de la renta disponible estén dominadas por la varianza de las perturbaciones temporales que se promediarían a largo plazo (ver, Orazio P. Attanasio y Weber 2010).\nEl desarrollo de las ideas en las contribuciones iniciales de (Modigliani y Brumberg 1980; Friedman 1957) también llevó a la realización de otras implicaciones. En una versión simple del modelo del ciclo de vida, si las rentas tienen forma de joroba y disminuyen al momento de la jubilación, los consumidores ahorraran cuando sean jóvenes para respaldar el consumo en la última parte de la vida y desahorraran cuando sean mayores. (Modigliani y Brumberg 1980) luego demostraron que este hecho puede explicar la correlación entre el crecimiento agregado y el ahorro agregado: el crecimiento implica que, en un año dado, las cohortes más jóvenes, que están ahorrando, son ``más ricas’’ en términos de vida que las de mayor edad, que están desahorrando. Cuanto mayor sea la tasa de crecimiento, mayor será la diferencia de recursos entre los ahorradores y los desahorradores y, por lo tanto, mayor será la tasa agregada de ahorro.\nDespués de su desarrollo inicial, el otro paso importante en el desarrollo del modelo de ciclo de vida/renta permanente, que actualmente se usa como el modelo estándar de la macroeconomía moderna, fue un tratamiento riguroso de la incertidumbre. A fines de la década de 1970, las contribuciones de (Hall 1978, y; MaCurdy 1981, en el contexto de la oferta laboral) explotó la idea de usar las condiciones de primer orden del problema de optimización intertemporal que enfrenta el consumidor para derivar implicaciones comprobables del modelo. Este enfoque, conocido como el enfoque de la ecuación de Euler1, hace posible el análisis empírico de un problema que es analíticamente intratable evitando la necesidad de derivar soluciones de forma cerrada. Esto se logra centrándose en la esencia económica del modelo: los consumidores, en el momento óptimo, actuarán para mantener constante la utilidad marginal de la riqueza a lo largo del tiempo. La utilidad marginal de la riqueza es, al mismo tiempo, una estadística suficiente para las elecciones de los consumidores y, dadas sus propiedades dinámicas, puede “diferenciarse” de manera análoga al tratamiento de los efectos fijos en la econometría (Orazio P. Attanasio y Weber 2010)."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "href": "proyects/Tesis_JXBS/index.html#acontecimientos",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "",
    "text": "Como se mencionó en la introducción, el modelo de ciclo de vida/renta permanente se desarrolló para explicar algunos hechos sobre el consumo.\n\nEl gasto de consumo (no duradero) es menos volátil que la renta y la propensión marginal a consumir parece ser menor en el corto plazo que en el largo plazo. Estos “macro acontecimientos” siguen siendo válidos y algunos también se pueden encontrar en micro datos (como la variabilidad relativa del consumo y las rentas no duraderas, consulte, O. Attanasio 2000, y; O. P. Attanasio y Borella 2006).\nSi uno mira los datos de la Encuesta de Gastos del Consumidor (CEX) de los Estados Unidos, encuentra que la tasa de ahorro de los afrodescendientes es más alta que la de los blancos en cualquier nivel de rentas, como señaló (Friedman 1957). Se puede obtener evidencia similar en EE. UU. y el Reino Unido si se observan las tasas de ahorro por nivel de renta actual de otros grupos que difieren por el nivel de renta “permanente”, como los hogares encabezados por personas con diferentes niveles de educación.\nAnálogamente, si se consideran por separado los individuos cuyas rentas han aumentado y los individuos cuyas rentas han disminuido, la tasa de ahorro de estos últimos es menor que la de los primeros, como señalaron hace varios años (Modigliani y Brumberg 2013), citando el trabajo de Margaret G. Reid.\nPatrones de ciclo de vida de baja frecuencia (Carroll y Summers 1991) muestran que los perfiles de rentas y consumos del de ciclo de vida se siguen mutuamente lo cual contradice una de las principales predicciones del modelo de ciclo de vida.\nFrecuencia del ciclo económico (J. Y. Campbell y Mankiw 1989) encontraron que la regresión de los cambios del \\(\\log(Consumo)\\) agregado para USA sobre las tasas de \\(inter\\acute{e}s\\) y \\(\\Delta \\log(renta_d)\\), atrajo un coeficiente de \\(0.4\\) estadísticamente diferente de cero aun cuando se instrumentaliza las variables. Atribuyen el resultado a la presencia de un gran número de consumidores que siguen una “regla general” y establecen su consumo igual o proporcional a su renta.\n(Hall y Mishkin 1982), usando micro data de USA sobre el consumo de alimentos del PSID encuentra una correlación significativa entre los cambios en el consumo de alimentos y los cambios retardados en las rentas. Interpretan esta evidencia como indicativa de que alrededor del \\(20\\%\\) de los hogares establecen el consumo sobre la base de renta actual, es decir no siguen el modelo de ciclo de vida.\n(Zeldes 1989) utilizando los mismos datos que (Hall y Mishkin 1982), pero categorizando por el nivel de activos (bajo y alto) encuentra que el consumo del primer grupo está mas ligado a la renta que el del segundo grupo e interpreta esta evidencia como la posibilidad de que algunos consumidores se ven afectados por restricciones de liquidez y/o endeudamiento que no les permite fijar el consumo actual en el nivel deseado.\nSi se especifica un modelo de series de tiempo de consumo y renta y además se identifica las innovaciones permanentes a está ultima variable, el modelo predice que estas innovaciones deberían traducirse uno a uno en consumo. Esto implica restricciones paramétricas de ecuaciones cruzadas sobre la representación \\(VAR\\) que se puede estimar. (J. Campbell y Deaton 1989; West 1988; Gali 1991; Hansen, Roberds, y Sargent 1991), señalaron las restricciones mencionadas y utilizando datos agregados de series temporales concluyen que el consumo puede ser demasiado suave en el sentido de que no reacciona lo suficiente para innovaciones en el componente permanente de la renta.\n(O. Attanasio y Pavoni 2008), usando micro datos encuentran Exceso de suavidad (Una excepción es (Deaton 1992))\n\n\n\n\n\n(Deaton y Paxson 1994), notan que en un modelo de ciclo de vida, si la renta tiene raíz unitaria (i.e es \\(I(1)\\). 2) la sección cruzada del consumo aumenta con el tiempo3, Concluyen que a medida que se acumulen las innovaciones, la distribución transversal del consumo se amplia con la edad.\n(Battistin, Blundell, y Lewbel 2009) utilizan un argumento similar para explicar una notable regularidad empírica: la distribución de la sección cruzada del consumo parece aproximarse muy bien a una \\(\\log Normal\\). Bajo una versión estándar del modelo de ciclo de vida, a cualquier edad el “\\(\\log consumo_t=\\log consumo_{t-1}+u_t\\)” 4 y por lo tanto, por sustitución recursiva, se obtiene que el \\(log(consumo)\\) está dado por la suma de innovaciones desde el comienzo de la vida hasta la era actual5.\n(Blundell y Preston 1998), bajo un supuesto de mercado específico, muestran que la evolución relativa del consumo y la desigualdad de la renta pueden utilizarse para identificar variaciones permanentes y transitorias del ingreso y por lo tanto la diferencia entre el aumento de la varianza de la sección cruzada de la renta y la del consumo identificará los cambios en la varianza de la sección cruzada de la renta transitoria.\n(Deaton y Paxson 1994; Jappelli y Pistaferri 2006), hallan evidencia de que dada una distribución inicial del consumo (sin importar cómo se determine) en presencia de un riesgo compartido perfecto esa distribución debe permanecer constante. Por un lado, (Deaton y Paxson 1994), notaron eso en una nota al pie y presentaron evidencia sobre la evolución de sección cruzada del consumo como un rechazo del modelo de mercado completo. Por otro lado, (Jappelli y Pistaferri 2006), explotan esa idea al observar explícitamente los movimientos en la clasificación relativa en la distribución del consumo en una encuesta italiana6\n(O. Attanasio y Davis 1996) al observar la evolución del consumo relativo en diferentes grupos educativos y relacionado con cambios en los cambios salariales relativos e interpretan la evidencia de fuerte correlación en bajas frecuencias entre estas dos variables como evidencia en contra de la hipótesis del mercado completo. No pueden rechazar la hipótesis de que a frecuencias relativamente altos (como un año) no existe una relación entre el consumo y los cambios salariales relativos7"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#ecuación-de-euler-del-consumo",
    "href": "proyects/Tesis_JXBS/index.html#ecuación-de-euler-del-consumo",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Ecuación de Euler del consumo",
    "text": "Ecuación de Euler del consumo\n(Parker 2007) Dice que, considerando un agente de vida infinita que elige una variable de control \\(C\\) en cada período \\(t\\) para maximizar un objetivo intertemporal: \\(\\sum\\limits_{t=1}^\\infty \\beta u(C_t)\\), donde \\(u(C_t)\\) representa el flujo de pago en \\(t\\), \\(u'&gt;0,~u''&lt;0\\), y \\(\\beta\\) es el factor de descuento, \\(0&lt;\\beta&lt;1\\). El agente se enfrenta a una restricción presupuestaria de valor presente: \\(\\sum\\limits_{t=1} R^{1-t}C_t\\leq W_1\\), donde \\(R\\) es la tasa de interés bruta (\\(R=1+r\\), donde \\(r\\) es la tasa de interés) y \\(W_1\\) es dado (mas adelante veremos que es el patrimonio). Por la teoría de optimización, si una trayectoria de tiempo del control es óptima, un aumento marginal en el control en cualquier \\(t\\), \\(dC_t\\), debe tener beneficios en \\(t+1\\) de la misma cantidad de valor presente, \\(-RdC_t\\), así: \\(\\beta^{t-1}u'(C_t)dC_t-\\beta^tu'(C_{t+1})RdC_t=0\\). Reorganizando obtenemos las ecuaciones de Euler: \\(u'(C_t)=\\beta Ru'(C_{t+1}),\\) para \\(t=1,2,3,\\dots.\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#modelo-teórico-el-modelo-de-ciclo-de-vida",
    "href": "proyects/Tesis_JXBS/index.html#modelo-teórico-el-modelo-de-ciclo-de-vida",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Modelo Teórico “EL modelo de ciclo de vida”",
    "text": "Modelo Teórico “EL modelo de ciclo de vida”\n\nPreferencias\nLa versión del modelo que se considera es aquella en la que una unidad de consumo maximiza la utilidad esperada en un intervalo finito sujeto a un conjunto de restricciones. \\[\n\\max E_t \\left[ \\sum_{j=0}^{T-t}\\beta_{t+j}U\\left(C_{t+j},z_{t+j},v_{t+j}\\right) \\right]\n\\tag{1}\\] donde, \\(C\\) representa el “consumo”, \\(z\\) es un vector de variables observables que afecta a la utilidad, \\(v\\) es un vector para factores no observables que afectan a la utilidad, y \\(\\beta\\) es un factor de descuento.\nsujeto a las siguientes restricciones\n\\[\nW_{t+j+1}=W_{t+j}\\left(1+R_{t+j}^\\ast\\right) +y_{t+j} -C_{t+j},\n\\tag{2}\\]\n\\[\nW_{t+j}=\\sum_{i=1}^N A_{t+j}^i,\n\\tag{3}\\]\n\\[\nR_{t+j}^\\ast=\\sum_{i=0}^N \\omega_{t+j}^i R_{t+j}^i,\n\\tag{4}\\]\n\\[\nW_T\\geq0\n\\tag{5}\\]\ndonde, \\(W\\) es el patrimonio neto y su rendimiento, \\(\\omega\\) son las ponderaciones de la cartera, \\(R\\) son los rendimientos, \\(A\\) son los activos y \\(y\\) es el ingreso.\nLa restricción Ecuación 2 es una restricción presupuestaria genérica donde el valor neto aparece junto con su retorno, ingreso y consumo8. Las restricciones Ecuación 3 y Ecuación 4 definen el patrimonio neto, \\(W\\), y su rendimiento \\(-\\omega_{t+j}^i\\): son las acciones (o ponderaciones) de la cartera. El rendimiento del patrimonio neto está dado por el promedio ponderado de los rendimientos individuales, \\(R_{t+j}^i\\). Se supone que estos rendimientos no dependen de la posición neta que tome el consumidor sobre cada uno de estos activos, \\(A_{t+j}^i\\). (ver , Orazio P. Attanasio y Weber 2010)\nLa restricción Ecuación 5 da el límite para el patrimonio neto total en el periodo \\(T\\). El consumidor tiene que morir sin deuda, es decir tiene que pagar su deuda con probabilidad uno9.\nEn esta formulación se supone varias restricciones importantes: * El consumidor maximiza la utilidad esperada. * Las preferencias son aditivamente separables a lo largo del tiempo * Implícitamente es posible anotar la utilidad como una función de una sola mercancía. Esta practica presupone un teorema de agregación del tipo estudiado por (Gorman 1959).\nEl problema formulado anteriormente es capaz de abarcar diferentes versiones del modelo que se han considerado en la literatura. En particular, tratamos como casos especiales el modelo estándar de ingresos permanentes/ciclo de vida con preferencias cuadráticas, el llamado ahorro de existencias reguladoras, así como versiones flexibles del modelo (con un papel importante para la demografía y la oferta laboral) que se han ajustado a los datos.\nComencemos con un caso en el que la función de consumo se puede derivar analíticamente. Sea la utilidad cuadrática en \\(C\\) (y aditivamente separable en sus otros argumentos \\(z\\)), y suponga que al menos un activo financiero se negocia libremente y produce un rendimiento real fijo, igual al parámetro de referencia temporal constante \\(\\frac{1-\\beta}{\\beta}\\). La condición de primer orden con respecto al consumo, o ecuación de Euler, implica que el consumo es paseo aleatorio. \\[\n     E(C_{t+1}|I_t )=C_t\n\\tag{6}\\]\ndonde \\(I_t\\), denota la información disponible al instante \\(t\\) . En efecto notemos que al ser la función de utilidad cuadrática en \\(C\\) y aditivamente separable en sus otros argumentos (Hall 1978) no dice que se cumple exactamente \\(C_{t+1}=\\beta_0 +\\gamma C_t -\\varepsilon_{t+1}\\) de donde tomando la Esperanza al tiempo \\(t\\) y dado que se produce un rendimiento real fijo, tenemos la Ecuación 6.\nen \\(t\\), el consumidor escoge \\(C_t\\) tal que maximiza \\[\n     \\beta_0 U(C_t,z_t,v_t)+ E_t \\sum_{\\tau=t+1}^{T-t} \\beta_{\\tau+j}U(C_\\tau,z_\\tau,v_\\tau)\n\\tag{7}\\] sujeto a \\[\n    W_{\\tau+j}=W_{\\tau-1+j} \\left(1+R_{\\tau-1+j}^\\ast \\right) + y_{\\tau-1+j}-C_{\\tau-1+j}\n\\tag{8}\\] la estrategia secuencial óptima tiene la forma \\[\n    C_t=g_t(w_\\tau,w_{\\tau-1},\\dots,w_0,A_0)\n\\]\nconsiderando una variación desde esta estrategia\nSi los consumidores tienen expectativas racionales, entonces: \\[\n     C_{t+1}=C_t+\\varepsilon_{t+1} \\qquad E\\left( \\varepsilon_{t+1}|W_t \\right)=0\n\\tag{9}\\] para todas las variables \\(W\\) conocidas al instante \\(t\\). La ecuación Ecuación 9 se puede utilizar para derivar una función de consumo, en el caso de que no exista ningún tipo otro activo disponible para el consumidor (como en, Bewley 1977) y la única variable estocástica es la renta del trabajo. Sustituyendo en Ecuación 9 en las restricciones presupuestarias, (Flavin 1981) muestra que el consumo se iguala a la renta permanente, definido como la tasa de interés multiplicada por el valor presente de los ingresos actuales y futuros esperados:\n\\[\n     C_t=\\frac{r}{1+r}A_t +\\frac{r}{1+r}\\sum_{k=0}^\\infty E\\left(y_{t+k} | I_t\\right)\n\\tag{10}\\]\nLa ecuación Ecuación 10 se deriva para el caso especial de vida infinita, pero se puede derivar una extensión a la vida finita."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "href": "proyects/Tesis_JXBS/index.html#otro-modelo-de-consumo",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Otro Modelo de Consumo",
    "text": "Otro Modelo de Consumo\nUna vez que tenemos estas definiciones podemos entonces incluir que si se sigue a (J. Y. Campbell 1987) y se define el ahorro como \\[\n    s_t=\\frac{rA_t}{1+r} +y_t -C_t\n\\tag{11}\\]\nAhora si \\(P(L) y_t=a+\\zeta_t\\), donde \\(P(L)\\) es un polinomio en el operador de retardos y \\(\\zeta_t\\) es un ruido blanco. En este caso la ecuación Ecuación 10 implica (Flavin 1981) que: \\[\n    P\\left(\\frac{1}{1+r}\\right)\\Delta C_{t+1}= \\frac{r}{1+r}\\zeta_{t+1}\n\\tag{12}\\]\nluego podemos reescribir Ecuación 12 como: \\[\n    s_t=- \\sum_{k=1}^\\infty (1+r)^{-k} E\\left(\\Delta y_{t+k}|I_t \\right)\n\\tag{13}\\]\nLa ecuación Ecuación 13 muestra que los individuos deberían “ahorrar para tiempos difíciles” (los ingresos futuros caen), y se mantiene (por la ley de las proyecciones iteradas) incluso si consideramos las expectativas condicionadas a un subconjunto de la información utilizada por los agentes económicos, como el pasado. ingreso y ahorro.\n\nProposición 1 Si \\(C_t\\) es un paseo aleatorio y además los consumidores tienen expectativas racionales y por otro lado \\(Y_t\\sim AR(1)\\) y además \\(Y_t\\sim I(1)\\) entonces: \\[\nC_{t+1}=\\frac{1}{1- \\eta}C_t +\\frac{\\eta}{1-\\eta} y_{t+1} +\\frac{\\eta^2}{1-\\eta}A_{t+1}\n\\tag{14}\\] en el caso de no tener información disponible sobre \\(\\eta\\), donde \\(\\eta\\) es la razón entre la tasa de interés pasada y la tasa de interés actual, y no disponer información del activo \\(A_{t+1}\\), el modelo puedo escribirse como \\[\n    C_{t+1}=aC_t+bY_{t+1}+ u_{t+1}\n\\tag{15}\\] donde \\(u_{t+1}\\) denotaría la innovación al tiempo \\(t+1\\)"
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#revisión-de-los-datos",
    "href": "proyects/Tesis_JXBS/index.html#revisión-de-los-datos",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Revisión de los datos",
    "text": "Revisión de los datos\nEn esta sección se empezara con el análisis de las series de datos obtenidos del Banco Mundial las cuales son Households and NPISHs Final consumption expenditure per capita (constant 2015 US$) [NE.CON.PRVT.PC.KD] de los cuales se seleccionaran 14 países los cuales no tienen perdida de información. Estos países son: Bolivia, Brasil, Chile, Colombia, Costa Rica, Ecuador, Guatemala, Honduras, México, Nicaragua, Panamá, Paraguay, Perú, y República Dominicana.\nEn esta sección Hablaremos un poco de la comunidad andina que es de la cual se tiene conocimiento relevante en cuanto a su historia. La comunidad andina (CAN) esta integrada por los siguientes países: Bolivia, Colombia, Ecuador y Perú\n\nBolivia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 1: Serie para Bolivia original y corregida por software TRAMO/SEATS10\n\n\n\n\n\n\n\n\n\nFigura 2: Serie de irregularidades para Bolivia\n\n\n\nEn la figura Figura 2 se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1962\\)\n\\(1964\\) El gobierno se vio interrumpido por medio de un golpe militar, a partir de lo cual Bolivia habría de vivir dictaduras.\n\\(1972\\) Presencia de una dictadura en el gobierno y contrato de venta de gas a Argentina.\n\\(1977\\) Extraordinario nivel de precios de las materias primas (el estaño llegó a cotizarse en ocho dólares la libra ﬁna) y una gran apertura de créditos internacionales.\n\nEn la figura Figura 1 (b) se puede observar que la serie posee cierta tendencia que en principio debe ser estocástica puesto que la población y el desarrollo sigue en crecimiento.\n\n\nColombia\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 3: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 4: Serie de irregularidades para Colombia\n\n\n\nEn la figura Figura 4} se observa que la serie presenta irregularidades que se describen a continuación\n\n\\(1964\\) Aparecen las FARC ,quienes aprobaron en su constitución un programa agrario que pretende la entrega gratuita de las tierras a los campesinos\n\\(1965\\) Empiezan los ataques de las guerrillas mismo que se extienden hasta estos días.\n\\(1992\\) Drásticos recortes de energía en este país, mismos que acarrearon perdidas millonarias.\n\\(1998\\) Se crea la “Zona de distensión” e inicia el proceso de paz con las guerrillas.\n\\(1999\\) Debido a la gran cantidad de demandas que se interpusieron frente a la voracidad de los bancos y contra el refinado mecanismo del UPAC, el cual estafó a quienes se arriesgaron a endeudarse para tener casa, la Corte Constitucional resolvió acabarlo con la sentencia C-700\n\\(2019\\)\n\\(2020\\) COVID\n\n\n\nEcuador\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 5: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 6: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura 6} se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1973\\) La apropiación de los beneficios del petróleo “la renta petrolera” se constituyó en objetivo de disputa de grupos sociales y organizaciones políticas.\n\\(1998\\) La permisividad social a favor de monopolios y oligopolios11\n\\(1999\\) Feriado Bancario\n\n\n\nPerú\n\n\n\n\n\n\n\n\n\n\n\n(a) Original\n\n\n\n\n\n\n\n\n\n\n\n(b) Corregida\n\n\n\n\n\n\n\nFigura 7: Serie para Colombia original y corregida por software TRAMO/SEATS\n\n\n\n\n\n\n\n\n\nFigura 8: Serie de irregularidades para Ecuador\n\n\n\nEn la figura Figura 8 se observa que la serie presenta irregularidades que se describen a continuación:\n\n\\(1983\\) Fenómeno del niño y caída del precio de los metales.\n\\(1985\\) El sol fue reemplazado por el inti con un valor de \\(1000\\) soles (devaluación de la moneda).\n\\(1987\\) Se empieza a sentir los efectos de las políticas intervencionistas implementadas un años atrás.\n\\(1989\\) Devaluación del inti\n\nA continuación en la figura Figura 9 presentamos las gráficas de Consumo y Renta para cada uno de los países de Latinoamérica y Caribe seleccionados para este estudio.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\nFigura 9: Gráfica de Series Consumo y Renta para los 14 países de Latinoámerica y Caribe\n\n\n\nEn la tabla Tabla 1 se realiza el contraste de Dickey-fuller aumentado (ADF) para las series corregidas.\n\n\n\nTabla 1: Contraste de raíz unitaria Dickey Fuller aumentado\n\n\n\n\n\nPaís\nRaiz Unitaria\n\\(p\\)-value\nlags\n\n\n\n\n\nRenta\n\n\n\n\nBolivia\nsi\n0,9861\n1\n\n\nBrasil\nsi\n0,9818\n1\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n1,0000\n0\n\n\nEcuador\nsi\n0,9680\n1\n\n\nGuatemala\nsi\n0,9965\n1\n\n\nHonduras\nsi\n0,9979\n0\n\n\nMéxico\nsi\n0,9945\n0\n\n\nNicaragua\nsi\n0,6621\n1\n\n\nPanamá\nsi\n0,9995\n0\n\n\nParaguay\nsi\n0,9966\n1\n\n\nPerú\nsi\n0,9830\n1\n\n\nR. Dominicana\nsi\n1,0000\n1\n\n\n\nConsumo\n\n\n\n\nBolivia\nsi\n0,9947\n1\n\n\nBrasil\nsi\n0,9982\n0\n\n\nChile\nsi\n1,0000\n0\n\n\nColombia\nsi\n1,0000\n0\n\n\nCosta Rica\nsi\n0,9978\n1\n\n\nEcuador\nsi\n0,9902\n0\n\n\nGuatemala\nsi\n0,9990\n1\n\n\nHonduras\nsi\n0,9998\n1\n\n\nMéxico\nsi\n0,9789\n1\n\n\nNicaragua\nsi\n0,6414\n1\n\n\nPanamá\nsi\n0,9975\n0\n\n\nParaguay\nsi\n1,0000\n0\n\n\nPerú\nsi\n0,9604\n1\n\n\nR. Dominicana\nsi\n1,0000\n0\n\n\n\n\n\n\nEn la tabla Tabla 1 se ha realizado el contraste ADF para cada una de las series tanto de Renta (PIB) como de Consumo (Consumo familiar). Es así que de manera general tenemos que para todos las series.\nDada la hipótesis nula de no estacionariedad frente a la alternativa de estacionariedad, El estadístico de contraste de ADF para el caso sin constante para el cual su distribución se encuentra tabulada por Dickey Fuller. Entonces para un nivel de significancia del \\(\\alpha=0.05\\) puesto que el valor \\(p\\) del estadístico es mayor que \\(\\alpha\\), No se rechazo la hipótesis nula de no estacionariedad.\nAhora bien se procederá a realizar el contraste de Johansen para el caso sin constante.\n\n\n\nTabla 2: Contraste de Cointegración de Johansen\n\n\n\n\n\n\n\n\n\n\n\n\nPaís\nCointegración\n\\(p\\)-valor asint.\nboot \\(p\\)-valor\nrango\n\n\n\n\nBolivia\nsi\n0,041\n0,089\n1\n\n\nBrasil\nsi\n0,345\n0,431\n1\n\n\nChile\nsi\n0,077\n0,131\n1\n\n\nColombia\nsi\n0,571\n0,814\n1\n\n\nCosta Rica\nsi\n0,297\n0,639\n1\n\n\nEcuador\nno\n0,018\n0,01\n1\n\n\nGuatemala\nsi\n0,602\n0,906\n1\n\n\nHonduras\nno\n0,082\n0,085\n0\n\n\nMéxico\nno\n0,144\n0,307\n0\n\n\nNicaragua\nno\n0,063\n0,079\n0\n\n\nPanamá\nsi\n0,247\n0,354\n1\n\n\nParaguay\nsi\n0,026\n0,055\n1\n\n\nPerú\nsi\n0,333\n0,499\n1\n\n\nR. Dominicana\nsi\n0,139\n0,379\n1\n\n\n\n\n\n\nEn la tabla Tabla 2 se ha realizado el contraste de cointegración de Johansen entre las series tanto de Renta (PIB) como de Consumo (Consumo familiar) para cada país. Es así que de manera general tenemos que para cada país.\nDada la hipótesis nula de que el rango de la matriz de cointegración es \\(0\\) frente a la alternativa de que el rango de cointegración es \\(1\\). Entonces tenemos que para Honduras, México, Nicaragua, las series de consumo y renta no están cointegradas pues para un nivel de significancia \\(alpha=0.05\\) no se rechaza la hipótesis nula de que el rango de la matriz de cointegración es \\(0\\). Por otro lado, para el resto de países menos Ecuador se tiene que dada la hipótesis nula rango de la matriz de cointegración es \\(1\\) frente a la alternativa rango de la matriz de cointegración es \\(2\\), para un nivel de significancia \\(\\alpha=0.05\\) no se rechaza la hipótesis nula de que el rango de cointegración es \\(1\\) con valores \\(p\\) dados en la tabla Tabla 2. Finalmente, para Ecuador se tienen que para un nivel de significancia \\(\\alpha=0.05\\) Se rechaza la hipótesis nula de que el rango de cointegración es \\(1\\).\n\n\n\nTabla 3: Contraste de no Causalidad de Dumitrescu-Hurlin entre Consumo y Renta\n\n\n\n\n\nPaís\n\\(Y\\) no causa \\(C\\)\np-valor\ncausalidad\n\n\n\n\nBolivia\nsi\n0,6174\nno\n\n\nBrasil\nno\n0,0002\nsi\n\n\nChile\nno\n0,0027\nsi\n\n\nColombia\nsi\n0,9225\nno\n\n\nCosta Rica\nsi\n0,2856\nno\n\n\nEcuador\nno\n0,0172\nsi\n\n\nGuatemala\nno\n0,0001\nsi\n\n\nHonduras\nsi\n0,6704\nno\n\n\nMéxico\nsi\n0,4979\nno\n\n\nNicaragua\nsi\n0,3064\nno\n\n\nPanamá\nsi\n0,5101\nno\n\n\nParaguay\nsi\n0,8212\nno\n\n\nPerú\nno\n0,0002\nsi\n\n\nR. Dominicana\nsi\n0,7106\nno\n\n\n\n\n\n\nEn la tabla Tabla 3 se ha realizado un resumen del contraste de no causalidad de Dumitrescu-Hurlin para cada modelo de consumo de cada país. Dada la hipótesis nula de \\(Y\\) no causa \\(X\\) frente a la hipótesis alternativa \\(Y\\) causa \\(X\\). tenemos que para los siguientes países: Brasil, Chile, Ecuador, Guatemala, Perú. Para un nivel de significancia \\(\\alpha=0.05\\) se rechaza la hipótesis nula de no causalidad con valores \\(p\\) dados en la tabla Tabla 3. Para el resto de países no se rechaza la hipótesis nula de no causalidad."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#resultados",
    "href": "proyects/Tesis_JXBS/index.html#resultados",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Resultados",
    "text": "Resultados\nEn la tabla Tabla 1 se ha evidenciado que tanto las series de Consumo así como las series De Renta poseen una raíz unitaria es decir son series integradas de orden uno. Además se observa que para la mayoría de países el valor \\(p\\) es mayor a \\(0.95\\) es decir tenemos una gran certeza de que nuestras series son no estacionarias, lo cual nos permite realizar el análisis sobre el modelo Ecuación 15.\nAhora bien, una vez tenemos que las series son no estacionarias en la tabla Tabla 2 se ha realizado la prueba de cointegración de Johansen para el caso sin constante. De donde, se ha obtenido que los países de Ecuador, Honduras, México, y Nicaragua no se encuentra cointegrados esto puede ser debido a que para estos países las series tanto de Consumo y Renta al momento de analizar las intervenciones para esta se evidencio que las series tienen ciertas irregularidades que si bien se esperaba que quedaran enterradas en las innovaciones, estas no pueden absorberlas del todo.\nFinalmente, para los países cuyo modelo si esta cointegrado se ha realizado un contraste de causalidad viendo a estos datos como un panel de series apiladas. Es así que se ha obtenido que para países cuyas intervenciones han sido absorbidas por las innovaciones el modelo propuesto en este trabajo ayuda a mejorar la predicción del consumo."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#conclusiones",
    "href": "proyects/Tesis_JXBS/index.html#conclusiones",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Conclusiones",
    "text": "Conclusiones\nDado que uno de los objetivos de este trabajo fue revisar los modelos de ciclo de vida y funciones de consumo desde el punto de vista neoclásico, para los cuales si bien el lineamiento esta bastante acertado, en este trabajo se ha propuesto un modelo que mejora el modelo neoclásico. Esto se evidencia para países en los que los datos no tienen intervenciones de gran impacto es decir que su perturbaciones pueden ser absorbidas por las innovaciones al instante \\(t+1\\). El modelo Ecuación 15 ayuda a predecir de manera mas eficiente el Consumo; debido a que para estos países el tener información acerca de la renta en cada instante ayuda a predecir el consumo al instante \\(t+1\\). Por otro lado para los países, para los cuales las innovaciones causan impactos demasiado grandes, no se puede decir con certeza que el modelo propuesto no seria bueno. Puesto que, si se lograra mitigar los mencionados impactos en el tratamiento de la data podría darse cualquier resultado. Desde un punto de vista econométrico el modelo propuesto bajo las condiciones impuestas ayuda a establecer una relación entre la Renta y el Consumo esta relación es del tipo causal de Granger."
  },
  {
    "objectID": "proyects/Tesis_JXBS/index.html#footnotes",
    "href": "proyects/Tesis_JXBS/index.html#footnotes",
    "title": "Estudio Teórico del Comportamiento del Consumo en Latinámerica y Caribe (Resumen)",
    "section": "Notas",
    "text": "Notas\n\n\nUna ecuación de Euler del consumo, a grandes rasgos, es una condición matemática que describe el comportamiento de una senda óptima de consumo bajo los supuestos de elección intertemporal, expectativas racionales y agente representativo, entre otros, (Parker 2007)↩︎\nLas series integradas son un caso particular de series no estacionarias. Se dice que una serie temporal \\(x_t\\) es integrada de orden \\(d\\), \\(I(d)\\), cuando es necesario diferenciarla \\(d\\) veces para convertirla en estacionaria (Engle y Granger 1987) ↩︎\nEntonces se puede considerar como la varianza cruzada del consumo para una cohorte de individuos.↩︎\n\\(u_t\\) innovaciones en la renta permanente↩︎\nPor el TCL, \\(\\sum u_t \\overset{d}{\\sim} Normal\\) con \\(u_t\\) independientes , bajo supuestos de regularidad incluso si \\(u_t \\not\\sim Normal\\)↩︎\nAl igual que con otros documentos, rechazan enérgicamente la suposición de una perfecta distribución del riesgo.↩︎\nSegún (Orazio P. Attanasio y Weber 2010) esto parece indicar que, de alguna manera, en altas frecuencias los choques salariales son absorbidos y no se reflejan en el consumo.↩︎\nPor ejemplo, es posible que el ingreso esté dado por la tasa de salario multiplicada por el número de horas trabajadas, donde el número de horas es uno de los componentes de \\(z\\)↩︎\nEsta simple restricción impone limitaciones cuantitativamente importantes a la capacidad de suavizar el consumo↩︎\nTRAMO significa “Time series Regression with ARIMA noise, Missing values and Outliers” y SEATS “Signal Extraction in ARIMA Time Series”. Estos programas (que normalmente se usan juntos) han sido desarrollados por Víctor Gómez y Agustín Maravall del Banco de España.↩︎\nDado el fenómeno del niño (1995) el cual fue uno de los desastres naturales mas grandes que ha impactado al Ecuador seria el principio de la bola de nieve que desencadenaría en 1999 con el ya tan conocido “Feriado Bancario”↩︎"
  }
]