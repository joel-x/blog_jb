[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joel Burbano",
    "section": "",
    "text": "Ingeniero Matem√°tico\nAnalista de Datos\nCient√≠fico de Datos"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de mi",
    "section": "",
    "text": "üëã ¬°Hola! Soy Joel, graduado de Ingeniero Matem√°tico en la Facultad de Ciencias de la Escuela Polit√©cnica Nacional.\nüéì Durante mi etapa universitaria he adquirido las bases para gestionar modelos de riesgo, modelos econom√©tricos, modelos estad√≠sticos, modelos de programaci√≥n entera. As√≠ tambi√©n, he adquirido las bases de matem√°tica actuarial, estad√≠stica matem√°tica, investigaci√≥n operativa. Adem√°s, me he capacitado en el manejo de lenguajes de programaci√≥n tales como: C++, Matlab, R, Python, y tambi√©n manejo de paquetes estad√≠sticos Statgraphics y Gretl.\nüìù El desarrollo de mi trabajo de titulaci√≥n se enfoca en desarrollar un modelo de consumo a partir de la Hip√≥tesis de Renta Permanente de Friedman (Novel Economia,1976) y El Ciclo de Vida de Modigliani.\nüéØMi objetivo es seguir desarrollando mis habilidades en el √°rea estad√≠stica, econom√©trica, actuarial y ciencia de datos, por lo que me encuentro altamente interesado en trabajar en las mencionadas √°reas."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          T√≠tulo\n        \n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - M√°s reciente\n        \n         \n          Autor/a\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nGaussian Mixture Models\n\n\n\nPython\n\n\nAprendizaje No Supervisado\n\n\nClusters\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalisis de Componentes Principales\n\n\n\nPython\n\n\nPCA\n\n\nReducci√≥n de dimensiones\n\n\nAprendizaje No Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n20 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegresi√≥n Lineal Simple\n\n\n\nPython\n\n\nAprendizaje Supervisado\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes Clasificaci√≥n\n\n\n\nAprendizaje supervisado\n\n\nPython\n\n\nNaive Bayes\n\n\n\n\n\n\n\nJoel Burbano\n\n\n19 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDetecci√≥n de fraude con tarjetas de Cr√©dito\n\n\n\nPython\n\n\n\n\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHiperparametros y Modelos de Validaci√≥n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\nJoel Burbano\n\n\n17 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\n15 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O‚ÄôMalley\n\n\n12 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTratando los datos Keane\n\n\n\nEconometr√≠a\n\n\nPython\n\n\n\n\n\n\n\nJoel Burbano\n\n\n7 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPower BI\n\n\n\nPower BI\n\n\nBI\n\n\n\n\n\n\n\nJoel Burbano\n\n\n6 dic 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEstad√≠stica Descriptiva\n\n\n\nEconometr√≠a\n\n\nPython\n\n\n\n\n\n\n\nJoel Burbano\n\n\n30 jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHello R Markdown\n\n\n\nR Markdown\n\n\nplot\n\n\nregression\n\n\n\n\n\n\n\nFrida Gomam\n\n\n1 dic 2020\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyects.html",
    "href": "proyects.html",
    "title": "Proyectos",
    "section": "",
    "text": "Tesis de Grado\n\n\n\nEconometr√≠a\n\n\nModelos de Consumo\n\n\nCausalidad\n\n\n\n\n\n\n\nJoel Burbano\n\n\n15 dic 2023\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/2020-12-01-r-rmarkdown/index.html",
    "href": "posts/2020-12-01-r-rmarkdown/index.html",
    "title": "Hello R Markdown",
    "section": "",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\n\nsummary(cars)\n##      speed           dist       \n##  Min.   : 4.0   Min.   :  2.00  \n##  1st Qu.:12.0   1st Qu.: 26.00  \n##  Median :15.0   Median : 36.00  \n##  Mean   :15.4   Mean   : 42.98  \n##  3rd Qu.:19.0   3rd Qu.: 56.00  \n##  Max.   :25.0   Max.   :120.00\nfit &lt;- lm(dist ~ speed, data = cars)\nfit\n## \n## Call:\n## lm(formula = dist ~ speed, data = cars)\n## \n## Coefficients:\n## (Intercept)        speed  \n##     -17.579        3.932\n\n\n\nIncluding Plots\nYou can also embed plots. See Figure @ref(fig:pie) for example:\n\npar(mar = c(0, 1, 0, 1))\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n\n\n\n\nA fancy pie chart.\n\n\n\n\n\nsummary(Orange)\n##  Tree       age         circumference  \n##  3:7   Min.   : 118.0   Min.   : 30.0  \n##  1:7   1st Qu.: 484.0   1st Qu.: 65.5  \n##  5:7   Median :1004.0   Median :115.0  \n##  2:7   Mean   : 922.1   Mean   :115.9  \n##  4:7   3rd Qu.:1372.0   3rd Qu.:161.5  \n##        Max.   :1582.0   Max.   :214.0"
  },
  {
    "objectID": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "href": "posts/2023-12-07-tratando-los-datos-keane/index.html",
    "title": "Tratando los datos Keane",
    "section": "",
    "text": "En este post abordaremos un poco los datos keane obtenidos de Gretl\nEmpezaremos por importar las librerias necesarias\n\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns=None\nfrom scipy import stats\nimport matplotlib.pyplot as plt \nimport seaborn as sb\n\nA continuaci√≥n nos disponemos a visualizar los datos\n\nkeane.head()\n\n   id  numyrs  year  choice  wage  educ  expwc  expbc  expser  manuf  black  \\\n0   1       9    81     2.0   NaN    10      0      0       0    0.0      1   \n1   1       9    82     2.0   NaN    10      0      0       0    0.0      1   \n2   1       9    83     2.0   NaN    10      0      0       0    0.0      1   \n3   1       9    84     1.0   NaN    10      0      0       0    0.0      1   \n4   1       9    85     2.0   NaN    11      0      0       0    0.0      1   \n\n   lwage  enroll  employ  attrit  exper  expersq  status  \n0    NaN       0       0       0      0        0     2.0  \n1    NaN       0       0       0      0        0     2.0  \n2    NaN       0       0       0      0        0     2.0  \n3    NaN       1       0       0      0        0     1.0  \n4    NaN       0       0       0      0        0     2.0  \n\n\nCrearemos etiquetas para las observaciones de acuerdo a ‚Äúchoice‚Äù estudiante=1, hogar=2, cualificado=3, no-cualificado=4, servicio=5\nProcedemos a gr√°ficar la evoluci√≥n de salarios separado por color de piel\n\n\n\n\n\nEn esta grafica evidenciamos que a lo largo de los a√±os aumenta la discriminaci√≥n.\nVisualicemos lo siguiente: seleccionando s√≥lo las personas que trabajan se realizara un gr√°fico de la evoluci√≥n de los salarios separados por la variable choice\n\n\n\n\n\nSe observa que la terciarizaci√≥n de la econom√≠a ha aumentado las diferencias entre trabajadores cualificados y no cualificados, as√≠ como, entre servicio e industria.\nAhora procedamos al an√°lisis de la variable educ para ello primero la Codificaremos de acuerdo a educaci√≥n b√°sica=1, educaci√≥n media=2, y educaci√≥n superior=3.\nSeleccionando solo las personas que trbajan tenemos lo siguiente:"
  },
  {
    "objectID": "posts/2023-07-30-prueba/index.html",
    "href": "posts/2023-07-30-prueba/index.html",
    "title": "Estad√≠stica Descriptiva",
    "section": "",
    "text": "La estad√≠stica descriptiva nos permite realizar el an√°lisis exploratorio de la informaci√≥n y el pre-procesamiento de la informaci√≥n antes de tener un modelo de machine learning.\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"cars.csv\")\ndf.head()\n\n   speed  dist\n0      4     2\n1      4    10\n2      7     4\n3      7    22\n4      8    16"
  },
  {
    "objectID": "posts/2023-12-06-power-bi/index.html",
    "href": "posts/2023-12-06-power-bi/index.html",
    "title": "Power BI",
    "section": "",
    "text": "Pruebas para publicar power bi"
  },
  {
    "objectID": "proyects/tesis/index.html",
    "href": "proyects/tesis/index.html",
    "title": "Tesis de Grado",
    "section": "",
    "text": "En este peque√±o blog, se hablara de las partes mas importantes desarrolladas en el trabajo de integraci√≥n curricular"
  },
  {
    "objectID": "posts/Deteccion_fraude/index.html",
    "href": "posts/Deteccion_fraude/index.html",
    "title": "Detecci√≥n de fraude con tarjetas de Cr√©dito",
    "section": "",
    "text": "En el presente proyecto se pretende analizar un conjunto de datos de transacciones crediticias recolectadas durante dos d√≠as en el mes de de Septiembre del 2013 por European cardholders\nEmpezaremos por importar las librerias necesarias para realizar el an√°lisis\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nProcedemos a leer los datos con pandas\n\n#df: base de datos de las transacciones\ndf=pd.read_csv(\"creditcard.csv.zip\")\n\nVerificamos que cantidad de datos tenemos\n\ndf.shape\n\n(284807, 31)\n\n\nTenemos que existen 31 columnas (variables) y 284807 filas (registros)\nAhora bien procedemos a revisar la calidad de este conjunto de datos\n\ndf.isnull().any()\n\nTime      False\nV1        False\nV2        False\nV3        False\nV4        False\nV5        False\nV6        False\nV7        False\nV8        False\nV9        False\nV10       False\nV11       False\nV12       False\nV13       False\nV14       False\nV15       False\nV16       False\nV17       False\nV18       False\nV19       False\nV20       False\nV21       False\nV22       False\nV23       False\nV24       False\nV25       False\nV26       False\nV27       False\nV28       False\nAmount    False\nClass     False\ndtype: bool\n\n\nObservamos que no hay variables con datos nulos.\nAhora bien echemos un vistazo a la variable Class la cual contiene la informaci√≥n sobre las transacciones fraudulentas\n\ndf[\"Class\"].value_counts()\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nNotamos que solamente 492 transacciones son fraudulentas\n\ndf['Class'].value_counts(normalize=True)\n\nClass\n0    0.998273\n1    0.001727\nName: proportion, dtype: float64\n\n\nes decir solo el \\(0.17\\%\\) de transacciones son fraudulentas\nAhora bien empecemos a intentar predecir\n\nfrom sklearn.model_selection import train_test_split\n\n\nX=df.drop(labels='Class',axis=1)\ny=df.loc[:,'Class']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=1, stratify=y)\n\nAhora bien, se realiza un an√°lisis exploratorio de los datos\n\nX_train['Time'].describe()\n\ncount    199364.000000\nmean      94675.212852\nstd       47536.519022\nmin           0.000000\n25%       54039.000000\n50%       84588.500000\n75%      139243.250000\nmax      172792.000000\nName: Time, dtype: float64\n\n\nRealicemos una conversi√≥n de la variable Time de segundos a horas para facilitar la interpretaci√≥n\n\nX_train.loc[:,'Time']=X_train.Time/3600\nX_test.loc[:,'Time']=X_test.Time/3600\n\n\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Time'],bins=40,kde=False)\nplt.xlim([0,40])\nplt.xticks(np.arange(0,48,6))\nplt.xlabel('Tiempo despues de la primera transacci√≥n (h)')\nplt.ylabel('Conteo')\nplt.title('Tiempo de transacciones')\nplt.show()\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\nNotamos que existen dos picos en la gr√°fica, el primero entre las 10 y 22 primeras horas y el segundo entre las 34 y 40 horas\nAnalicemos la variable Amount\n\nX_train['Amount'].describe()\n\ncount    199364.000000\nmean         88.659351\nstd         247.240287\nmin           0.000000\n25%           5.637500\n50%          22.000000\n75%          78.000000\nmax       25691.160000\nName: Amount, dtype: float64\n\n\nrealicemos una revisi√≥n gr√°fica\nprimero un histograma\n\nplt.figure(figsize=(12,4))\nsb.displot(X_train['Amount'],bins=50,kde=False)\nplt.ylabel('Conteo')\nplt.title('Montos de Transacci√≥n')\nplt.show()\n\n&lt;Figure size 1152x384 with 0 Axes&gt;\n\n\n\n\n\n\nplt.figure(figsize=(12,4))\nsb.boxplot(x=X_train[\"Amount\"])\nplt.show()\n\n\n\n\nObservemos que los datos se encuentran fuertemente sesgados a la derecha. Para asegurarnos calculamos la asimetr√≠a\n\nX_train['Amount'].skew()\n\n16.950540423177653\n\n\n\nX_train.head(5)\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\n\n\n\n\n105644\n19.340833\n1.135011\n-0.663898\n0.703924\n0.069871\n-0.488154\n1.312078\n-0.897198\n0.463148\n-0.478801\n...\n-0.677938\n-0.331487\n-0.069644\n0.183987\n-0.618678\n0.089015\n0.521419\n0.086390\n0.004782\n1.00\n\n\n139790\n23.155278\n-1.786262\n1.118886\n1.347969\n-0.379954\n-1.240680\n0.467667\n0.081125\n0.964933\n0.042585\n...\n-0.099266\n-0.047902\n-0.182530\n-0.162509\n-0.405178\n0.512595\n0.299398\n-0.042882\n-0.059130\n141.73\n\n\n158758\n31.034167\n-0.683414\n0.679341\n2.615556\n2.362138\n-0.012716\n0.603826\n0.574245\n-0.679978\n-0.811409\n...\n0.372610\n-0.007167\n0.463597\n-0.243134\n0.084557\n-0.453177\n2.687676\n-1.084269\n-0.511626\n36.19\n\n\n130845\n22.067778\n1.183540\n-0.493000\n0.755202\n-0.963160\n-0.850295\n0.145905\n-0.794616\n0.302199\n1.656943\n...\n-0.168134\n0.039588\n0.339340\n-0.053125\n-0.298049\n0.423994\n-0.652284\n0.102582\n0.017292\n1.00\n\n\n88908\n17.318056\n1.137583\n0.105478\n0.784402\n1.254973\n-0.600870\n-0.360836\n-0.161727\n0.076092\n0.280587\n...\n-0.178887\n-0.195692\n-0.443664\n0.046270\n0.516246\n0.447943\n-0.554949\n0.031821\n0.018177\n7.60\n\n\n\n\n5 rows √ó 30 columns"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html",
    "href": "posts/Hyperparameters and Model Validation/index.html",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta raz√≥n es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAqu√≠, cada fila de la data se refiere a la observaci√≥n de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#representaci√≥n-de-la-data-en-scikit-learn",
    "href": "posts/Hyperparameters and Model Validation/index.html#representaci√≥n-de-la-data-en-scikit-learn",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "",
    "text": "Machine Learning se trata de crear models desde los datos: por esta raz√≥n es necesario entender como se representa la data en una computadora. En nuestro caso particular con Scikit-Learn la manera de tratar la data es como una tabla.\n\n\nUna tabla basica es un arreglo bi-dimensional de datos, en donde cada fila representa un elemento individual del conjunto de datos, y cada columna representa cantidades realacionadas con cada uno de estos elementos.\nPor ejemplo la ya conocidada base iris\n\nimport seaborn as sb\niris=sb.load_dataset('iris')\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAqu√≠, cada fila de la data se refiere a la observaci√≥n de una flor, y el numero de filas es el total de flores observadas."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "href": "posts/Hyperparameters and Model Validation/index.html#matriz-de-caracteristicas-features-matrix",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Matriz de caracteristicas (Features matrix)",
    "text": "Matriz de caracteristicas (Features matrix)\nEsta tabla contienene la informaci√≥n caracteeristica del conjunto de datos en nuestro ejemplo contiene informaci√≥n de flores. Matem√°ticamente estas caracteristicas pasan a representar las variables independientes de nuestro conjunto de datos generalmente denotado por \\(X\\)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "href": "posts/Hyperparameters and Model Validation/index.html#objetivo-target-array",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Objetivo (Target Array)",
    "text": "Objetivo (Target Array)\nEs un arraglo que matem√°ticamente representa la variable dependiente generalmente notada por \\(y\\)\n\nimport matplotlib.pyplot as plt\n\n\n#plt.figure(figsize=(12,8))\nsb.set()\nsb.pairplot(iris,hue='species',size=1.5)\n\nC:\\Users\\JXBS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\nPara usar los datos en Scikit-Learn, tenemos que extraer los matriz \\(X\\) y el objetivo \\(y\\)\n\nX_iris=iris.drop('species',axis=1)\nprint(X_iris.shape)\ny_iris=iris['species']\nprint(y_iris.shape)\n\n(150, 4)\n(150,)"
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "href": "posts/Hyperparameters and Model Validation/index.html#scikit-learns-api-estimador",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Scikit-Learn‚Äôs API estimador",
    "text": "Scikit-Learn‚Äôs API estimador\nLa API de Scikit-Learn esta dise√±ada con los siguientes principios en mente\n\nCoherencia.- Todos los objetos comparten unaa interfaz com√∫n extraida de un conunto limitado de m√©todos, con documentaci√≥n consistente.\nInspecci√≥n Todos los valores de par√°metros especificados se exponen como atributos p√∫blicos. Jerarqu√≠a de objetos limitada S√≥lo los algoritmos est√°n representados por clases de Python; los conjuntos de datos se representan en formatos est√°ndar (matrices NumPy, Pandas DataFrames, matrices dispersas de SciPy) y los nombres de los par√°metros utilizan cadenas est√°ndar de Python.\nComposici√≥n Muchas tareas de aprendizaje autom√°tico se pueden expresar como secuencias de algoritmos m√°s fundamentales, y ScikitLearn hace uso de esto siempre que es posible.\nValores predeterminados sensatos Cuando los modelos requieren par√°metros especificados por el usuario, la biblioteca define un valor predeterminado apropiado."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#b√°sicos-de-la-api",
    "href": "posts/Hyperparameters and Model Validation/index.html#b√°sicos-de-la-api",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "B√°sicos de la API",
    "text": "B√°sicos de la API\nPor lo general, los pasos para usar la API del estimador Scikit-Learn son los siguientes (repasaremos algunos ejemplos detallados en las secciones siguientes):\n\nElija una clase de modelo importando la clase de estimador adecuada de ScikitLearn.\nElija los hiperpar√°metros del modelo creando una instancia de esta clase con los valores deseados.\nOrganice los datos en una matriz de caracter√≠sticas y un vector objetivo siguiendo la discusi√≥n anterior.\nAjuste el modelo a sus datos llamando al m√©todo fit() de la instancia del modelo.\nAplique el modelo a nuevos datos:\n\n‚Ä¢ Para el aprendizaje supervisado, a menudo predecimos etiquetas para datos desconocidos usando el m√©todo predict().\n‚Ä¢ Para el aprendizaje no supervisado, a menudo transformamos o inferimos propiedades de los datos utilizando el m√©todo transform() o predict()."
  },
  {
    "objectID": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresi√≥n-lineal-simple",
    "href": "posts/Hyperparameters and Model Validation/index.html#ejemplo-de-aprendizaje-supervisado-regresi√≥n-lineal-simple",
    "title": "Hiperparametros y Modelos de Validaci√≥n",
    "section": "Ejemplo De Aprendizaje Supervisado: Regresi√≥n Lineal Simple",
    "text": "Ejemplo De Aprendizaje Supervisado: Regresi√≥n Lineal Simple\n\nimport numpy as np\n\n\nrng=np.random.RandomState(42)\nx=10+rng.rand(50)\ny=2*x-1+rng.rand(50)\nplt.scatter(x,y)\n\n&lt;matplotlib.collections.PathCollection at 0x2a4c8d6aae0&gt;\n\n\n\n\n\n\nEscojemos el modelo\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nEscogemos los hiperparametros\n\n\nmodel=LinearRegression(fit_intercept=True)\nmodel\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nOrganizando los datos en variables independientes y variable dependiente\n\n\nX=x[:,np.newaxis]\nX.shape\n\n(50, 1)\n\n\n\nAjustando el modelo\n\n\nmodel.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nmodel.coef_\n\narray([2.06607665])\n\n\n\nmodel.intercept_\n\n-1.1957940680607742\n\n\n\nPredicciendo data desconocida\n\n\nxfit=np.linspace(-1,11)\n\n\nXfit=xfit[:,np.newaxis]\nyfit=model.predict(Xfit)\n\n\nplt.scatter(x,y)\nplt.plot(xfit,yfit)"
  },
  {
    "objectID": "posts/Linear_Regression/index.html",
    "href": "posts/Linear_Regression/index.html",
    "title": "Regresi√≥n Lineal Simple",
    "section": "",
    "text": "En este post presentamos un ejemplo b√°sico de regresi√≥n lineal simple, es decir, el caso de ajustar una l√≠nea a datos \\(x,y\\).\nAntes que nada importamos las librerias necesarias\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nPrimeramente generamos unos puntos para \\(x\\) y para \\(y\\)\n\nrng=np.random.RandomState(42)\nx=18*rng.rand(40)\ny=1.5*x-3+rng.rand(40)\nplt.scatter(x,y)\nplt.show()\n\n\n\n\nAhora definimos los hiperpar√°metros de nuestro modelo\n\nLineal=LinearRegression(fit_intercept=True) # La regresi√≥n lineal es de la forma y=ax+b, donde b!=0\n\nDefinimos la variable \\(X\\)\n\nX=x[:,np.newaxis]\nX.shape\n\n(40, 1)\n\n\nNota: en este caso como estamos trabajando con datos simulados no es necesario definir \\(y\\) puesto que sklearn si nos permite ingresar ese array\nAhor ajustamos el modelo\n\nLineal.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nObservemos el valor del coeficiente (\\(a\\))\n\nLineal.coef_\n\narray([1.50922213])\n\n\nObservemos el valor del intercepto (\\(b\\))\n\nLineal.intercept_\n\n-2.60022468062337\n\n\nProbemos el poder predictivo de nuestro modelo\n\nxfit=np.arange(17,22,0.5)\nXfit=xfit[:,np.newaxis]\nyfit=Lineal.predict(Xfit)\n\nAhora realizemos una visualizaci√≥n\n\ndef f(x):\n  return 1.51*x-2.6\n\nplt.scatter(x,y)\nplt.scatter(xfit,yfit,color='red')\nplt.plot(range(-1,22),[f(i) for i in range(-1,22)],color='cyan')\nplt.legend(['Datos modelo','Datos predichos','y=ax+b'])\nplt.show()"
  },
  {
    "objectID": "posts/naive_bayes/index.html",
    "href": "posts/naive_bayes/index.html",
    "title": "Naive Bayes Clasificaci√≥n",
    "section": "",
    "text": "En este post vamos a utilizar los datos iris para entrenar un modelo de clasificaci√≥n y ver que tan bien se puede predecir las etiquetas\nEn este caso para evitarnos particionar el conjunto a ‚Äúmano‚Äù utilizaremos la funci√≥n train_test_split\nPrieramente importamos las librerias necesarias\n\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\nluego importamos el conjunto de datos\n\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\nAhora creamos los datos de entrenamiento y validaci√≥n\n\nXtrain,Xtest,ytrain,ytest =train_test_split(X_iris,y_iris,random_state=1)\n\nAhora importamos el modelo que utilizaremos\n\nfrom sklearn.naive_bayes import GaussianNB\n\nDefinimos un nombre\n\nNB=GaussianNB()\n\najustamos el modelo\n\nNB.fit(Xtrain,ytrain)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\npredecimos la data que no se utilizo para establecer los par√°metros del modelo\n\ny_NB=NB.predict(Xtest)\n\nAhora utilizando la m√©trica de exactitud (accurancy score) evaluaremos que tan bien predice el modelo que se ha creado\n\nfrom sklearn.metrics import accuracy_score\n\n\naccuracy_score(ytest,y_NB)\n\n0.9736842105263158\n\n\nVemos que con una exactitud del \\(97,4\\%\\) el modelo implementado puede etiquetar los datos requeridos."
  },
  {
    "objectID": "posts/2023-12-20-PCA/index.html",
    "href": "posts/2023-12-20-PCA/index.html",
    "title": "Analisis de Componentes Principales",
    "section": "",
    "text": "En este post se pretende reducir dimensiones de una cantidad de datos es decir encontrar una transformaci√≥n en la cu√°l se represente de mejor manera los datos reduciendo asi su diemensi√≥n\nComo ya es costumbre primero importamos las librerias necesarias\n\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nImportemos los datos\n\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\nDefinimos el modelo\n\nACP=PCA(n_components=2)\n\nAjustamos el modelo\n\nACP.fit(X_iris)\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2)\n\n\ntransformamos la data\n\nX_2d=ACP.transform(X_iris)\n\nRealizamos una representaci√≥n gr√°fica\n\n#para facilitarnos el trabajo vamos a extender sobre el data iris los nuevos ejes encontrados\niris['PCA1']=X_2d[:,0]\niris['PCA2']=X_2d[:,1]\nsb.lmplot(x=\"PCA1\",y=\"PCA2\",hue='species',data=iris,fit_reg=False)"
  },
  {
    "objectID": "posts/2023-12-20-Clustering/index.html",
    "href": "posts/2023-12-20-Clustering/index.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "En este post veremos un modelos de cluster\nComo ya es costumbre primero importamos las librerias necesarias\n\nimport seaborn as sb \nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nImportemos los datos\n\niris=sb.load_dataset('iris')\nX_iris=iris.drop('species',axis=1)\ny_iris=iris['species']\n\nDefinimos el modelo\n\nGauss=GaussianMixture(n_components=3,\ncovariance_type='full')\n\nAjustamos el modelo\n\nGauss.fit(X_iris)\n\nGaussianMixture(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=3)\n\n\nDeterminamos las etiquetas\n\ny_gmm=Gauss.predict(X_iris)\n\ngraficamente\n\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\niris['cluster']=y_gmm\nsb.lmplot(data=iris,x='sepal_length',y='petal_length',hue='species',col='cluster',fit_reg=False)"
  }
]